{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "# Detectron2 Object Detection\n",
        "### Overview\n",
        "* Train an instance segmentation model using detecton2\n",
        "* Convert Labelbox data into the mscoco object detection format\n",
        "* Upload predictions to the Labelbox model diagnostics tool\n",
        "* Upload predictions for MAL to accelerate labeling efforts\n",
        "\n",
        "### Usage\n",
        "- <b>Model Training</b>:\n",
        "  * Set a project ID containing polygons and/or rectangles\n",
        "- <b>Diagnostics</b>:\n",
        "  * No additional configuration is necessary. As long as the model has been   \n",
        "- <b>MAL</b>:\n",
        "  * Set a dataset ID for the dataset you would like to upload predictions to. A new project will automatically be created.\n",
        "trained this will work.\n",
        "\n",
        "### Suggested Workflow\n",
        "* To get the most out of Labelbox, we suggest training a model on a small amount of data, exploring model performance using diagnostics, selecting a dataset using catalog to address model shortcomings, make any model architecture adjustments, and then upload predictions via MAL made on this new dataset for faster labeling.\n",
        "\n",
        "\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "<b>Setup</b>:\n",
        "* This is the only section that needs to be configured."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "API_KEY = None\n",
        "# For training:\n",
        "project_id = \"\"\n",
        "# The model will make predictions on the following dataset \n",
        "# and upload predictions to a new project for model assisted labeling.\n",
        "mal_dataset_id = \"\""
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Installation"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Based on:\n",
        "# https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=Ya5nEuMELeq8"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "!pip install -q torch \\\n",
        "                torchvision \\\n",
        "                git+https://github.com/cocodataset/panopticapi.git \\\n",
        "                'git+https://github.com/facebookresearch/detectron2.git'           \n",
        "!pip install -q \"labelbox[data]\""
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "import os\n",
        "import json\n",
        "import uuid\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import cv2\n",
        "import requests\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from tqdm import tqdm \n",
        "from shapely.geometry import MultiPolygon\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.data import build_detection_test_loader\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.data.datasets.coco import load_coco_json\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "import labelbox as lb\n",
        "import labelbox.types as lb_types\n",
        "from labelbox.data.serialization import COCOConverter\n",
        "\n",
        "from labelbox.data.metrics import (\n",
        "    feature_miou_metric, \n",
        "    feature_confusion_matrix_metric\n",
        ")\n",
        "\n",
        "with open('./coco_utils.py', 'w' ) as file:\n",
        "    helper = requests.get(\"https://raw.githubusercontent.com/Labelbox/labelbox-python/master/examples/integrations/detectron2/coco_utils.py\").text\n",
        "    file.write(helper)\n",
        "from coco_utils import visualize_coco_examples, visualize_object_inferences, partition_coco    \n"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "client = lb.Client(api_key = API_KEY)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Optional Config:\n",
        "* `project_id` - Indicates which project labels should be exported from.\n",
        "* `mal_dataset_id` - Dataset to use for MAL\n",
        "* `image_root` - Where to write images to on disk\n",
        "* `mask_root` - Where to write semantic segmentation masks to on disk\n",
        "* `train_json_path` - Where the test partition coco dataset will be written to\n",
        "* `train_json_path` - Where the train partition coco dataset will be written to\n",
        "* `train_test_split` - How much of the data to add to each parition (by percent)\n",
        "* `model_zoo_config` - Detectron2 model config see more here : https://github.com/facebookresearch/detectron2/blob/master/detectron2/model_zoo/model_zoo.py \n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# These don't need to be set\n",
        "image_root = \"/tmp/images/\"\n",
        "mask_root = \"/tmp/masks/\"\n",
        "train_json_path = '/tmp/json_train_annotations.json'\n",
        "test_json_path = '/tmp/json_test_annotations.json'\n",
        "train_test_split = [0.8, 0.2]\n",
        "model_zoo_config = \"COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.yaml\"\n",
        "\n",
        "# These can be set to anything. As long as this process doesn't have \n",
        "# another dataset with these names\n",
        "train_ds_name = \"custom_coco_train\"\n",
        "test_ds_name = \"custom_coco_test\"\n",
        "\n",
        "model_name = \"detectron_object_model\""
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "* Set up directory structure\n",
        "* Create Train/Test/Val Partition\n",
        "    - Test is for testing locally\n",
        "    - Val is is for tracking over time in labelbox\n",
        "\n",
        "\n",
        "* Add your own data. It will attempt to turn masks into instances.\n",
        "* If you want panoptic segmentation follow the panoptic tutorial"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "proj = client.get_project(project_id)\n",
        "for path in [image_root, mask_root]:\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)\n"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "* Create Train and Test data for local dev\n",
        "* Use val data for Diagnostics and to track progress over time"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "labels = proj.label_generator()\n",
        "val_labels = [next(labels) for idx in range(25)]\n",
        "\n",
        "\n",
        "coco = COCOConverter.serialize_instances(\n",
        "    labels = labels, \n",
        "    image_root = image_root\n",
        ")\n",
        "train_partition, test_partition = partition_coco(coco, splits = [0.8, 0.2])\n",
        "\n",
        "for parition, file_name in [[train_partition, train_json_path], [test_partition, test_json_path]]:\n",
        "    with open(file_name, 'w') as file:\n",
        "        json.dump(parition['instance'], file)\n"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Explore Data\n",
        "* Register detectron datasets\n",
        "* Visualize the data to make sure everything was converted properly"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "register_coco_instances(train_ds_name, {} , train_json_path, image_root)\n",
        "register_coco_instances(test_ds_name , {} , test_json_path, image_root)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "\n",
        "MetadataCatalog.get(test_ds_name).thing_classes = {r['id'] : r['name'] for r in coco['categories']}\n",
        "test_json = load_coco_json(test_json_path, image_root)\n",
        "visualize_coco_examples(MetadataCatalog.get(test_ds_name), test_json , resize_dims = (768, 512), max_images = 2)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Train Model\n",
        "* Configure the hyperparameters"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Clear metadata so detectron recomputes.\n",
        "if hasattr(MetadataCatalog.get(train_ds_name), 'thing_classes'):\n",
        "    del MetadataCatalog.get(train_ds_name).thing_classes\n",
        "if hasattr(MetadataCatalog.get(test_ds_name), 'thing_classes'):\n",
        "    del MetadataCatalog.get(test_ds_name).thing_classes    \n",
        "\n",
        "# Set model config.\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(model_zoo_config))\n",
        "cfg.DATASETS.TRAIN = (train_ds_name,)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_zoo_config)  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 120\n",
        "cfg.SOLVER.STEPS = []        \n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(coco['categories']) \n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) \n",
        "\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Evaluate Model"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Use this for validation if you would like..\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)\n",
        "evaluator = COCOEvaluator(test_ds_name)\n",
        "val_loader = build_detection_test_loader(cfg, test_ds_name)\n",
        "print(inference_on_dataset(trainer.model, val_loader, evaluator))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Visualize Inferences"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "\n",
        "# Export Data From Catalog or another dataset here..\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7 \n",
        "predictor = DefaultPredictor(cfg)\n",
        "test_json = load_coco_json(test_json_path, image_root)\n",
        "del MetadataCatalog.get(test_ds_name).thing_classes\n",
        "MetadataCatalog.get(test_ds_name).thing_classes = {idx : r['name'] for idx, r in enumerate(coco['categories'])}\n"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        " visualize_object_inferences(MetadataCatalog.get(test_ds_name),test_json, predictor)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "# for single test image"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Make Predictions\n",
        "* Create helper function for making inferences\n",
        "* Predict and upload for model assisted labeling\n",
        "* Predict and upload for model diagnostics"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "\n",
        "def get_label(image):\n",
        "  # This is a bit slow, there is some i/o for downloading the images but then inference is slow\n",
        "  # Runs inference on an image and returns a label\n",
        "    res = predictor(image.value)\n",
        "    annotations = []\n",
        "    for idx in range(len(res['instances'])):\n",
        "        mask = lb_types.MaskData.from_2D_arr(res['instances'][idx].pred_masks[0].cpu().numpy().astype(np.uint8))\n",
        "        mask = lb_types.Mask(mask = mask, color = (1,1,1))\n",
        "        geom = mask.shapely.buffer(0).simplify(3)\n",
        "        if isinstance(geom, MultiPolygon):\n",
        "            geom = geom.convex_hull\n",
        "\n",
        "        annotations.append(lb_types.ObjectAnnotation(\n",
        "            name = MetadataCatalog.get(test_ds_name).thing_classes[res['instances'][idx].pred_classes[0].cpu().numpy().item()],\n",
        "            value = lb_types.Polygon(points = [lb_types.Point(x=x,y=y) for x,y in list(geom.exterior.coords)]),\n",
        "        ))\n",
        "    return lb_types.Label(data = image, annotations = annotations)\n",
        "\n",
        "# Allows us to upload local images to labelbox\n",
        "signer = lambda _bytes: client.upload_data(content=_bytes, sign=True)\n"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Upload for Diagnostics"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "labels_mea = lb_types.LabelList()\n",
        "\n",
        "with ThreadPoolExecutor(4) as executor:\n",
        "    futures = [executor.submit(get_label, label.data) for label in val_labels]\n",
        "    for future in tqdm(as_completed(futures)):\n",
        "        labels_mea.append(future.result())\n",
        "\n",
        "labels_mea.add_url_to_masks(signer) \\\n",
        "      .add_url_to_data(signer) "
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# If the model already exists fetch it with the following:\n",
        "\n",
        "model = next(client.get_models(where = lb.Model.name == model_name), None)\n",
        "if model is None:\n",
        "    model = client.create_model(model_name, ontology_id=proj.ontology().uid)\n",
        "\n",
        "\n",
        "# Increment model run version if it exists. Otherwise use the initial 0.0.0\n",
        "model_run_names = [model_run.name for model_run in model.model_runs()]\n",
        "if len(model_run_names):\n",
        "    model_run_names.sort(key=lambda s: [int(u) for u in s.split('.')])\n",
        "    latest_model_run_name = model_run_names[-1]\n",
        "    model_run_suffix = int(latest_model_run_name.split('.')[-1]) + 1\n",
        "    model_run_name = \".\".join([*latest_model_run_name.split('.')[:-1], str(model_run_suffix)])\n",
        "else:\n",
        "    model_run_name = \"0.0.0\"\n",
        "\n",
        "print(f\"Model Name: {model.name} | Model Run Version : {model_run_name}\")\n",
        "model_run = model.create_model_run(model_run_name)\n",
        "model_run.upsert_labels([label.uid for label in val_labels])"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "pairs = get_label_pairs(val_labels, labels_mea, filter_mismatch = True)\n",
        "for (ground_truth, prediction) in pairs.values():\n",
        "    metrics = []\n",
        "    metrics.extend(feature_miou_metric(ground_truth.annotations, prediction.annotations))\n",
        "    metrics.extend(feature_confusion_matrix_metric(ground_truth.annotations, prediction.annotations))    \n",
        "    prediction.annotations.extend(metrics)\n"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "upload_task = model_run.add_predictions(f'diagnostics-import-{uuid.uuid4()}', labels_mea)\n",
        "upload_task.wait_until_done()\n",
        "print(upload_task.state)\n",
        "print(upload_task.errors)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "\n",
        "for idx, model_run_data_row in enumerate(model_run.model_run_data_rows()):\n",
        "    if idx == 5:\n",
        "        break\n",
        "    print(model_run_data_row.url)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Dataset Selection\n",
        "* Explore model performance by clicking on the links above\n",
        "* Determine what data the model needs to improve\n",
        "* Use metadata and other features in [Catalog](https://app.labelbox.com/catalog) to select the highest valued data to get labeled.\n",
        "  * If necessary update `mal_dataset_id` update this variable to point to this new dataset\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Upload for Model Assisted Labeling\n",
        "\n",
        "* Upload via MAL.\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Some additional unlabeled data rows\n",
        "dataset = client.get_dataset(mal_dataset_id)\n",
        "\n",
        "\n",
        "# Use ThreadPoolExecutor to parallelize image downloads.\n",
        "# This is still a bit slow due to the amount of processing for each data row.\n",
        "# For larger datasets this has to leverage multiprocessing.\n",
        "\n",
        "labels_mal = lb_types.LabelList()\n",
        "with ThreadPoolExecutor(4) as executor:\n",
        "    data_rows = dataset.data_rows()\n",
        "    images = [lb_types.ImageData(url = data_row.row_data, uid = data_row.uid, external_id = data_row.external_id) for data_row in data_rows]\n",
        "    futures = [executor.submit(get_label, image) for image in images]\n",
        "    for future in tqdm(as_completed(futures)):\n",
        "        labels_mal.append(future.result())\n"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Create a new project for upload\n",
        "project = client.create_project(name=\"detectron_mal_project\", media_type=lb.MediaType.Image)\n",
        "editor = next(\n",
        "    client.get_labeling_frontends(where=lb.LabelingFrontend.name == 'editor'))\n",
        "project.setup(editor, labels_mal.get_ontology().asdict())\n",
        "project.enable_model_assisted_labeling()\n",
        "project.datasets.connect(dataset)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "\n",
        "labels_mal.add_url_to_masks(signer) \\\n",
        "      .add_url_to_data(signer) "
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "upload_task = project.upload_annotations(name=f\"upload-job-{uuid.uuid4()}\",\n",
        "                                        annotations=labels_mal,\n",
        "                                        validate=False)\n",
        "# Wait for upload to finish\n",
        "upload_task.wait_until_done()\n",
        "# Review the upload status\n",
        "print(upload_task.errors)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}