{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "# Setting the stage\n",
        "\n",
        "First, we import and prepare the prerequisites to process the video."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "### General dependencies"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "!nvidia-smi"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)\n",
        "\n",
        "import sys\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import uuid\n",
        "import tempfile\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "from IPython.display import display, Image\n",
        "from io import BytesIO"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# You can also use the Labelbox Client API to get specific videos or an entire\n",
        "# dataset from your Catalog. Refer to these docs:\n",
        "# https://labelbox-python.readthedocs.io/en/latest/#labelbox.client.Client.get_data_row\n",
        "\n",
        "VIDEO_PATH = \"https://storage.googleapis.com/labelbox-datasets/image_sample_data/skateboarding.mp4\"\n",
        "\n",
        "%cd {HOME}\n",
        "!wget -v {VIDEO_PATH}"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "--2023-06-01 17:47:31--  https://storage.googleapis.com/labelbox-datasets/image_sample_data/skateboarding.mp4\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.4.128, 142.251.10.128, 142.251.12.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.4.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5275594 (5.0M) [video/mp4]\n",
            "Saving to: \u2018skateboarding.mp4\u2019\n",
            "\n",
            "skateboarding.mp4   100%[===================>]   5.03M  3.21MB/s    in 1.6s    \n",
            "\n",
            "2023-06-01 17:47:33 (3.21 MB/s) - \u2018skateboarding.mp4\u2019 saved [5275594/5275594]\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### YOLOv8 dependencies"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Dependencies for YOLOv8\n",
        "\n",
        "!pip install ultralytics==8.0.20"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ultralytics==8.0.20 in /usr/local/lib/python3.10/dist-packages (8.0.20)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (1.22.4)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (4.7.0.72)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (8.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (6.0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (4.65.0)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (2.12.2)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (0.12.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (7.34.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (5.9.5)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.20) (1.24.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.20) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.20) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.20) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.20) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.20) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.20) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.20) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics==8.0.20) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.20) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.20) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.20) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.20) (3.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.20) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.20) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.20) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.20) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.20) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.20) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.20) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.20) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.20) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.20) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.20) (0.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.20) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.20) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.20) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.20) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.20) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.20) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics==8.0.20) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics==8.0.20) (16.0.5)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->ultralytics==8.0.20) (0.18.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ultralytics==8.0.20) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ultralytics==8.0.20) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ultralytics==8.0.20) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ultralytics==8.0.20) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ultralytics==8.0.20) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ultralytics==8.0.20) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ultralytics==8.0.20) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ultralytics==8.0.20) (4.8.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics==8.0.20) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics==8.0.20) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics==8.0.20) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics==8.0.20) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->ultralytics==8.0.20) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ultralytics==8.0.20) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ultralytics==8.0.20) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ultralytics==8.0.20) (0.2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->ultralytics==8.0.20) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->ultralytics==8.0.20) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics==8.0.20) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->ultralytics==8.0.20) (3.2.2)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Import packages\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "from ultralytics import YOLO"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.20 \ud83d\ude80 Python-3.10.11 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete \u2705 (2 CPUs, 12.7 GB RAM, 23.3/78.2 GB disk)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Instantiate YOLOv8 model\n",
        "\n",
        "model = YOLO(f'{HOME}/yolov8n.pt')\n",
        "colors = np.random.randint(0, 256, size=(len(model.names), 3))\n",
        "\n",
        "print(model.names)\n",
        "\n",
        "# Specify which classes you care about. The rest of classes will be filtered out.\n",
        "chosen_class_ids = [0] # person"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to /content/yolov8n.pt...\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.23M/6.23M [00:00<00:00, 318MB/s]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### SAM dependencies"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Download SAM model SDK\n",
        "\n",
        "%cd {HOME}\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-ksq8lhww\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-ksq8lhww\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment-anything\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36589 sha256=60dab8cf4f0eb0fb50e4788f8a046b3e6cdf867dae3b06260959632c39a3b29d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ujfm6ag7/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
            "Successfully built segment-anything\n",
            "Installing collected packages: segment-anything\n",
            "Successfully installed segment-anything-1.0\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Download SAM model weights\n",
        "\n",
        "%cd {HOME}\n",
        "!mkdir {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/weights\n",
            "/content/weights/sam_vit_h_4b8939.pth ; exist: True\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Import packages\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Instantiate SAM model\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "sam = sam_model_registry[\"vit_h\"](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
        "mask_predictor = SamPredictor(sam)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Labelbox dependencies"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Install labelbox package\n",
        "\n",
        "!pip install -q \"labelbox[data]\""
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/209.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m209.1/209.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for backports-datetime-fromisoformat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pygeotile (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Import packages\n",
        "\n",
        "import labelbox as lb\n",
        "import labelbox.types as lb_types"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Create a Labelbox API key for your account by following the instructions here:\n",
        "# https://docs.labelbox.com/reference/create-api-key\n",
        "# Then, fill it in here\n",
        "\n",
        "API_KEY = \"\"\n",
        "client = lb.Client(API_KEY)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Helper functions"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Cast color to ints\n",
        "def get_color(color):\n",
        "  return (int(color[0]), int(color[1]), int(color[2]))\n",
        "\n",
        "# Get video dimensions\n",
        "def get_video_dimensions(input_cap):\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  return height, width\n",
        "\n",
        "# Get output video writer with same dimensions and fps as input video\n",
        "def get_output_video_writer(input_cap, output_path):\n",
        "  # Get the video's properties (width, height, FPS)\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "  # Define the output video file\n",
        "  output_codec = cv2.VideoWriter_fourcc(*\"mp4v\")  # MP4 codec\n",
        "  output_video = cv2.VideoWriter(output_path, output_codec, fps, (width, height))\n",
        "\n",
        "  return output_video\n",
        "\n",
        "# Visualize a video frame with bounding boxes, classes and confidence scores\n",
        "def visualize_detections(frame, boxes, conf_thresholds, class_ids):\n",
        "    frame_copy = np.copy(frame)\n",
        "    for idx in range(len(boxes)):\n",
        "        class_id = int(class_ids[idx])\n",
        "        conf = float(conf_thresholds[idx])\n",
        "        x1, y1, x2, y2 = int(boxes[idx][0]), int(boxes[idx][1]), int(boxes[idx][2]), int(boxes[idx][3])\n",
        "        color = colors[class_id]\n",
        "        label = f\"{model.names[class_id]}: {conf:.2f}\"\n",
        "        cv2.rectangle(frame_copy, (x1, y1), (x2, y2), get_color(color), 2)\n",
        "        cv2.putText(frame_copy, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, get_color(color), 2)\n",
        "    return frame_copy\n",
        "\n",
        "def add_color_to_mask(mask, color):\n",
        "  next_mask = mask.astype(np.uint8)\n",
        "  next_mask = np.expand_dims(next_mask, 0).repeat(3, axis=0)\n",
        "  next_mask = np.moveaxis(next_mask, 0, -1)\n",
        "  return next_mask * color\n",
        "\n",
        "# Merge masks into a single, multi-colored mask\n",
        "def merge_masks_colored(masks, class_ids):\n",
        "  filtered_class_ids = []\n",
        "  filtered_masks = []\n",
        "  for idx, cid in enumerate(class_ids):\n",
        "    if int(cid) in chosen_class_ids:\n",
        "      filtered_class_ids.append(cid)\n",
        "      filtered_masks.append(masks[idx])\n",
        "\n",
        "  merged_with_colors = add_color_to_mask(filtered_masks[0][0], get_color(colors[int(filtered_class_ids[0])])).astype(np.uint8)\n",
        "\n",
        "  if len(filtered_masks) == 1:\n",
        "    return merged_with_colors\n",
        "\n",
        "  for i in range(1, len(filtered_masks)):\n",
        "    curr_mask_with_colors = add_color_to_mask(filtered_masks[i][0], get_color(colors[int(filtered_class_ids[i])]))\n",
        "    merged_with_colors = np.bitwise_or(merged_with_colors, curr_mask_with_colors)\n",
        "\n",
        "  return merged_with_colors.astype(np.uint8)\n",
        "\n",
        "def get_instance_uri(client, global_key, array):\n",
        "    \"\"\" Reads a numpy array into a temp Labelbox data row to-be-uploaded to Labelbox\n",
        "    Args:\n",
        "        client        :   Required (lb.Client) - Labelbox Client object\n",
        "        global_key    :   Required (str) - Data row global key\n",
        "        array         :   Required (np.ndarray) - NumPy ndarray representation of an image\n",
        "    Returns:\n",
        "        Temp Labelbox data row to-be-uploaded to Labelbox as row data\n",
        "    \"\"\"\n",
        "    # Convert array to PIL image\n",
        "    image_as_pil = PIL.Image.fromarray(array)\n",
        "    # Convert PIL image to PNG file bytes\n",
        "    image_as_bytes = BytesIO()\n",
        "    image_as_pil.save(image_as_bytes, format='PNG')\n",
        "    image_as_bytes = image_as_bytes.getvalue()\n",
        "    # Convert PNG file bytes to a temporary Labelbox URL\n",
        "    url = client.upload_data(\n",
        "        content=image_as_bytes, \n",
        "        filename=global_key,\n",
        "        content_type=\"image/jpeg\",\n",
        "        sign=True\n",
        "    )\n",
        "    # Return the URL\n",
        "    return url\n",
        "\n",
        "def get_local_instance_uri(array):\n",
        "    # Convert array to PIL image\n",
        "    image_as_pil = PIL.Image.fromarray(array)\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(suffix='.png', dir=\"/content\", delete=False) as temp_file:\n",
        "      image_as_pil.save(temp_file)\n",
        "      file_name = temp_file.name\n",
        "\n",
        "    # Return the URL\n",
        "    return file_name\n",
        "\n",
        "def create_mask_frame(frame_num, instance_uri):\n",
        "  return lb_types.MaskFrame(index=frame_num, instance_uri=instance_uri)\n",
        "\n",
        "def create_mask_instances(class_ids):\n",
        "  instances = []\n",
        "  for cid in list(set(class_ids)): # get unique class ids\n",
        "    if int(cid) in chosen_class_ids:\n",
        "      color = get_color(colors[int(cid)])\n",
        "      name = model.names[int(cid)]\n",
        "      instances.append(lb_types.MaskInstance(color_rgb=color, name=name))\n",
        "  return instances\n",
        "\n",
        "def create_video_mask_annotation(frames, instance):\n",
        "  return lb_types.VideoMaskAnnotation(\n",
        "        frames=frames,\n",
        "        instances=[instance]\n",
        "    )"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Labelbox setup"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Create a new dataset\n",
        "\n",
        "# read more here: https://docs.labelbox.com/reference/data-row-global-keys\n",
        "global_key = os.path.basename(VIDEO_PATH)\n",
        "\n",
        "asset = {\n",
        "    \"row_data\": VIDEO_PATH,\n",
        "    \"global_key\": global_key,\n",
        "    #\"media_type\": \"VIDEO\"\n",
        "}\n",
        "\n",
        "dataset = client.create_dataset(name=\"yolo-sam-video-masks-dataset\")\n",
        "task = dataset.create_data_rows([asset])\n",
        "task.wait_till_done()\n",
        "\n",
        "print(f\"Errors: {task.errors}\")\n",
        "print(f\"Failed data rows: {task.failed_data_rows}\")"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errors: None\n",
            "Failed data rows: None\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Run through YOLOv8 on the video once quickly to get unique class ids present\n",
        "# This will inform which classes we add to the ontology\n",
        "\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "\n",
        "unique_class_ids = set()\n",
        "\n",
        "# Loop through the frames of the video\n",
        "frame_num = 1\n",
        "while cap.isOpened():\n",
        "  if frame_num % 30 == 0 or frame_num == 1:\n",
        "    print(\"Processing frame number\", frame_num)\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "      break\n",
        "\n",
        "  # Run frame through YOLOv8 and get class ids predicted\n",
        "  detections = model.predict(frame, conf=0.7) # frame is a numpy array\n",
        "  for cid in detections[0].boxes.cls:\n",
        "    unique_class_ids.add(int(cid))\n",
        "  frame_num += 1\n",
        "\n",
        "cap.release()"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing frame number 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.20 \ud83d\ude80 Python-3.10.11 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing frame number 30\n",
            "Processing frame number 60\n",
            "Processing frame number 90\n",
            "Processing frame number 120\n",
            "Processing frame number 150\n",
            "Processing frame number 180\n",
            "Processing frame number 210\n",
            "Processing frame number 240\n",
            "Processing frame number 270\n",
            "Processing frame number 300\n",
            "Processing frame number 330\n",
            "Processing frame number 360\n",
            "Processing frame number 390\n",
            "Processing frame number 420\n",
            "Processing frame number 450\n",
            "Processing frame number 480\n",
            "Processing frame number 510\n",
            "Processing frame number 540\n",
            "Processing frame number 570\n",
            "Processing frame number 600\n",
            "Processing frame number 630\n",
            "Processing frame number 660\n",
            "Processing frame number 690\n",
            "Processing frame number 720\n",
            "Processing frame number 750\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "unique_class_ids"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 2, 36}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Create a new ontology if you don't have one\n",
        "\n",
        "# Add all chosen classes into the ontology\n",
        "tools = []\n",
        "for cls in chosen_class_ids:\n",
        "  tools.append(lb.Tool(tool=lb.Tool.Type.RASTER_SEGMENTATION, name=model.names[cls]))\n",
        "\n",
        "ontology_builder = lb.OntologyBuilder(\n",
        "    classifications=[],\n",
        "    tools=tools\n",
        "  )\n",
        "\n",
        "ontology = client.create_ontology(\"yolo-sam-video-masks-ontology\",\n",
        "                                  ontology_builder.asdict()\n",
        "                                  #media_type=lb.MediaType.Image\n",
        "                                  )\n",
        "\n",
        "# Or get an existing ontology by name or ID (uncomment one of the below)\n",
        "\n",
        "# ontology = client.get_ontologies(\"Demo Chair\").get_one()\n",
        "\n",
        "# ontology = client.get_ontology(\"clhee8kzt049v094h7stq7v25\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Create a new project if you don't have one\n",
        "\n",
        "# Project defaults to batch mode with benchmark quality settings if this argument is not provided\n",
        "# Queue mode will be deprecated once dataset mode is deprecated\n",
        "project = client.create_project(name=\"yolo-sam-video-masks-project\",\n",
        "                                media_type=lb.MediaType.Video)\n",
        "\n",
        "# Or get an existing project by ID (uncomment the below)\n",
        "\n",
        "# project = get_project(\"fill_in_project_id\")\n",
        "\n",
        "# If the project already has an ontology set up, comment out this line\n",
        "project.setup_editor(ontology)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Create a new batch of data for the project you specified above\n",
        "\n",
        "# Uncomment if you are using `data_rows` parameter below\n",
        "# data_row_ids = client.get_data_row_ids_for_global_keys([global_key])['results']\n",
        "\n",
        "batch = project.create_batch(\n",
        "    \"yolo-sam-video-masks-project\",  # each batch in a project must have a unique name\n",
        "    \n",
        "    # you can also specify global_keys instead of data_rows\n",
        "    global_keys=[global_key],\n",
        "\n",
        "    # you can also specify data_rows instead of global_keys\n",
        "    #data_rows=data_row_ids,\n",
        "    \n",
        "    priority=1  # priority between 1(highest) - 5(lowest)\n",
        ")\n",
        "\n",
        "print(f\"Batch: {batch}\")"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: <Batch {\n",
            "    \"consensus_settings_json\": \"{\\\"numberOfLabels\\\":1,\\\"coveragePercentage\\\":0}\",\n",
            "    \"created_at\": \"2023-06-01 17:04:59+00:00\",\n",
            "    \"name\": \"yolo-sam-video-masks-project\",\n",
            "    \"size\": 1,\n",
            "    \"uid\": \"70e28430-009e-11ee-8071-9d7460cd95c5\",\n",
            "    \"updated_at\": \"2023-06-01 17:04:59+00:00\"\n",
            "}>\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "tools = ontology.tools()\n",
        "\n",
        "feature_schema_ids = dict()\n",
        "for tool in tools:\n",
        "  feature_schema_ids[tool.name] = tool.feature_schema_id\n",
        "\n",
        "print(feature_schema_ids)"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'person': 'cliddzk7u01gn070a7q98gex1'}\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Loop through each frame of video and process it"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Run YOLOv8 and then SAM on each frame, and write visualization videos to disk\n",
        "# You can download /content/skateboarding_boxes.mp4 and /content/skateboarding_masks.mp4\n",
        "# to visualize the results\n",
        "\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "\n",
        "output_video_boxes = get_output_video_writer(cap, \"/content/skateboarding_boxes.mp4\")\n",
        "output_video_masks = get_output_video_writer(cap, \"/content/skateboarding_masks.mp4\")\n",
        "mask_frames = []\n",
        "\n",
        "# Loop through the frames of the video\n",
        "frame_num = 1\n",
        "while cap.isOpened():\n",
        "  if frame_num % 30 == 0 or frame_num == 1:\n",
        "    print(\"Processing frames\", frame_num, \"-\", frame_num+29)\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "      break\n",
        "\n",
        "  # Run frame through YOLOv8 to get detections\n",
        "  detections = model.predict(frame, conf=0.7) # frame is a numpy array\n",
        "  \n",
        "  # Write detections to output video\n",
        "  frame_with_detections = visualize_detections(frame, \n",
        "                                                 detections[0].boxes.cpu().xyxy, \n",
        "                                                 detections[0].boxes.cpu().conf, \n",
        "                                                 detections[0].boxes.cpu().cls)\n",
        "  output_video_boxes.write(frame_with_detections)\n",
        "\n",
        "  # Run frame and detections through SAM to get masks\n",
        "  transformed_boxes = mask_predictor.transform.apply_boxes_torch(detections[0].boxes.xyxy, list(get_video_dimensions(cap)))\n",
        "  if len(transformed_boxes) == 0:\n",
        "    print(\"No boxes found on frame\", frame_num)\n",
        "    output_video_masks.write(frame)\n",
        "    frame_num += 1\n",
        "    continue\n",
        "  mask_predictor.set_image(frame)\n",
        "  masks, scores, logits = mask_predictor.predict_torch(\n",
        "    boxes = transformed_boxes,\n",
        "    multimask_output=False,\n",
        "    point_coords=None,\n",
        "    point_labels=None\n",
        "  )\n",
        "  masks = np.array(masks.cpu())\n",
        "  if masks is None or len(masks) == 0:\n",
        "    print(\"No masks found on frame\", frame_num)\n",
        "    output_video_masks.write(frame)\n",
        "    frame_num += 1\n",
        "    continue\n",
        "  merged_colored_mask = merge_masks_colored(masks, detections[0].boxes.cls)\n",
        "  \n",
        "  # Write masks to output video\n",
        "  image_combined = cv2.addWeighted(frame, 0.7, merged_colored_mask, 0.7, 0)\n",
        "  output_video_masks.write(image_combined)\n",
        "\n",
        "  # Create video mask annotation for upload to Labelbox\n",
        "  instance_uri = get_instance_uri(client, global_key, merged_colored_mask)\n",
        "  mask_frame = create_mask_frame(frame_num, instance_uri)\n",
        "  mask_frames.append(mask_frame)\n",
        "  \n",
        "  frame_num += 1\n",
        "\n",
        "  # For the purposes of this demo, only look at the first 90 frames\n",
        "  if frame_num > 90:\n",
        "    break\n",
        "\n",
        "cap.release()\n",
        "output_video_boxes.release()\n",
        "output_video_masks.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing frame number 1\n",
            "No boxes found on frame 1\n",
            "No boxes found on frame 2\n",
            "No boxes found on frame 3\n",
            "No boxes found on frame 4\n",
            "No boxes found on frame 5\n",
            "No boxes found on frame 6\n",
            "No boxes found on frame 7\n",
            "No boxes found on frame 8\n",
            "No boxes found on frame 9\n",
            "No boxes found on frame 10\n",
            "No boxes found on frame 11\n",
            "No boxes found on frame 12\n",
            "No boxes found on frame 13\n",
            "No boxes found on frame 14\n",
            "No boxes found on frame 15\n",
            "No boxes found on frame 16\n",
            "No boxes found on frame 17\n",
            "No boxes found on frame 18\n",
            "No boxes found on frame 19\n",
            "No boxes found on frame 20\n",
            "No boxes found on frame 21\n",
            "No boxes found on frame 22\n",
            "No boxes found on frame 23\n",
            "No boxes found on frame 24\n",
            "No boxes found on frame 25\n",
            "No boxes found on frame 26\n",
            "No boxes found on frame 27\n",
            "No boxes found on frame 28\n",
            "No boxes found on frame 29\n",
            "Processing frame number 30\n",
            "No boxes found on frame 30\n",
            "No boxes found on frame 31\n",
            "No boxes found on frame 32\n",
            "No boxes found on frame 33\n",
            "No boxes found on frame 34\n",
            "No boxes found on frame 35\n",
            "No boxes found on frame 36\n",
            "No boxes found on frame 37\n",
            "No boxes found on frame 38\n",
            "No boxes found on frame 39\n",
            "No boxes found on frame 40\n",
            "No boxes found on frame 41\n",
            "No boxes found on frame 42\n",
            "No boxes found on frame 43\n",
            "No boxes found on frame 44\n",
            "Processing frame number 60\n",
            "Processing frame number 90\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Create annotations for LB upload\n",
        "mask_instances = create_mask_instances(unique_class_ids)\n",
        "annotations = []\n",
        "for instance in mask_instances:\n",
        "  annotations.append(create_video_mask_annotation(mask_frames, instance))\n",
        "\n",
        "labels = []\n",
        "labels.append(\n",
        "    lb_types.Label(data=lb_types.VideoData(global_key=global_key),\n",
        "                   annotations=annotations))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Upload the predictions to your specified project and data rows as pre-labels\n",
        "# Note: This may take a few minutes, depending on size of video and number of masks\n",
        "\n",
        "upload_job = lb.MALPredictionImport.create_from_objects(\n",
        "    client=client,\n",
        "    project_id=project.uid,\n",
        "    name=\"mal_import_job\" + str(uuid.uuid4()),\n",
        "    predictions=labels\n",
        ")\n",
        "upload_job.wait_until_done()\n",
        "\n",
        "print(f\"Errors: {upload_job.errors}\", )\n",
        "print(f\"Status of uploads: {upload_job.statuses}\")"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errors: []\n",
            "Status of uploads: [{'uuid': 'b11e35fb-657a-4a21-96de-005d911e5229', 'dataRow': {'id': 'cliddyrjv12yw07470vjra12r', 'globalKey': 'skateboarding.mp4'}, 'status': 'SUCCESS'}]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "Now head on over to your Labelbox account! You should see a new project by the name you specified above, and when you hit Start Labeling, you should see all the predicted masks rendered.\n",
        "\n",
        "Using the tools in the video editor, you can then modify or review the masks."
      ],
      "cell_type": "markdown"
    }
  ]
}