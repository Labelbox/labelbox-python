{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {},
    "cells": [
        {
            "cell_type": "markdown",
            "id": "db768cda",
            "metadata": {},
            "source": [
                "<td>",
                "   <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/></a>",
                "</td>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb5611d0",
            "metadata": {},
            "source": [
                "<td>\n",
                "<a href=\"https://colab.research.google.com/github/Labelbox/labelbox-python/blob/develop/examples/integrations/sam/meta_sam.ipynb\" target=\"_blank\"><img",
                "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>",
                "</td>\n",
                "\n",
                "<td>\n",
                "<a href=\"https://github.com/Labelbox/labelbox-python/tree/develop/examples/integrations/sam/meta_sam.ipynb\" target=\"_blank\"><img",
                "src=\"https://img.shields.io/badge/GitHub-100000?logo=github&logoColor=white\" alt=\"GitHub\"></a>\n",
                "</td>"
            ]
        },
        {
            "metadata": {},
            "source": [
                "# Setup\n",
                "This notebook is used to show how to use Meta's Segment Anything model to create masks that can then be uploaded to a Labelbox project"
            ],
            "cell_type": "markdown"
        },
        {
            "metadata": {},
            "source": "%pip install -q \"labelbox[data]\"\n%pip install -q ultralytics==8.0.20\n%pip install -q \"git+https://github.com/facebookresearch/segment-anything.git\"",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": "# Check if in google colab\ntry:\n    import google.colab\n\n    IN_COLAB = True\nexcept:\n    IN_COLAB = False",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": "from IPython import display\n\ndisplay.clear_output()\n\nimport ultralytics\n\nultralytics.checks()\n\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\nfrom IPython.display import display, Image\nimport torch\nimport matplotlib.pyplot as plt\nfrom segment_anything import (\n    sam_model_registry,\n    SamAutomaticMaskGenerator,\n    SamPredictor,\n)\nimport os\nimport urllib.request\nimport uuid\n\nimport labelbox as lb\nimport labelbox.types as lb_types\n\nHOME = os.getcwd()\n\nif IN_COLAB:\n    from google.colab.patches import cv2_imshow",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": [
                "# API Key and Client\n",
                "Provide a valid api key below in order to properly connect to the Labelbox Client."
            ],
            "cell_type": "markdown"
        },
        {
            "metadata": {},
            "source": "# Add your api key\nAPI_KEY = \"\"\n# To get your API key go to: Workspace settings -> API -> Create API Key\nclient = lb.Client(api_key=API_KEY)",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": [
                "# Predicting bounding boxes around common objects using YOLOv8\n",
                "\n",
                "First, we start with loading the YOLOv8 model, getting a sample image, and running the model on it to generate bounding boxes around some common objects."
            ],
            "cell_type": "markdown"
        },
        {
            "metadata": {},
            "source": [
                "### Utilize YOLOV8 to Create Bounding Boxes\n",
                "\n",
                "We use YOLOV8 in this demo to obtain bounding boxes around our images that we can later feed into SAM for our masks."
            ],
            "cell_type": "markdown"
        },
        {
            "metadata": {},
            "source": [
                "Below we run inference on a image using the YOLOv8 model."
            ],
            "cell_type": "markdown"
        },
        {
            "metadata": {},
            "source": "# You can also use the Labelbox Client API to get specific images or an entire\n# dataset from your Catalog. Refer to these docs:\n# https://labelbox-python.readthedocs.io/en/latest/#labelbox.client.Client.get_data_row\n\nIMAGE_PATH = \"https://storage.googleapis.com/labelbox-datasets/image_sample_data/chairs.jpeg\"",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": "model = YOLO(f\"{HOME}/yolov8n.pt\")\nresults = model.predict(source=IMAGE_PATH, conf=0.25)\n\n# print(results[0].boxes.xyxy) # print bounding box coordinates\n\n# print(results[0].boxes.conf) # print confidence scores\n\n# for c in results[0].boxes.cls:\n# print(model.names[int(c)]) # print predicted classes",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": [
                "Below we visualize the bounding boxes on the image using CV2."
            ],
            "cell_type": "markdown"
        },
        {
            "metadata": {},
            "source": "image_bgr = cv2.imread(\"./chairs.jpeg\")\n\nfor box in results[0].boxes.xyxy:\n    cv2.rectangle(\n        image_bgr,\n        (int(box[0]), int(box[1])),\n        (int(box[2]), int(box[3])),\n        (0, 255, 0),\n        2,\n    )\n\nif IN_COLAB:\n    cv2_imshow(image_bgr)\nelse:\n    cv2.imshow(\"demo\", image_bgr)\n    cv2.waitKey()",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": [
                "### Predicting segmentation masks using Meta's Segment Anything model\n",
                "\n",
                "Now we load Meta's Segment Anything model and feed the bounding boxes to it, so it can generate segmentation masks within them."
            ],
            "cell_type": "markdown"
        },
        {
            "metadata": {},
            "source": "# Download SAM model weights\n\nCHECKPOINT_PATH = os.path.join(HOME, \"sam_vit_h_4b8939.pth\")\n\nif not os.path.isfile(CHECKPOINT_PATH):\n    req = urllib.request.urlretrieve(\n        \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n        \"sam_vit_h_4b8939.pth\",\n    )",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nMODEL_TYPE = \"vit_h\"",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(\n    device=DEVICE)\nmask_predictor = SamPredictor(sam)",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": "transformed_boxes = mask_predictor.transform.apply_boxes_torch(\n    results[0].boxes.xyxy, image_bgr.shape[:2])\n\nmask_predictor.set_image(image_bgr)\n\nmasks, scores, logits = mask_predictor.predict_torch(\n    boxes=transformed_boxes,\n    multimask_output=False,\n    point_coords=None,\n    point_labels=None,\n)\nmasks = np.array(masks.cpu())\n\n# print(masks)\n# print(scores)",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": [
                "Here we visualize the segmentation masks drawn on the image."
            ],
            "cell_type": "markdown"
        },
        {
            "metadata": {},
            "source": "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n\nfinal_mask = None\nfor i in range(len(masks) - 1):\n    if final_mask is None:\n        final_mask = np.bitwise_or(masks[i][0], masks[i + 1][0])\n    else:\n        final_mask = np.bitwise_or(final_mask, masks[i + 1][0])\n\nplt.figure(figsize=(10, 10))\nplt.imshow(image_rgb)\nplt.axis(\"off\")\nplt.imshow(final_mask, cmap=\"gray\", alpha=0.7)\n\nplt.show()",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": [
                "### Uploading predicted segmentation masks with class names to Labelbox using Python SDK"
            ],
            "cell_type": "markdown"
        },
        {
            "metadata": {},
            "source": "# Create a Labelbox ObjectAnnotation of type mask for each predicted mask\n\n# Identifying what values in the numpy array correspond to the mask annotation\ncolor = (1, 1, 1)\n\nclass_names = []\nfor c in results[0].boxes.cls:\n    class_names.append(model.names[int(c)])\n\nannotations = []\nfor idx, mask in enumerate(masks):\n    mask_data = lb_types.MaskData.from_2D_arr(np.asarray(mask[0],\n                                                         dtype=\"uint8\"))\n    mask_annotation = lb_types.ObjectAnnotation(\n        name=class_names[\n            idx],  # this is the class predicted in Step 1 (object detector)\n        value=lb_types.Mask(mask=mask_data, color=color),\n    )\n    annotations.append(mask_annotation)",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": "# Create a new dataset\n\n# read more here: https://docs.labelbox.com/reference/data-row-global-keys\nglobal_key = \"my_unique_global_key\"\n\ntest_img_url = {\"row_data\": IMAGE_PATH, \"global_key\": global_key}\n\ndataset = client.create_dataset(name=\"auto-mask-classification-dataset\")\ntask = dataset.create_data_rows([test_img_url])\ntask.wait_till_done()\n\nprint(f\"Errors: {task.errors}\")\nprint(f\"Failed data rows: {task.failed_data_rows}\")",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": "# Create a new ontology if you don't have one\n\n# Add all unique classes detected in Step 1\ntools = []\nfor name in set(class_names):\n    tools.append(lb.Tool(tool=lb.Tool.Type.RASTER_SEGMENTATION, name=name))\n\nontology_builder = lb.OntologyBuilder(classifications=[], tools=tools)\n\nontology = client.create_ontology(\n    \"auto-mask-classification-ontology\",\n    ontology_builder.asdict(),\n    media_type=lb.MediaType.Image,\n)\n\n# Or get an existing ontology by name or ID (uncomment one of the below)\n\n# ontology = client.get_ontologies(\"Demo Chair\").get_one()\n\n# ontology = client.get_ontology(\"clhee8kzt049v094h7stq7v25\")",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": "# Create a new project if you don't have one\n\n# Project defaults to batch mode with benchmark quality settings if this argument is not provided\n# Queue mode will be deprecated once dataset mode is deprecated\nproject = client.create_project(name=\"auto-mask-classification-project\",\n                                media_type=lb.MediaType.Image)\n\n# Or get an existing project by ID (uncomment the below)\n\n# project = get_project(\"fill_in_project_id\")\n\n# If the project already has an ontology set up, comment out this line\nproject.setup_editor(ontology)",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": "# Create a new batch of data for the project you specified above\n\ndata_row_ids = client.get_data_row_ids_for_global_keys([global_key])[\"results\"]\n\nbatch = project.create_batch(\n    \"auto-mask-classification-batch\",  # each batch in a project must have a unique name\n    data_rows=data_row_ids,\n    # you can also specify global_keys instead of data_rows\n    # global_keys=[global_key],  # paginated collection of data row objects, list of data row ids or global keys\n    priority=1,  # priority between 1(highest) - 5(lowest)\n)\n\nprint(f\"Batch: {batch}\")",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": "labels = []\nlabels.append(\n    lb_types.Label(data=lb_types.ImageData(global_key=global_key),\n                   annotations=annotations))",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": "# Upload the predictions to your specified project and data rows as pre-labels\n\nupload_job = lb.MALPredictionImport.create_from_objects(\n    client=client,\n    project_id=project.uid,\n    name=\"mal_job\" + str(uuid.uuid4()),\n    predictions=labels,\n)\nupload_job.wait_until_done()\n\nprint(f\"Errors: {upload_job.errors}\",)\nprint(f\"Status of uploads: {upload_job.statuses}\")",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        },
        {
            "metadata": {},
            "source": [
                "### Cleanup"
            ],
            "cell_type": "markdown"
        },
        {
            "metadata": {},
            "source": "# dataset.delete()\n# project.delete()",
            "cell_type": "code",
            "outputs": [],
            "execution_count": null
        }
    ]
}