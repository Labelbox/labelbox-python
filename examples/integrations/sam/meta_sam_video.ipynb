{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<td>\n",
        "   <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/></a>\n",
        "</td>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<td>\n",
        "<a href=\"https://colab.research.google.com/github/Labelbox/labelbox-python/blob/develop/examples/integrations/sam/meta_sam_labelbox_video.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "<a href=\"https://github.com/Labelbox/labelbox-python/blob/develop/examples/integrations/sam/meta_sam_labelbox_video.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://img.shields.io/badge/GitHub-100000?logo=github&logoColor=white\" alt=\"GitHub\"></a>\n",
        "</td>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setting the stage\n",
        "\n",
        "First, we import and prepare the prerequisites to process the video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### General dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)\n",
        "\n",
        "import sys\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import uuid\n",
        "import tempfile\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "from IPython.display import display, Image\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You can also use the Labelbox Client API to get specific videos or an entire\n",
        "# dataset from your Catalog. Refer to these docs:\n",
        "# https://labelbox-python.readthedocs.io/en/latest/#labelbox.client.Client.get_data_row\n",
        "\n",
        "VIDEO_PATH = \"https://storage.googleapis.com/labelbox-datasets/image_sample_data/skateboarding.mp4\"\n",
        "\n",
        "%cd {HOME}\n",
        "!wget -v {VIDEO_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### YOLOv8 dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dependencies for YOLOv8\n",
        "\n",
        "!pip install ultralytics==8.0.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import packages\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate YOLOv8 model\n",
        "\n",
        "model = YOLO(f'{HOME}/yolov8n.pt')\n",
        "colors = np.random.randint(0, 256, size=(len(model.names), 3))\n",
        "\n",
        "print(model.names)\n",
        "\n",
        "# Specify which classes you care about. The rest of classes will be filtered out.\n",
        "chosen_class_ids = [0] # person"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SAM dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download SAM model SDK\n",
        "\n",
        "%cd {HOME}\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download SAM model weights\n",
        "\n",
        "%cd {HOME}\n",
        "!mkdir {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import packages\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate SAM model\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "sam = sam_model_registry[\"vit_h\"](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
        "mask_predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Labelbox dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install labelbox package\n",
        "\n",
        "!pip install -q \"labelbox[data]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import packages\n",
        "\n",
        "import labelbox as lb\n",
        "import labelbox.types as lb_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a Labelbox API key for your account by following the instructions here:\n",
        "# https://docs.labelbox.com/reference/create-api-key\n",
        "# Then, fill it in here\n",
        "\n",
        "API_KEY = \"\"\n",
        "client = lb.Client(API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cast color to ints\n",
        "def get_color(color):\n",
        "  return (int(color[0]), int(color[1]), int(color[2]))\n",
        "\n",
        "# Get video dimensions\n",
        "def get_video_dimensions(input_cap):\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  return height, width\n",
        "\n",
        "# Get output video writer with same dimensions and fps as input video\n",
        "def get_output_video_writer(input_cap, output_path):\n",
        "  # Get the video's properties (width, height, FPS)\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "  # Define the output video file\n",
        "  output_codec = cv2.VideoWriter_fourcc(*\"mp4v\")  # MP4 codec\n",
        "  output_video = cv2.VideoWriter(output_path, output_codec, fps, (width, height))\n",
        "\n",
        "  return output_video\n",
        "\n",
        "# Visualize a video frame with bounding boxes, classes and confidence scores\n",
        "def visualize_detections(frame, boxes, conf_thresholds, class_ids):\n",
        "    frame_copy = np.copy(frame)\n",
        "    for idx in range(len(boxes)):\n",
        "        class_id = int(class_ids[idx])\n",
        "        conf = float(conf_thresholds[idx])\n",
        "        x1, y1, x2, y2 = int(boxes[idx][0]), int(boxes[idx][1]), int(boxes[idx][2]), int(boxes[idx][3])\n",
        "        color = colors[class_id]\n",
        "        label = f\"{model.names[class_id]}: {conf:.2f}\"\n",
        "        cv2.rectangle(frame_copy, (x1, y1), (x2, y2), get_color(color), 2)\n",
        "        cv2.putText(frame_copy, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, get_color(color), 2)\n",
        "    return frame_copy\n",
        "\n",
        "def add_color_to_mask(mask, color):\n",
        "  next_mask = mask.astype(np.uint8)\n",
        "  next_mask = np.expand_dims(next_mask, 0).repeat(3, axis=0)\n",
        "  next_mask = np.moveaxis(next_mask, 0, -1)\n",
        "  return next_mask * color\n",
        "\n",
        "# Merge masks into a single, multi-colored mask\n",
        "def merge_masks_colored(masks, class_ids):\n",
        "  filtered_class_ids = []\n",
        "  filtered_masks = []\n",
        "  for idx, cid in enumerate(class_ids):\n",
        "    if int(cid) in chosen_class_ids:\n",
        "      filtered_class_ids.append(cid)\n",
        "      filtered_masks.append(masks[idx])\n",
        "\n",
        "  merged_with_colors = add_color_to_mask(filtered_masks[0][0], get_color(colors[int(filtered_class_ids[0])])).astype(np.uint8)\n",
        "\n",
        "  if len(filtered_masks) == 1:\n",
        "    return merged_with_colors\n",
        "\n",
        "  for i in range(1, len(filtered_masks)):\n",
        "    curr_mask_with_colors = add_color_to_mask(filtered_masks[i][0], get_color(colors[int(filtered_class_ids[i])]))\n",
        "    merged_with_colors = np.bitwise_or(merged_with_colors, curr_mask_with_colors)\n",
        "\n",
        "  return merged_with_colors.astype(np.uint8)\n",
        "\n",
        "def get_instance_uri(client, global_key, array):\n",
        "    \"\"\" Reads a numpy array into a temp Labelbox data row to-be-uploaded to Labelbox\n",
        "    Args:\n",
        "        client        :   Required (lb.Client) - Labelbox Client object\n",
        "        global_key    :   Required (str) - Data row global key\n",
        "        array         :   Required (np.ndarray) - NumPy ndarray representation of an image\n",
        "    Returns:\n",
        "        Temp Labelbox data row to-be-uploaded to Labelbox as row data\n",
        "    \"\"\"\n",
        "    # Convert array to PIL image\n",
        "    image_as_pil = PIL.Image.fromarray(array)\n",
        "    # Convert PIL image to PNG file bytes\n",
        "    image_as_bytes = BytesIO()\n",
        "    image_as_pil.save(image_as_bytes, format='PNG')\n",
        "    image_as_bytes = image_as_bytes.getvalue()\n",
        "    # Convert PNG file bytes to a temporary Labelbox URL\n",
        "    url = client.upload_data(\n",
        "        content=image_as_bytes,\n",
        "        filename=global_key,\n",
        "        content_type=\"image/jpeg\",\n",
        "        sign=True\n",
        "    )\n",
        "    # Return the URL\n",
        "    return url\n",
        "\n",
        "def get_local_instance_uri(array):\n",
        "    # Convert array to PIL image\n",
        "    image_as_pil = PIL.Image.fromarray(array)\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(suffix='.png', dir=\"/content\", delete=False) as temp_file:\n",
        "      image_as_pil.save(temp_file)\n",
        "      file_name = temp_file.name\n",
        "\n",
        "    # Return the URL\n",
        "    return file_name\n",
        "\n",
        "def create_mask_frame(frame_num, instance_uri):\n",
        "  return lb_types.MaskFrame(index=frame_num, instance_uri=instance_uri)\n",
        "\n",
        "def create_mask_instances(class_ids):\n",
        "  instances = []\n",
        "  for cid in list(set(class_ids)): # get unique class ids\n",
        "    if int(cid) in chosen_class_ids:\n",
        "      color = get_color(colors[int(cid)])\n",
        "      name = model.names[int(cid)]\n",
        "      instances.append(lb_types.MaskInstance(color_rgb=color, name=name))\n",
        "  return instances\n",
        "\n",
        "def create_video_mask_annotation(frames, instance):\n",
        "  return lb_types.VideoMaskAnnotation(\n",
        "        frames=frames,\n",
        "        instances=[instance]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Labelbox setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new dataset\n",
        "\n",
        "# read more here: https://docs.labelbox.com/reference/data-row-global-keys\n",
        "global_key = os.path.basename(VIDEO_PATH)\n",
        "\n",
        "asset = {\n",
        "    \"row_data\": VIDEO_PATH,\n",
        "    \"global_key\": global_key,\n",
        "    #\"media_type\": \"VIDEO\"\n",
        "}\n",
        "\n",
        "dataset = client.create_dataset(name=\"yolo-sam-video-masks-dataset\")\n",
        "task = dataset.create_data_rows([asset])\n",
        "task.wait_till_done()\n",
        "\n",
        "print(f\"Errors: {task.errors}\")\n",
        "print(f\"Failed data rows: {task.failed_data_rows}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run through YOLOv8 on the video once quickly to get unique class ids present\n",
        "# This will inform which classes we add to the ontology\n",
        "\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "\n",
        "unique_class_ids = set()\n",
        "\n",
        "# Loop through the frames of the video\n",
        "frame_num = 1\n",
        "while cap.isOpened():\n",
        "  if frame_num % 30 == 0 or frame_num == 1:\n",
        "    print(\"Processing frame number\", frame_num)\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "      break\n",
        "\n",
        "  # Run frame through YOLOv8 and get class ids predicted\n",
        "  detections = model.predict(frame, conf=0.7) # frame is a numpy array\n",
        "  for cid in detections[0].boxes.cls:\n",
        "    unique_class_ids.add(int(cid))\n",
        "  frame_num += 1\n",
        "\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_class_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new ontology if you don't have one\n",
        "\n",
        "# Add all chosen classes into the ontology\n",
        "tools = []\n",
        "for cls in chosen_class_ids:\n",
        "  tools.append(lb.Tool(tool=lb.Tool.Type.RASTER_SEGMENTATION, name=model.names[cls]))\n",
        "\n",
        "ontology_builder = lb.OntologyBuilder(\n",
        "    classifications=[],\n",
        "    tools=tools\n",
        "  )\n",
        "\n",
        "ontology = client.create_ontology(\"yolo-sam-video-masks-ontology\",\n",
        "                                  ontology_builder.asdict()\n",
        "                                  #media_type=lb.MediaType.Image\n",
        "                                  )\n",
        "\n",
        "# Or get an existing ontology by name or ID (uncomment one of the below)\n",
        "\n",
        "# ontology = client.get_ontologies(\"Demo Chair\").get_one()\n",
        "\n",
        "# ontology = client.get_ontology(\"clhee8kzt049v094h7stq7v25\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new project if you don't have one\n",
        "\n",
        "# Project defaults to batch mode with benchmark quality settings if this argument is not provided\n",
        "# Queue mode will be deprecated once dataset mode is deprecated\n",
        "project = client.create_project(name=\"yolo-sam-video-masks-project\",\n",
        "                                media_type=lb.MediaType.Video)\n",
        "\n",
        "# Or get an existing project by ID (uncomment the below)\n",
        "\n",
        "# project = get_project(\"fill_in_project_id\")\n",
        "\n",
        "# If the project already has an ontology set up, comment out this line\n",
        "project.setup_editor(ontology)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new batch of data for the project you specified above\n",
        "\n",
        "# Uncomment if you are using `data_rows` parameter below\n",
        "# data_row_ids = client.get_data_row_ids_for_global_keys([global_key])['results']\n",
        "\n",
        "batch = project.create_batch(\n",
        "    \"yolo-sam-video-masks-project\",  # each batch in a project must have a unique name\n",
        "\n",
        "    # you can also specify global_keys instead of data_rows\n",
        "    global_keys=[global_key],\n",
        "\n",
        "    # you can also specify data_rows instead of global_keys\n",
        "    #data_rows=data_row_ids,\n",
        "\n",
        "    priority=1  # priority between 1(highest) - 5(lowest)\n",
        ")\n",
        "\n",
        "print(f\"Batch: {batch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tools = ontology.tools()\n",
        "\n",
        "feature_schema_ids = dict()\n",
        "for tool in tools:\n",
        "  feature_schema_ids[tool.name] = tool.feature_schema_id\n",
        "\n",
        "print(feature_schema_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loop through each frame of video and process it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run YOLOv8 and then SAM on each frame, and write visualization videos to disk\n",
        "# You can download /content/skateboarding_boxes.mp4 and /content/skateboarding_masks.mp4\n",
        "# to visualize the results\n",
        "\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "\n",
        "output_video_boxes = get_output_video_writer(cap, \"/content/skateboarding_boxes.mp4\")\n",
        "output_video_masks = get_output_video_writer(cap, \"/content/skateboarding_masks.mp4\")\n",
        "mask_frames = []\n",
        "\n",
        "# Loop through the frames of the video\n",
        "frame_num = 1\n",
        "while cap.isOpened():\n",
        "  if frame_num % 30 == 0 or frame_num == 1:\n",
        "    print(\"Processing frames\", frame_num, \"-\", frame_num+29)\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "      break\n",
        "\n",
        "  # Run frame through YOLOv8 to get detections\n",
        "  detections = model.predict(frame, conf=0.7) # frame is a numpy array\n",
        "\n",
        "  # Write detections to output video\n",
        "  frame_with_detections = visualize_detections(frame,\n",
        "                                                 detections[0].boxes.cpu().xyxy,\n",
        "                                                 detections[0].boxes.cpu().conf,\n",
        "                                                 detections[0].boxes.cpu().cls)\n",
        "  output_video_boxes.write(frame_with_detections)\n",
        "\n",
        "  # Run frame and detections through SAM to get masks\n",
        "  transformed_boxes = mask_predictor.transform.apply_boxes_torch(detections[0].boxes.xyxy, list(get_video_dimensions(cap)))\n",
        "  if len(transformed_boxes) == 0:\n",
        "    print(\"No boxes found on frame\", frame_num)\n",
        "    output_video_masks.write(frame)\n",
        "    frame_num += 1\n",
        "    continue\n",
        "  mask_predictor.set_image(frame)\n",
        "  masks, scores, logits = mask_predictor.predict_torch(\n",
        "    boxes = transformed_boxes,\n",
        "    multimask_output=False,\n",
        "    point_coords=None,\n",
        "    point_labels=None\n",
        "  )\n",
        "  masks = np.array(masks.cpu())\n",
        "  if masks is None or len(masks) == 0:\n",
        "    print(\"No masks found on frame\", frame_num)\n",
        "    output_video_masks.write(frame)\n",
        "    frame_num += 1\n",
        "    continue\n",
        "  merged_colored_mask = merge_masks_colored(masks, detections[0].boxes.cls)\n",
        "\n",
        "  # Write masks to output video\n",
        "  image_combined = cv2.addWeighted(frame, 0.7, merged_colored_mask, 0.7, 0)\n",
        "  output_video_masks.write(image_combined)\n",
        "\n",
        "  # Create video mask annotation for upload to Labelbox\n",
        "  instance_uri = get_instance_uri(client, global_key, merged_colored_mask)\n",
        "  mask_frame = create_mask_frame(frame_num, instance_uri)\n",
        "  mask_frames.append(mask_frame)\n",
        "\n",
        "  frame_num += 1\n",
        "\n",
        "  # For the purposes of this demo, only look at the first 90 frames\n",
        "  if frame_num > 90:\n",
        "    break\n",
        "\n",
        "cap.release()\n",
        "output_video_boxes.release()\n",
        "output_video_masks.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create annotations for LB upload\n",
        "mask_instances = create_mask_instances(unique_class_ids)\n",
        "annotations = []\n",
        "for instance in mask_instances:\n",
        "  annotations.append(create_video_mask_annotation(mask_frames, instance))\n",
        "\n",
        "labels = []\n",
        "labels.append(\n",
        "    lb_types.Label(data=lb_types.VideoData(global_key=global_key),\n",
        "                   annotations=annotations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload the predictions to your specified project and data rows as pre-labels\n",
        "# Note: This may take a few minutes, depending on size of video and number of masks\n",
        "\n",
        "upload_job = lb.MALPredictionImport.create_from_objects(\n",
        "    client=client,\n",
        "    project_id=project.uid,\n",
        "    name=\"mal_import_job\" + str(uuid.uuid4()),\n",
        "    predictions=labels\n",
        ")\n",
        "upload_job.wait_until_done()\n",
        "print(f\"Errors: {upload_job.errors}\", )\n",
        "print(f\"Status of uploads: {upload_job.statuses}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
