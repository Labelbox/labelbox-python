{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<td>\n",
        "   <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/></a>\n",
        "</td>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<td>\n",
        "<a href=\"https://colab.research.google.com/github/Labelbox/labelbox-python/blob/develop/examples/integrations/sam/meta_sam_labelbox_video.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "<a href=\"https://github.com/Labelbox/labelbox-python/blob/develop/examples/integrations/sam/meta_sam_labelbox_video.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://img.shields.io/badge/GitHub-100000?logo=github&logoColor=white\" alt=\"GitHub\"></a>\n",
        "</td>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup\n",
        "\n",
        "This notebook is used to show how to use Meta's Segment Anything model and YOLO to create masks for videos that can then be uploaded to a Labelbox project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### General dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q \"labelbox[data]\"\n",
        "%pip install -q ultralytics==8.0.20\n",
        "%pip install -q \"git+https://github.com/facebookresearch/segment-anything.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if in google colab\n",
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.13 torch-2.3.0 CPU\n",
            "Setup complete âœ… (10 CPUs, 16.0 GB RAM, 258.1/460.4 GB disk)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib\n",
        "import cv2\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import uuid\n",
        "import tempfile\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab.patches import cv2_imshow\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "display.clear_output()\n",
        "from IPython.display import display, Image\n",
        "from io import BytesIO\n",
        "\n",
        "# YOLOv8 dependencies\n",
        "import ultralytics\n",
        "\n",
        "ultralytics.checks()\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# SAM dependencies\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "# Labelbox dependencies\n",
        "import labelbox as lb\n",
        "import labelbox.types as lb_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You can also use the Labelbox Client API to get specific videos or an entire\n",
        "# dataset from your Catalog. Refer to these docs:\n",
        "# https://labelbox-python.readthedocs.io/en/latest/#labelbox.client.Client.get_data_row\n",
        "HOME = os.getcwd()\n",
        "VIDEO_PATH = os.path.join(HOME, \"skateboarding.mp4\")\n",
        "\n",
        "if not os.path.isfile(VIDEO_PATH):\n",
        "    req = urllib.request.urlretrieve(\n",
        "        \"https://storage.googleapis.com/labelbox-datasets/image_sample_data/skateboarding.mp4\",\n",
        "        \"skateboarding.mp4\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### YOLOv8 setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to /Users/gabrielunderwood/repos/labelbox-python/examples/integrations/sam/yolov8n.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.23M/6.23M [00:00<00:00, 6.98MB/s]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
          ]
        }
      ],
      "source": [
        "# Instantiate YOLOv8 model\n",
        "model = YOLO(f\"{HOME}/yolov8n.pt\")\n",
        "colors = np.random.randint(0, 256, size=(len(model.names), 3))\n",
        "\n",
        "print(model.names)\n",
        "\n",
        "# Specify which classes you care about. The rest of classes will be filtered out.\n",
        "chosen_class_ids = [0]  # person"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SAM setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download SAM model weights\n",
        "CHECKPOINT_PATH = os.path.join(HOME, \"sam_vit_h_4b8939.pth\")\n",
        "\n",
        "if not os.path.isfile(CHECKPOINT_PATH):\n",
        "    req = urllib.request.urlretrieve(\n",
        "        \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n",
        "        \"sam_vit_h_4b8939.pth\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate SAM model\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "sam = sam_model_registry[\"vit_h\"](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
        "mask_predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Labelbox setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add your API key\n",
        "API_KEY = None\n",
        "# To get your API key go to: Workspace settings -> API -> Create API Key\n",
        "client = lb.Client(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cast color to ints\n",
        "def get_color(color):\n",
        "    return (int(color[0]), int(color[1]), int(color[2]))\n",
        "\n",
        "\n",
        "# Get video dimensions\n",
        "def get_video_dimensions(input_cap):\n",
        "    width = int(input_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(input_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    return height, width\n",
        "\n",
        "\n",
        "# Get output video writer with same dimensions and fps as input video\n",
        "def get_output_video_writer(input_cap, output_path):\n",
        "    # Get the video's properties (width, height, FPS)\n",
        "    width = int(input_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(input_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(input_cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Define the output video file\n",
        "    output_codec = cv2.VideoWriter_fourcc(*\"mp4v\")  # MP4 codec\n",
        "    output_video = cv2.VideoWriter(output_path, output_codec, fps,\n",
        "                                   (width, height))\n",
        "\n",
        "    return output_video\n",
        "\n",
        "\n",
        "# Visualize a video frame with bounding boxes, classes and confidence scores\n",
        "def visualize_detections(frame, boxes, conf_thresholds, class_ids):\n",
        "    frame_copy = np.copy(frame)\n",
        "    for idx in range(len(boxes)):\n",
        "        class_id = int(class_ids[idx])\n",
        "        conf = float(conf_thresholds[idx])\n",
        "        x1, y1, x2, y2 = (\n",
        "            int(boxes[idx][0]),\n",
        "            int(boxes[idx][1]),\n",
        "            int(boxes[idx][2]),\n",
        "            int(boxes[idx][3]),\n",
        "        )\n",
        "        color = colors[class_id]\n",
        "        label = f\"{model.names[class_id]}: {conf:.2f}\"\n",
        "        cv2.rectangle(frame_copy, (x1, y1), (x2, y2), get_color(color), 2)\n",
        "        cv2.putText(\n",
        "            frame_copy,\n",
        "            label,\n",
        "            (x1, y1 - 10),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.9,\n",
        "            get_color(color),\n",
        "            2,\n",
        "        )\n",
        "    return frame_copy\n",
        "\n",
        "\n",
        "def add_color_to_mask(mask, color):\n",
        "    next_mask = mask.astype(np.uint8)\n",
        "    next_mask = np.expand_dims(next_mask, 0).repeat(3, axis=0)\n",
        "    next_mask = np.moveaxis(next_mask, 0, -1)\n",
        "    return next_mask * color\n",
        "\n",
        "\n",
        "# Merge masks into a single, multi-colored mask\n",
        "def merge_masks_colored(masks, class_ids):\n",
        "    filtered_class_ids = []\n",
        "    filtered_masks = []\n",
        "    for idx, cid in enumerate(class_ids):\n",
        "        if int(cid) in chosen_class_ids:\n",
        "            filtered_class_ids.append(cid)\n",
        "            filtered_masks.append(masks[idx])\n",
        "\n",
        "    merged_with_colors = add_color_to_mask(\n",
        "        filtered_masks[0][0],\n",
        "        get_color(colors[int(filtered_class_ids[0])])).astype(np.uint8)\n",
        "\n",
        "    if len(filtered_masks) == 1:\n",
        "        return merged_with_colors\n",
        "\n",
        "    for i in range(1, len(filtered_masks)):\n",
        "        curr_mask_with_colors = add_color_to_mask(\n",
        "            filtered_masks[i][0], get_color(colors[int(filtered_class_ids[i])]))\n",
        "        merged_with_colors = np.bitwise_or(merged_with_colors,\n",
        "                                           curr_mask_with_colors)\n",
        "\n",
        "    return merged_with_colors.astype(np.uint8)\n",
        "\n",
        "\n",
        "def get_instance_uri(client: lb.Client, global_key, array):\n",
        "    \"\"\"Reads a numpy array into a temp Labelbox data row to-be-uploaded to Labelbox\n",
        "    Args:\n",
        "        client        :   Required (lb.Client) - Labelbox Client object\n",
        "        global_key    :   Required (str) - Data row global key\n",
        "        array         :   Required (np.ndarray) - NumPy ndarray representation of an image\n",
        "    Returns:\n",
        "        Temp Labelbox data row to-be-uploaded to Labelbox as row data\n",
        "    \"\"\"\n",
        "    # Convert array to PIL image\n",
        "    image_as_pil = PIL.Image.fromarray(array)\n",
        "    # Convert PIL image to PNG file bytes\n",
        "    image_as_bytes = BytesIO()\n",
        "    image_as_pil.save(image_as_bytes, format=\"PNG\")\n",
        "    image_as_bytes = image_as_bytes.getvalue()\n",
        "    # Convert PNG file bytes to a temporary Labelbox URL\n",
        "    url = client.upload_data(\n",
        "        content=image_as_bytes,\n",
        "        filename=f\"{uuid.uuid4()}{global_key}\",\n",
        "        content_type=\"image/jpeg\",\n",
        "        sign=True,\n",
        "    )\n",
        "    # Return the URL\n",
        "    return url\n",
        "\n",
        "\n",
        "def get_local_instance_uri(array):\n",
        "    # Convert array to PIL image\n",
        "    image_as_pil = PIL.Image.fromarray(array)\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".png\",\n",
        "                                     dir=\"/content\",\n",
        "                                     delete=False) as temp_file:\n",
        "        image_as_pil.save(temp_file)\n",
        "        file_name = temp_file.name\n",
        "\n",
        "    # Return the URL\n",
        "    return file_name\n",
        "\n",
        "\n",
        "def create_mask_frame(frame_num, instance_uri):\n",
        "    return lb_types.MaskFrame(index=frame_num, instance_uri=instance_uri)\n",
        "\n",
        "\n",
        "def create_mask_instances(class_ids):\n",
        "    instances = []\n",
        "    for cid in list(set(class_ids)):  # get unique class ids\n",
        "        if int(cid) in chosen_class_ids:\n",
        "            color = get_color(colors[int(cid)])\n",
        "            name = model.names[int(cid)]\n",
        "            instances.append(lb_types.MaskInstance(color_rgb=color, name=name))\n",
        "    return instances\n",
        "\n",
        "\n",
        "def create_video_mask_annotation(frames, instance):\n",
        "    return lb_types.VideoMaskAnnotation(frames=frames, instances=[instance])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Labelbox create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Errors: None\n",
            "Failed data rows: None\n"
          ]
        }
      ],
      "source": [
        "# Create a new dataset\n",
        "# read more here: https://docs.labelbox.com/reference/data-row-global-keys\n",
        "global_key = os.path.basename(VIDEO_PATH)\n",
        "\n",
        "asset = {\n",
        "    \"row_data\": VIDEO_PATH,\n",
        "    \"global_key\": global_key,\n",
        "    \"media_type\": \"VIDEO\"\n",
        "}\n",
        "\n",
        "dataset = client.create_dataset(name=\"yolo-sam-video-masks-dataset\")\n",
        "task = dataset.create_data_rows([asset])\n",
        "task.wait_till_done()\n",
        "\n",
        "print(f\"Errors: {task.errors}\")\n",
        "print(f\"Failed data rows: {task.failed_data_rows}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.13 torch-2.3.0 CPU\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing frame number 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing frame number 30\n",
            "Processing frame number 60\n",
            "Processing frame number 90\n",
            "Processing frame number 120\n",
            "Processing frame number 150\n",
            "Processing frame number 180\n",
            "Processing frame number 210\n",
            "Processing frame number 240\n",
            "Processing frame number 270\n",
            "Processing frame number 300\n",
            "Processing frame number 330\n",
            "Processing frame number 360\n",
            "Processing frame number 390\n",
            "Processing frame number 420\n",
            "Processing frame number 450\n",
            "Processing frame number 480\n",
            "Processing frame number 510\n",
            "Processing frame number 540\n",
            "Processing frame number 570\n",
            "Processing frame number 600\n",
            "Processing frame number 630\n",
            "Processing frame number 660\n",
            "Processing frame number 690\n",
            "Processing frame number 720\n",
            "Processing frame number 750\n"
          ]
        }
      ],
      "source": [
        "# Run through YOLOv8 on the video once quickly to get unique class ids present\n",
        "# This will inform which classes we add to the ontology\n",
        "\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "\n",
        "unique_class_ids = set()\n",
        "\n",
        "# Loop through the frames of the video\n",
        "frame_num = 1\n",
        "while cap.isOpened():\n",
        "    if frame_num % 30 == 0 or frame_num == 1:\n",
        "        print(\"Processing frame number\", frame_num)\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run frame through YOLOv8 and get class ids predicted\n",
        "    detections = model.predict(frame, conf=0.7)  # frame is a numpy array\n",
        "    for cid in detections[0].boxes.cls:\n",
        "        unique_class_ids.add(int(cid))\n",
        "    frame_num += 1\n",
        "\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0, 2, 36}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unique_class_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new ontology if you don't have one\n",
        "\n",
        "# Add all chosen classes into the ontology\n",
        "tools = []\n",
        "for cls in chosen_class_ids:\n",
        "    tools.append(\n",
        "        lb.Tool(tool=lb.Tool.Type.RASTER_SEGMENTATION, name=model.names[cls]))\n",
        "\n",
        "ontology_builder = lb.OntologyBuilder(classifications=[], tools=tools)\n",
        "\n",
        "ontology = client.create_ontology(\n",
        "    \"yolo-sam-video-masks-ontology\",\n",
        "    ontology_builder.asdict(),\n",
        ")\n",
        "\n",
        "# Or get an existing ontology by name or ID (uncomment one of the below)\n",
        "\n",
        "# ontology = client.get_ontologies(\"yolo-sam-video-masks-ontology\").get_one()\n",
        "\n",
        "# ontology = client.get_ontology(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new project if you don't have one\n",
        "\n",
        "# Project defaults to batch mode with benchmark quality settings if this argument is not provided\n",
        "# Queue mode will be deprecated once dataset mode is deprecated\n",
        "project = client.create_project(name=\"yolo-sam-video-masks-project\",\n",
        "                                media_type=lb.MediaType.Video)\n",
        "\n",
        "# Or get an existing project by ID (uncomment the below)\n",
        "\n",
        "# project = get_project(\"fill_in_project_id\")\n",
        "\n",
        "# If the project already has an ontology set up, comment out this line\n",
        "project.setup_editor(ontology)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch: <Batch {\n",
            "    \"consensus_settings_json\": \"{\\\"numberOfLabels\\\":1,\\\"coveragePercentage\\\":0}\",\n",
            "    \"created_at\": \"2024-05-17 13:47:18+00:00\",\n",
            "    \"name\": \"yolo-sam-video-masks-project\",\n",
            "    \"size\": 1,\n",
            "    \"uid\": \"fa403e10-1453-11ef-a673-ed5ddd66edf6\",\n",
            "    \"updated_at\": \"2024-05-17 13:47:18+00:00\"\n",
            "}>\n"
          ]
        }
      ],
      "source": [
        "# Create a new batch of data for the project you specified above\n",
        "\n",
        "# Uncomment if you are using `data_rows` parameter below\n",
        "# data_row_ids = client.get_data_row_ids_for_global_keys([global_key])['results']\n",
        "\n",
        "batch = project.create_batch(\n",
        "    \"yolo-sam-video-masks-project\",  # each batch in a project must have a unique name\n",
        "    # you can also specify global_keys instead of data_rows\n",
        "    global_keys=[global_key],\n",
        "    # you can also specify data_rows instead of global_keys\n",
        "    # data_rows=data_row_ids,\n",
        "    priority=1,  # priority between 1(highest) - 5(lowest)\n",
        ")\n",
        "\n",
        "print(f\"Batch: {batch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'person': 'clwaqhnhp010a072lc77th7v8'}\n"
          ]
        }
      ],
      "source": [
        "tools = ontology.tools()\n",
        "\n",
        "feature_schema_ids = dict()\n",
        "for tool in tools:\n",
        "    feature_schema_ids[tool.name] = tool.feature_schema_id\n",
        "\n",
        "print(feature_schema_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loop through each frame of video and process it\n",
        "* Run YOLOv8 and then SAM on each frame, and write visualization videos to disk\n",
        "* This might take a few minutes to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing frames 1 - 30\n",
            "No boxes found on frame 1\n",
            "No boxes found on frame 2\n",
            "No boxes found on frame 3\n",
            "No boxes found on frame 4\n",
            "No boxes found on frame 5\n",
            "No boxes found on frame 6\n",
            "No boxes found on frame 7\n",
            "No boxes found on frame 8\n",
            "No boxes found on frame 9\n",
            "No boxes found on frame 10\n",
            "No boxes found on frame 11\n",
            "No boxes found on frame 12\n",
            "No boxes found on frame 13\n",
            "No boxes found on frame 14\n",
            "No boxes found on frame 15\n",
            "No boxes found on frame 16\n",
            "No boxes found on frame 17\n",
            "No boxes found on frame 18\n",
            "No boxes found on frame 19\n",
            "No boxes found on frame 20\n",
            "No boxes found on frame 21\n",
            "No boxes found on frame 22\n",
            "No boxes found on frame 23\n",
            "No boxes found on frame 24\n",
            "No boxes found on frame 25\n",
            "No boxes found on frame 26\n",
            "No boxes found on frame 27\n",
            "No boxes found on frame 28\n",
            "No boxes found on frame 29\n",
            "Processing frames 30 - 59\n",
            "No boxes found on frame 30\n",
            "No boxes found on frame 31\n",
            "No boxes found on frame 32\n",
            "No boxes found on frame 33\n",
            "No boxes found on frame 34\n",
            "No boxes found on frame 35\n",
            "No boxes found on frame 36\n",
            "No boxes found on frame 37\n",
            "No boxes found on frame 38\n",
            "No boxes found on frame 39\n",
            "No boxes found on frame 40\n",
            "No boxes found on frame 41\n",
            "No boxes found on frame 42\n",
            "No boxes found on frame 43\n",
            "No boxes found on frame 44\n",
            "Boxes found on frame 45\n",
            "Boxes found on frame 46\n",
            "Boxes found on frame 47\n",
            "Boxes found on frame 48\n",
            "Boxes found on frame 49\n",
            "Boxes found on frame 50\n",
            "Boxes found on frame 51\n",
            "Boxes found on frame 52\n",
            "Boxes found on frame 53\n",
            "Boxes found on frame 54\n",
            "Boxes found on frame 55\n",
            "Boxes found on frame 56\n",
            "Boxes found on frame 57\n",
            "Boxes found on frame 58\n",
            "Boxes found on frame 59\n",
            "Processing frames 60 - 89\n",
            "Boxes found on frame 60\n",
            "Boxes found on frame 61\n",
            "Boxes found on frame 62\n",
            "Boxes found on frame 63\n",
            "Boxes found on frame 64\n",
            "Boxes found on frame 65\n",
            "Boxes found on frame 66\n",
            "Boxes found on frame 67\n",
            "Boxes found on frame 68\n",
            "Boxes found on frame 69\n",
            "Boxes found on frame 70\n",
            "Boxes found on frame 71\n",
            "Boxes found on frame 72\n",
            "Boxes found on frame 73\n",
            "Boxes found on frame 74\n",
            "Boxes found on frame 75\n",
            "Boxes found on frame 76\n",
            "Boxes found on frame 77\n",
            "Boxes found on frame 78\n",
            "Boxes found on frame 79\n",
            "Boxes found on frame 80\n"
          ]
        }
      ],
      "source": [
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "\n",
        "output_video_boxes = get_output_video_writer(\n",
        "    cap, \"/content/skateboarding_boxes.mp4\")\n",
        "output_video_masks = get_output_video_writer(\n",
        "    cap, \"/content/skateboarding_masks.mp4\")\n",
        "mask_frames = []\n",
        "\n",
        "# Loop through the frames of the video\n",
        "frame_num = 1\n",
        "while cap.isOpened():\n",
        "    if frame_num % 30 == 0 or frame_num == 1:\n",
        "        print(\"Processing frames\", frame_num, \"-\", frame_num + 29)\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run frame through YOLOv8 to get detections\n",
        "    detections = model.predict(frame, conf=0.7)  # frame is a numpy array\n",
        "\n",
        "    # Write detections to output video\n",
        "    frame_with_detections = visualize_detections(\n",
        "        frame,\n",
        "        detections[0].boxes.cpu().xyxy,\n",
        "        detections[0].boxes.cpu().conf,\n",
        "        detections[0].boxes.cpu().cls,\n",
        "    )\n",
        "    output_video_boxes.write(frame_with_detections)\n",
        "\n",
        "    # Run frame and detections through SAM to get masks\n",
        "    transformed_boxes = mask_predictor.transform.apply_boxes_torch(\n",
        "        detections[0].boxes.xyxy, list(get_video_dimensions(cap)))\n",
        "    if len(transformed_boxes) == 0:\n",
        "        print(\"No boxes found on frame\", frame_num)\n",
        "        output_video_masks.write(frame)\n",
        "        frame_num += 1\n",
        "        continue\n",
        "    mask_predictor.set_image(frame)\n",
        "    masks, scores, logits = mask_predictor.predict_torch(\n",
        "        boxes=transformed_boxes,\n",
        "        multimask_output=False,\n",
        "        point_coords=None,\n",
        "        point_labels=None,\n",
        "    )\n",
        "    masks = np.array(masks.cpu())\n",
        "    if masks is None or len(masks) == 0:\n",
        "        print(\"No masks found on frame\", frame_num)\n",
        "        output_video_masks.write(frame)\n",
        "        frame_num += 1\n",
        "        continue\n",
        "    merged_colored_mask = merge_masks_colored(masks, detections[0].boxes.cls)\n",
        "\n",
        "    # Write masks to output video\n",
        "    image_combined = cv2.addWeighted(frame, 0.7, merged_colored_mask, 0.7, 0)\n",
        "    output_video_masks.write(image_combined)\n",
        "\n",
        "    # Create video mask annotation for upload to Labelbox\n",
        "    instance_uri = get_instance_uri(client, global_key, merged_colored_mask)\n",
        "    mask_frame = create_mask_frame(frame_num, instance_uri)\n",
        "    mask_frames.append(mask_frame)\n",
        "    print(\"Boxes found on frame\", frame_num)\n",
        "    frame_num += 1\n",
        "\n",
        "    # For the purposes of this demo, only look at the first 80 frames\n",
        "    if frame_num > 80:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "output_video_boxes.release()\n",
        "output_video_masks.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create annotations for LB upload\n",
        "mask_instances = create_mask_instances(unique_class_ids)\n",
        "annotations = []\n",
        "for instance in mask_instances:\n",
        "    annotations.append(create_video_mask_annotation(mask_frames, instance))\n",
        "\n",
        "labels = []\n",
        "labels.append(\n",
        "    lb_types.Label(data={\"global_key\": global_key}, annotations=annotations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Upload the predictions to your specified project and data rows as pre-labels\n",
        "\n",
        "Note: This may take a few minutes, depending on size of video and number of masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Errors: []\n",
            "Status of uploads: [{'uuid': '0b0e259f-e352-48e3-b3c2-2bfa4c781d19', 'dataRow': {'id': 'clwaqgioi3tvg0749fkyc2ap8', 'globalKey': 'skateboarding.mp4'}, 'status': 'SUCCESS'}]\n"
          ]
        }
      ],
      "source": [
        "upload_job = lb.MALPredictionImport.create_from_objects(\n",
        "    client=client,\n",
        "    project_id=project.uid,\n",
        "    name=\"mal_import_job\" + str(uuid.uuid4()),\n",
        "    predictions=labels,\n",
        ")\n",
        "upload_job.wait_until_done()\n",
        "print(f\"Errors: {upload_job.errors}\",)\n",
        "print(f\"Status of uploads: {upload_job.statuses}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
