{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "<td>\n",
        "   <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/></a>\n",
        "</td>\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "<td>\n",
        "<a href=\"https://colab.research.google.com/github/Labelbox/labelbox-python/blob/develop/examples/integrations/sam/meta_sam_labelbox_video.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "<a href=\"https://github.com/Labelbox/labelbox-python/blob/develop/examples/integrations/sam/meta_sam_labelbox_video.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://img.shields.io/badge/GitHub-100000?logo=github&logoColor=white\" alt=\"GitHub\"></a>\n",
        "</td>"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Setup\n",
        "\n",
        "This notebook is used to show how to use Meta's Segment Anything model and YOLO to create masks for videos that can then be uploaded to a Labelbox project"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "### General dependencies"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "%pip install -q \"labelbox[data]\"\n",
        "%pip install -q ultralytics==8.0.20\n",
        "%pip install -q \"git+https://github.com/facebookresearch/segment-anything.git\""
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Check if in google colab\n",
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "import os\n",
        "import urllib\n",
        "import cv2\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import uuid\n",
        "import tempfile\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab.patches import cv2_imshow\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "display.clear_output()\n",
        "from IPython.display import display, Image\n",
        "from io import BytesIO\n",
        "\n",
        "# YOLOv8 dependencies\n",
        "import ultralytics\n",
        "\n",
        "ultralytics.checks()\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# SAM dependencies\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "# Labelbox dependencies\n",
        "import labelbox as lb\n",
        "import labelbox.types as lb_types"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# You can also use the Labelbox Client API to get specific videos or an entire\n",
        "# dataset from your Catalog. Refer to these docs:\n",
        "# https://labelbox-python.readthedocs.io/en/latest/#labelbox.client.Client.get_data_row\n",
        "HOME = os.getcwd()\n",
        "VIDEO_PATH = os.path.join(HOME, \"skateboarding.mp4\")\n",
        "\n",
        "if not os.path.isfile(VIDEO_PATH):\n",
        "    req = urllib.request.urlretrieve(\n",
        "        \"https://storage.googleapis.com/labelbox-datasets/image_sample_data/skateboarding.mp4\",\n",
        "        \"skateboarding.mp4\",\n",
        "    )"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### YOLOv8 setup"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Instantiate YOLOv8 model\n",
        "model = YOLO(f\"{HOME}/yolov8n.pt\")\n",
        "colors = np.random.randint(0, 256, size=(len(model.names), 3))\n",
        "\n",
        "print(model.names)\n",
        "\n",
        "# Specify which classes you care about. The rest of classes will be filtered out.\n",
        "chosen_class_ids = [0]  # person"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### SAM setup"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Download SAM model weights\n",
        "CHECKPOINT_PATH = os.path.join(HOME, \"sam_vit_h_4b8939.pth\")\n",
        "\n",
        "if not os.path.isfile(CHECKPOINT_PATH):\n",
        "    req = urllib.request.urlretrieve(\n",
        "        \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n",
        "        \"sam_vit_h_4b8939.pth\",\n",
        "    )"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Instantiate SAM model\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "sam = sam_model_registry[\"vit_h\"](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
        "mask_predictor = SamPredictor(sam)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Labelbox setup"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Add your API key\n",
        "API_KEY = None\n",
        "# To get your API key go to: Workspace settings -> API -> Create API Key\n",
        "client = lb.Client(api_key=API_KEY)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Helper functions"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Cast color to ints\n",
        "def get_color(color):\n",
        "    return (int(color[0]), int(color[1]), int(color[2]))\n",
        "\n",
        "\n",
        "# Get video dimensions\n",
        "def get_video_dimensions(input_cap):\n",
        "    width = int(input_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(input_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    return height, width\n",
        "\n",
        "\n",
        "# Get output video writer with same dimensions and fps as input video\n",
        "def get_output_video_writer(input_cap, output_path):\n",
        "    # Get the video's properties (width, height, FPS)\n",
        "    width = int(input_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(input_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(input_cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Define the output video file\n",
        "    output_codec = cv2.VideoWriter_fourcc(*\"mp4v\")  # MP4 codec\n",
        "    output_video = cv2.VideoWriter(output_path, output_codec, fps,\n",
        "                                   (width, height))\n",
        "\n",
        "    return output_video\n",
        "\n",
        "\n",
        "# Visualize a video frame with bounding boxes, classes and confidence scores\n",
        "def visualize_detections(frame, boxes, conf_thresholds, class_ids):\n",
        "    frame_copy = np.copy(frame)\n",
        "    for idx in range(len(boxes)):\n",
        "        class_id = int(class_ids[idx])\n",
        "        conf = float(conf_thresholds[idx])\n",
        "        x1, y1, x2, y2 = (\n",
        "            int(boxes[idx][0]),\n",
        "            int(boxes[idx][1]),\n",
        "            int(boxes[idx][2]),\n",
        "            int(boxes[idx][3]),\n",
        "        )\n",
        "        color = colors[class_id]\n",
        "        label = f\"{model.names[class_id]}: {conf:.2f}\"\n",
        "        cv2.rectangle(frame_copy, (x1, y1), (x2, y2), get_color(color), 2)\n",
        "        cv2.putText(\n",
        "            frame_copy,\n",
        "            label,\n",
        "            (x1, y1 - 10),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.9,\n",
        "            get_color(color),\n",
        "            2,\n",
        "        )\n",
        "    return frame_copy\n",
        "\n",
        "\n",
        "def add_color_to_mask(mask, color):\n",
        "    next_mask = mask.astype(np.uint8)\n",
        "    next_mask = np.expand_dims(next_mask, 0).repeat(3, axis=0)\n",
        "    next_mask = np.moveaxis(next_mask, 0, -1)\n",
        "    return next_mask * color\n",
        "\n",
        "\n",
        "# Merge masks into a single, multi-colored mask\n",
        "def merge_masks_colored(masks, class_ids):\n",
        "    filtered_class_ids = []\n",
        "    filtered_masks = []\n",
        "    for idx, cid in enumerate(class_ids):\n",
        "        if int(cid) in chosen_class_ids:\n",
        "            filtered_class_ids.append(cid)\n",
        "            filtered_masks.append(masks[idx])\n",
        "\n",
        "    merged_with_colors = add_color_to_mask(\n",
        "        filtered_masks[0][0],\n",
        "        get_color(colors[int(filtered_class_ids[0])])).astype(np.uint8)\n",
        "\n",
        "    if len(filtered_masks) == 1:\n",
        "        return merged_with_colors\n",
        "\n",
        "    for i in range(1, len(filtered_masks)):\n",
        "        curr_mask_with_colors = add_color_to_mask(\n",
        "            filtered_masks[i][0], get_color(colors[int(filtered_class_ids[i])]))\n",
        "        merged_with_colors = np.bitwise_or(merged_with_colors,\n",
        "                                           curr_mask_with_colors)\n",
        "\n",
        "    return merged_with_colors.astype(np.uint8)\n",
        "\n",
        "\n",
        "def get_instance_uri(client: lb.Client, global_key, array):\n",
        "    \"\"\"Reads a numpy array into a temp Labelbox data row to-be-uploaded to Labelbox\n",
        "    Args:\n",
        "        client        :   Required (lb.Client) - Labelbox Client object\n",
        "        global_key    :   Required (str) - Data row global key\n",
        "        array         :   Required (np.ndarray) - NumPy ndarray representation of an image\n",
        "    Returns:\n",
        "        Temp Labelbox data row to-be-uploaded to Labelbox as row data\n",
        "    \"\"\"\n",
        "    # Convert array to PIL image\n",
        "    image_as_pil = PIL.Image.fromarray(array)\n",
        "    # Convert PIL image to PNG file bytes\n",
        "    image_as_bytes = BytesIO()\n",
        "    image_as_pil.save(image_as_bytes, format=\"PNG\")\n",
        "    image_as_bytes = image_as_bytes.getvalue()\n",
        "    # Convert PNG file bytes to a temporary Labelbox URL\n",
        "    url = client.upload_data(\n",
        "        content=image_as_bytes,\n",
        "        filename=f\"{uuid.uuid4()}{global_key}\",\n",
        "        content_type=\"image/jpeg\",\n",
        "        sign=True,\n",
        "    )\n",
        "    # Return the URL\n",
        "    return url\n",
        "\n",
        "\n",
        "def get_local_instance_uri(array):\n",
        "    # Convert array to PIL image\n",
        "    image_as_pil = PIL.Image.fromarray(array)\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".png\",\n",
        "                                     dir=\"/content\",\n",
        "                                     delete=False) as temp_file:\n",
        "        image_as_pil.save(temp_file)\n",
        "        file_name = temp_file.name\n",
        "\n",
        "    # Return the URL\n",
        "    return file_name\n",
        "\n",
        "\n",
        "def create_mask_frame(frame_num, instance_uri):\n",
        "    return lb_types.MaskFrame(index=frame_num, instance_uri=instance_uri)\n",
        "\n",
        "\n",
        "def create_mask_instances(class_ids):\n",
        "    instances = []\n",
        "    for cid in list(set(class_ids)):  # get unique class ids\n",
        "        if int(cid) in chosen_class_ids:\n",
        "            color = get_color(colors[int(cid)])\n",
        "            name = model.names[int(cid)]\n",
        "            instances.append(lb_types.MaskInstance(color_rgb=color, name=name))\n",
        "    return instances\n",
        "\n",
        "\n",
        "def create_video_mask_annotation(frames, instance):\n",
        "    return lb_types.VideoMaskAnnotation(frames=frames, instances=[instance])"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Labelbox create dataset"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Create a new dataset\n",
        "# read more here: https://docs.labelbox.com/reference/data-row-global-keys\n",
        "global_key = os.path.basename(VIDEO_PATH)\n",
        "\n",
        "asset = {\n",
        "    \"row_data\": VIDEO_PATH,\n",
        "    \"global_key\": global_key,\n",
        "    \"media_type\": \"VIDEO\"\n",
        "}\n",
        "\n",
        "dataset = client.create_dataset(name=\"yolo-sam-video-masks-dataset\")\n",
        "task = dataset.create_data_rows([asset])\n",
        "task.wait_till_done()\n",
        "\n",
        "print(f\"Errors: {task.errors}\")\n",
        "print(f\"Failed data rows: {task.failed_data_rows}\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Run through YOLOv8 on the video once quickly to get unique class ids present\n",
        "# This will inform which classes we add to the ontology\n",
        "\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "\n",
        "unique_class_ids = set()\n",
        "\n",
        "# Loop through the frames of the video\n",
        "frame_num = 1\n",
        "while cap.isOpened():\n",
        "    if frame_num % 30 == 0 or frame_num == 1:\n",
        "        print(\"Processing frame number\", frame_num)\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run frame through YOLOv8 and get class ids predicted\n",
        "    detections = model.predict(frame, conf=0.7)  # frame is a numpy array\n",
        "    for cid in detections[0].boxes.cls:\n",
        "        unique_class_ids.add(int(cid))\n",
        "    frame_num += 1\n",
        "\n",
        "cap.release()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "unique_class_ids"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Create a new ontology if you don't have one\n",
        "\n",
        "# Add all chosen classes into the ontology\n",
        "tools = []\n",
        "for cls in chosen_class_ids:\n",
        "    tools.append(\n",
        "        lb.Tool(tool=lb.Tool.Type.RASTER_SEGMENTATION, name=model.names[cls]))\n",
        "\n",
        "ontology_builder = lb.OntologyBuilder(classifications=[], tools=tools)\n",
        "\n",
        "ontology = client.create_ontology(\n",
        "    \"yolo-sam-video-masks-ontology\",\n",
        "    ontology_builder.asdict(),\n",
        ")\n",
        "\n",
        "# Or get an existing ontology by name or ID (uncomment one of the below)\n",
        "\n",
        "# ontology = client.get_ontologies(\"yolo-sam-video-masks-ontology\").get_one()\n",
        "\n",
        "# ontology = client.get_ontology(\"\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Create a new project if you don't have one\n",
        "\n",
        "# Project defaults to batch mode with benchmark quality settings if this argument is not provided\n",
        "# Queue mode will be deprecated once dataset mode is deprecated\n",
        "project = client.create_project(name=\"yolo-sam-video-masks-project\",\n",
        "                                media_type=lb.MediaType.Video)\n",
        "\n",
        "# Or get an existing project by ID (uncomment the below)\n",
        "\n",
        "# project = get_project(\"fill_in_project_id\")\n",
        "\n",
        "# If the project already has an ontology set up, comment out this line\n",
        "project.setup_editor(ontology)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Create a new batch of data for the project you specified above\n",
        "\n",
        "# Uncomment if you are using `data_rows` parameter below\n",
        "# data_row_ids = client.get_data_row_ids_for_global_keys([global_key])['results']\n",
        "\n",
        "batch = project.create_batch(\n",
        "    \"yolo-sam-video-masks-project\",  # each batch in a project must have a unique name\n",
        "    # you can also specify global_keys instead of data_rows\n",
        "    global_keys=[global_key],\n",
        "    # you can also specify data_rows instead of global_keys\n",
        "    # data_rows=data_row_ids,\n",
        "    priority=1,  # priority between 1(highest) - 5(lowest)\n",
        ")\n",
        "\n",
        "print(f\"Batch: {batch}\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "tools = ontology.tools()\n",
        "\n",
        "feature_schema_ids = dict()\n",
        "for tool in tools:\n",
        "    feature_schema_ids[tool.name] = tool.feature_schema_id\n",
        "\n",
        "print(feature_schema_ids)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Loop through each frame of video and process it\n",
        "* Run YOLOv8 and then SAM on each frame, and write visualization videos to disk\n",
        "* This might take a few minutes to run"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "\n",
        "output_video_boxes = get_output_video_writer(\n",
        "    cap, \"/content/skateboarding_boxes.mp4\")\n",
        "output_video_masks = get_output_video_writer(\n",
        "    cap, \"/content/skateboarding_masks.mp4\")\n",
        "mask_frames = []\n",
        "\n",
        "# Loop through the frames of the video\n",
        "frame_num = 1\n",
        "while cap.isOpened():\n",
        "    if frame_num % 30 == 0 or frame_num == 1:\n",
        "        print(\"Processing frames\", frame_num, \"-\", frame_num + 29)\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run frame through YOLOv8 to get detections\n",
        "    detections = model.predict(frame, conf=0.7)  # frame is a numpy array\n",
        "\n",
        "    # Write detections to output video\n",
        "    frame_with_detections = visualize_detections(\n",
        "        frame,\n",
        "        detections[0].boxes.cpu().xyxy,\n",
        "        detections[0].boxes.cpu().conf,\n",
        "        detections[0].boxes.cpu().cls,\n",
        "    )\n",
        "    output_video_boxes.write(frame_with_detections)\n",
        "\n",
        "    # Run frame and detections through SAM to get masks\n",
        "    transformed_boxes = mask_predictor.transform.apply_boxes_torch(\n",
        "        detections[0].boxes.xyxy, list(get_video_dimensions(cap)))\n",
        "    if len(transformed_boxes) == 0:\n",
        "        print(\"No boxes found on frame\", frame_num)\n",
        "        output_video_masks.write(frame)\n",
        "        frame_num += 1\n",
        "        continue\n",
        "    mask_predictor.set_image(frame)\n",
        "    masks, scores, logits = mask_predictor.predict_torch(\n",
        "        boxes=transformed_boxes,\n",
        "        multimask_output=False,\n",
        "        point_coords=None,\n",
        "        point_labels=None,\n",
        "    )\n",
        "    masks = np.array(masks.cpu())\n",
        "    if masks is None or len(masks) == 0:\n",
        "        print(\"No masks found on frame\", frame_num)\n",
        "        output_video_masks.write(frame)\n",
        "        frame_num += 1\n",
        "        continue\n",
        "    merged_colored_mask = merge_masks_colored(masks, detections[0].boxes.cls)\n",
        "\n",
        "    # Write masks to output video\n",
        "    image_combined = cv2.addWeighted(frame, 0.7, merged_colored_mask, 0.7, 0)\n",
        "    output_video_masks.write(image_combined)\n",
        "\n",
        "    # Create video mask annotation for upload to Labelbox\n",
        "    instance_uri = get_instance_uri(client, global_key, merged_colored_mask)\n",
        "    mask_frame = create_mask_frame(frame_num, instance_uri)\n",
        "    mask_frames.append(mask_frame)\n",
        "    print(\"Boxes found on frame\", frame_num)\n",
        "    frame_num += 1\n",
        "\n",
        "    # For the purposes of this demo, only look at the first 80 frames\n",
        "    if frame_num > 80:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "output_video_boxes.release()\n",
        "output_video_masks.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Create annotations for LB upload\n",
        "mask_instances = create_mask_instances(unique_class_ids)\n",
        "annotations = []\n",
        "for instance in mask_instances:\n",
        "    annotations.append(create_video_mask_annotation(mask_frames, instance))\n",
        "\n",
        "labels = []\n",
        "labels.append(\n",
        "    lb_types.Label(data={\"global_key\": global_key}, annotations=annotations))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Upload the predictions to your specified project and data rows as pre-labels\n",
        "# Note: This may take a few minutes, depending on size of video and number of masks\n",
        "\n",
        "upload_job = lb.MALPredictionImport.create_from_objects(\n",
        "    client=client,\n",
        "    project_id=project.uid,\n",
        "    name=\"mal_import_job\" + str(uuid.uuid4()),\n",
        "    predictions=labels,\n",
        ")\n",
        "upload_job.wait_until_done()\n",
        "print(f\"Errors: {upload_job.errors}\",)\n",
        "print(f\"Status of uploads: {upload_job.statuses}\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}