{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "<td>\n",
        "   <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/></a>\n",
        "</td>"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "<td>\n",
        "<a href=\"https://colab.research.google.com/github/Labelbox/labelbox-python/blob/develop/examples/integrations/langchain/langchain.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "<a href=\"https://github.com/Labelbox/labelbox-python/tree/develop/examples/integrations/langchain/langchain.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://img.shields.io/badge/GitHub-100000?logo=github&logoColor=white\" alt=\"GitHub\"></a>\n",
        "</td>"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# LangChain -> Labelbox\n",
        "This notebook is used to show an example workflow of getting LangChain traces into Labelbox conversation data format. Please review the [associated written guide](https://labelbox.com/guides/turn-langchain-logs-into-conversational-data-with-labelbox/) for more information."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "!pip install --upgrade --quiet  langchain langsmith langchainhub\n",
        "!pip install --upgrade --quiet  langchain-openai tiktoken pandas duckduckgo-search\n",
        "!pip install --upgrade --quiet  labelbox"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "import labelbox as lb\n",
        "from uuid import uuid4\n",
        "from pprint import pprint\n",
        "from uuid import uuid4\n",
        "import os\n",
        "import functools\n",
        "from typing import List\n",
        "\n",
        "# LangSmith Imports\n",
        "from langsmith.client import Client\n",
        "from langsmith.client import Client\n",
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, AgentType, initialize_agent, load_tools\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.agents.format_scratchpad.openai_tools import (\n",
        "    format_to_openai_tool_messages,\n",
        ")\n",
        "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
        "from langchain_community.tools import DuckDuckGoSearchResults\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langsmith.evaluation import EvaluationResult\n",
        "from langsmith.schemas import Example, Run, DataType\n",
        "from langchain.smith import arun_on_dataset, run_on_dataset\n",
        "from langchain.evaluation import EvaluatorType\n",
        "from langchain.smith import RunEvalConfig\n"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## API Key and Setup\n",
        "Provide a valid API key below for Labelbox, LangSmith and OpenAI in order for the notebook to work correctly."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "\n",
        "LB_API_KEY = \"\"\n",
        "LS_API_KEY = \"\"\n",
        "OPENAI_API_KEY = \"\"\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = LS_API_KEY\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "lb_client = lb.Client(LB_API_KEY)\n",
        "client = Client()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### LangSmith Dataset Name\n",
        "Create a sample chat data set with an example chat based run"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "dataset_name = f\"Sample Dataset - {str(uuid4())}\"\n",
        "dataset = client.create_dataset(dataset_name, description=\"A sample dataset in LangSmith.\", data_type=DataType.chat)\n",
        "client.create_chat_example(\n",
        "    messages=[{\"type\": \"ai\", \"data\": {\"content\": \"hi how are you\"}}, {\"type\": \"human\", \"data\": {\"content\": \"Im doing great how about you\"}}],\n",
        "    generations={\"type\": \"ai\", \"data\": {\"content\": \"Im doing great\"}}, # Custom model output\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### LangSmith\n",
        "Below is an example of running a list of raw text evaluation strings and a LangSmith example run with Chat Gpt 3.5. Please review [LangSmith Docs](https://docs.smith.langchain.com/) for more information."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "tools = [\n",
        "    DuckDuckGoSearchResults(\n",
        "        name=\"duck_duck_go\"\n",
        "    ),  # General internet search using DuckDuckGo\n",
        "]\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo-16k\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# Since chains can be stateful (e.g. they can have memory), we provide\n",
        "# a way to initialize a new chain for each row in the dataset. This is done\n",
        "# by passing in a factory function that returns a new chain for each row.\n",
        "def create_agent(prompt, llm_with_tools):\n",
        "    runnable_agent = (\n",
        "        {\n",
        "            \"input\": lambda x: x[\"input\"],\n",
        "            \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
        "                x[\"intermediate_steps\"]\n",
        "            ),\n",
        "        }\n",
        "        | prompt\n",
        "        | llm_with_tools\n",
        "        | OpenAIToolsAgentOutputParser()\n",
        "    )\n",
        "    return AgentExecutor(agent=runnable_agent, tools=tools, handle_parsing_errors=True)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "def max_pred_length(runs, examples):\n",
        "    predictions = [len(run.outputs[\"output\"]) for run in runs]\n",
        "    return EvaluationResult(key=\"max_pred_length\", score=max(predictions))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "def check_not_idk(run: Run, example: Example):\n",
        "    \"\"\"Illustration of a custom evaluator.\"\"\"\n",
        "    agent_response = run.outputs[\"output\"]\n",
        "    if \"don't know\" in agent_response or \"not sure\" in agent_response:\n",
        "        score = 0\n",
        "    else:\n",
        "        score = 1\n",
        "    # You can access the dataset labels in example.outputs[key]\n",
        "    # You can also access the model inputs in run.inputs[key]\n",
        "    return EvaluationResult(\n",
        "        key=\"not_uncertain\",\n",
        "        score=score,\n",
        "    )"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "evaluation_config = RunEvalConfig(\n",
        "    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator\n",
        "    evaluators=[\n",
        "        check_not_idk,\n",
        "        # Measures whether a QA response is \"Correct\", based on a reference answer\n",
        "        # You can also select via the raw string \"qa\"\n",
        "        EvaluatorType.QA,\n",
        "        # Measure the embedding distance between the output and the reference answer\n",
        "        # Equivalent to: EvalConfig.EmbeddingDistance(embeddings=OpenAIEmbeddings())\n",
        "        EvaluatorType.EMBEDDING_DISTANCE,\n",
        "        # Grade whether the output satisfies the stated criteria.\n",
        "        # You can select a default one such as \"helpfulness\" or provide your own.\n",
        "        RunEvalConfig.LabeledCriteria(\"helpfulness\"),\n",
        "        # The LabeledScoreString evaluator outputs a score on a scale from 1-10.\n",
        "        # You can use default criteria or write our own rubric\n",
        "        RunEvalConfig.LabeledScoreString(\n",
        "            {\n",
        "                \"accuracy\": \"\"\"\n",
        "Score 1: The answer is completely unrelated to the reference.\n",
        "Score 3: The answer has minor relevance but does not align with the reference.\n",
        "Score 5: The answer has moderate relevance but contains inaccuracies.\n",
        "Score 7: The answer aligns with the reference but has minor errors or omissions.\n",
        "Score 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\"\n",
        "            },\n",
        "            normalize_by=10,\n",
        "        ),\n",
        "    ],\n",
        "    batch_evaluators=[max_pred_length],\n",
        ")\n",
        "\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "prompt = hub.pull(\"gabe/labelboxtutorialdemo\") # Change prompt in LangSmith hub to reflect example run\n",
        "\n",
        "chain_results = run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=functools.partial(\n",
        "        create_agent, prompt=prompt, llm_with_tools=llm_with_tools\n",
        "    ),\n",
        "    evaluation=evaluation_config,\n",
        "    verbose=True,\n",
        "    client=client,\n",
        "    project_name=f\"tools-agent-test-5d466cbc-{unique_id}\",\n",
        "    # Project metadata communicates the experiment parameters,\n",
        "    # Useful for reviewing the test results\n",
        "    project_metadata={\n",
        "        \"env\": \"testing-notebook\",\n",
        "        \"model\": \"gpt-3.5-turbo\",\n",
        "        \"prompt\": \"5d466cbc\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Sometimes, the agent will error due to parsing issues, incompatible tool inputs, etc.\n",
        "# These are logged as warnings here and captured as errors in the tracing UI."
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Labelbox\n",
        "Below converts the results of the above LangSmith run to Labelbox conversation text. Please review [Labelbox conversation data docs](https://docs.labelbox.com/reference/text-conversational) for more information."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "def import_conversational (chain_results: dict[str:str], user_id_dict: dict[str:dict[str:str]], output_user_name:str = \"ai\") -> dict[str:str]:\n",
        "  '''Converts LangSmith chain_results from model invocation to Labelbox conversation data. Output is based on popular model response to chat message.\n",
        "  \n",
        "  Args:\n",
        "    chain_results(dict[str:str]): Results from LangSmith model invocation against example dataset runs.\n",
        "    user_id_dict(dict[str:dict[str:str]]): Dictionary mapping of LangSmith example run type to Labelbox chat names and alignment.\n",
        "    output_user_name(str): Name of the LangSmith output agent from model invocation. Defaults to \"ai\". \n",
        "\n",
        "  Returns:\n",
        "      dict[str:str]: Labelbox conversation text format\n",
        "  '''\n",
        "  lb_conversations = []\n",
        "  message_counter = 0\n",
        "  for conversational in chain_results[\"results\"].values():\n",
        "    message_counter += 1\n",
        "    lb_conversation =   {\n",
        "    \"row_data\": {\n",
        "      \"type\": \"application/vnd.labelbox.conversational\",\n",
        "      \"version\": 1,\n",
        "      \"messages\": []\n",
        "    },\n",
        "    \"global_key\": str(uuid4()),\n",
        "    \"media_type\": \"CONVERSATIONAL\",\n",
        "    }\n",
        "    if \"input\" in conversational[\"output\"]:\n",
        "      for input in conversational[\"output\"][\"input\"]:\n",
        "        lb_conversation[\"row_data\"][\"messages\"].append({\n",
        "            \"content\": input[\"data\"][\"content\"],\n",
        "            \"user\": {\n",
        "                \"userId\": user_id_dict[input[\"type\"]][\"id\"],\n",
        "                \"name\": input[\"type\"]\n",
        "            },\n",
        "            \"canLabel\": True,\n",
        "            \"align\": user_id_dict[input[\"type\"]][\"align\"],\n",
        "            \"messageId\": str(uuid4())\n",
        "        })\n",
        "    if \"output\" in conversational[\"output\"]:\n",
        "      output = conversational[\"output\"][\"output\"]\n",
        "      lb_conversation[\"row_data\"][\"messages\"].append({\n",
        "          \"content\": output,\n",
        "          \"user\": {\n",
        "              \"userId\": user_id_dict[output_user_name][\"id\"],\n",
        "              \"name\": output_user_name\n",
        "          },\n",
        "          \"canLabel\": True,\n",
        "          \"align\": user_id_dict[output_user_name][\"align\"],\n",
        "          \"messageId\": str(uuid4())\n",
        "      })\n",
        "    lb_conversations.append(lb_conversation)\n",
        "  return lb_conversations"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Create Labelbox Dataset"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "dataset = lb_client.create_dataset(name=\"test_langchain\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Attach Conversation Text to Dataset"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "task = dataset.create_data_rows(import_conversational(chain_results, {\"human\": {\"id\":\"human\",\"align\": \"right\"}, \"ai\":{\"id\":\"ai\", \"align\": \"left\"}}, \"ai\"))\n",
        "task.wait_till_done()\n",
        "\n",
        "print(task.errors)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Cleanup"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# dataset.delete()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}