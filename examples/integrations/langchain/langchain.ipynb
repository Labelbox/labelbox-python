{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "<td>",
        "   <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/></a>",
        "</td>\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "<td>\n",
        "<a href=\"https://colab.research.google.com/github/Labelbox/labelbox-python/blob/develop/examples/integrations/langchain/langchain.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "<a href=\"https://github.com/Labelbox/labelbox-python/tree/develop/examples/integrations/langchain/langchain.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://img.shields.io/badge/GitHub-100000?logo=github&logoColor=white\" alt=\"GitHub\"></a>\n",
        "</td>"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# LangChain -> Labelbox\n",
        "This notebook is used to show an example workflow of getting LangChain traces into Labelbox conversation data format. Please review the [associated written guide](https://labelbox.com/guides/turn-langchain-logs-into-conversational-data-with-labelbox/) for more information."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "%pip install --upgrade --quiet  langchain langsmith langchainhub\n%pip install --upgrade --quiet  langchain-openai tiktoken pandas duckduckgo-search\n%pip install --upgrade --quiet  \"labelbox[data]\"",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": "import labelbox as lb\nfrom uuid import uuid4\nimport os\nimport functools\n\n# LangSmith Imports\nfrom langsmith.client import Client\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor\nfrom langchain.agents.format_scratchpad.openai_tools import (\n    format_to_openai_tool_messages,)\nfrom langchain.agents.output_parsers.openai_tools import (\n    OpenAIToolsAgentOutputParser,)\nfrom langchain_community.tools import DuckDuckGoSearchResults\nfrom langchain_openai import ChatOpenAI\nfrom langsmith.evaluation import EvaluationResult\nfrom langsmith.schemas import Example, Run, DataType\nfrom langchain.smith import run_on_dataset\nfrom langchain.evaluation import EvaluatorType\nfrom langchain.smith import RunEvalConfig",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## API Key and Setup\n",
        "Provide a valid API key below for Labelbox, LangSmith and OpenAI in order for the notebook to work correctly."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "LB_API_KEY = \"\"\nLS_API_KEY = \"\"\nOPENAI_API_KEY = \"\"\n\nunique_id = uuid4().hex[0:8]\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\nos.environ[\"LANGCHAIN_API_KEY\"] = LS_API_KEY\n\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n\nlb_client = lb.Client(LB_API_KEY)\nclient = Client()",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### LangSmith Dataset Name\n",
        "Create a sample chat data set with an example chat based run"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "dataset_name = f\"Sample Dataset - {str(uuid4())}\"\ndataset = client.create_dataset(\n    dataset_name,\n    description=\"A sample dataset in LangSmith.\",\n    data_type=DataType.chat,\n)\nclient.create_chat_example(\n    messages=[\n        {\n            \"type\": \"ai\",\n            \"data\": {\n                \"content\": \"hi how are you\"\n            }\n        },\n        {\n            \"type\": \"human\",\n            \"data\": {\n                \"content\": \"Im doing great how about you\"\n            }\n        },\n    ],\n    generations={\n        \"type\": \"ai\",\n        \"data\": {\n            \"content\": \"Im doing great\"\n        },\n    },  # Custom model output\n    dataset_id=dataset.id,\n)",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### LangSmith\n",
        "Below is an example of running a list of raw text evaluation strings and a LangSmith example run with Chat Gpt 3.5. Please review [LangSmith Docs](https://docs.smith.langchain.com/) for more information."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "tools = [\n    DuckDuckGoSearchResults(\n        name=\"duck_duck_go\"),  # General internet search using DuckDuckGo\n]\n\nllm = ChatOpenAI(\n    model=\"gpt-3.5-turbo-16k\",\n    temperature=0,\n)\n\n\n# Since chains can be stateful (e.g. they can have memory), we provide\n# a way to initialize a new chain for each row in the dataset. This is done\n# by passing in a factory function that returns a new chain for each row.\ndef create_agent(prompt, llm_with_tools):\n    runnable_agent = ({\n        \"input\":\n            lambda x: x[\"input\"],\n        \"agent_scratchpad\":\n            lambda x: format_to_openai_tool_messages(x[\"intermediate_steps\"]),\n    } | prompt | llm_with_tools | OpenAIToolsAgentOutputParser())\n    return AgentExecutor(agent=runnable_agent,\n                         tools=tools,\n                         handle_parsing_errors=True)",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": "def max_pred_length(runs, examples):\n    predictions = [len(run.outputs[\"output\"]) for run in runs]\n    return EvaluationResult(key=\"max_pred_length\", score=max(predictions))",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": "def check_not_idk(run: Run, example: Example):\n    \"\"\"Illustration of a custom evaluator.\"\"\"\n    agent_response = run.outputs[\"output\"]\n    if \"don't know\" in agent_response or \"not sure\" in agent_response:\n        score = 0\n    else:\n        score = 1\n    # You can access the dataset labels in example.outputs[key]\n    # You can also access the model inputs in run.inputs[key]\n    return EvaluationResult(\n        key=\"not_uncertain\",\n        score=score,\n    )",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": "evaluation_config = RunEvalConfig(\n    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator\n    evaluators=[\n        check_not_idk,\n        # Measures whether a QA response is \"Correct\", based on a reference answer\n        # You can also select via the raw string \"qa\"\n        EvaluatorType.QA,\n        # Measure the embedding distance between the output and the reference answer\n        # Equivalent to: EvalConfig.EmbeddingDistance(embeddings=OpenAIEmbeddings())\n        EvaluatorType.EMBEDDING_DISTANCE,\n        # Grade whether the output satisfies the stated criteria.\n        # You can select a default one such as \"helpfulness\" or provide your own.\n        RunEvalConfig.LabeledCriteria(\"helpfulness\"),\n        # The LabeledScoreString evaluator outputs a score on a scale from 1-10.\n        # You can use default criteria or write our own rubric\n        RunEvalConfig.LabeledScoreString(\n            {\n                \"accuracy\":\n                    \"\"\"\nScore 1: The answer is completely unrelated to the reference.\nScore 3: The answer has minor relevance but does not align with the reference.\nScore 5: The answer has moderate relevance but contains inaccuracies.\nScore 7: The answer aligns with the reference but has minor errors or omissions.\nScore 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\"\n            },\n            normalize_by=10,\n        ),\n    ],\n    batch_evaluators=[max_pred_length],\n)\n\nllm_with_tools = llm.bind_tools(tools)\nprompt = hub.pull(\"gabe/labelboxtutorialdemo\"\n                 )  # Change prompt in LangSmith hub to reflect example run\n\nchain_results = run_on_dataset(\n    dataset_name=dataset_name,\n    llm_or_chain_factory=functools.partial(create_agent,\n                                           prompt=prompt,\n                                           llm_with_tools=llm_with_tools),\n    evaluation=evaluation_config,\n    verbose=True,\n    client=client,\n    project_name=f\"tools-agent-test-5d466cbc-{unique_id}\",\n    # Project metadata communicates the experiment parameters,\n    # Useful for reviewing the test results\n    project_metadata={\n        \"env\": \"testing-notebook\",\n        \"model\": \"gpt-3.5-turbo\",\n        \"prompt\": \"5d466cbc\",\n    },\n)\n\n# Sometimes, the agent will error due to parsing issues, incompatible tool inputs, etc.\n# These are logged as warnings here and captured as errors in the tracing UI.",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Labelbox\n",
        "Below converts the results of the above LangSmith run to Labelbox conversation text. Please review [Labelbox conversation data docs](https://docs.labelbox.com/docs/llm-human-preference) for more information."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "def import_conversational(\n        chain_results: dict[str:str],\n        user_id_dict: dict[str:dict[str:str]]) -> dict[str:str]:\n    \"\"\"Converts LangSmith chain_results from model invocation to Labelbox conversation data for model response comparison. Output is based on popular model response and custom model response towards prompts.\n\n    Args:\n      chain_results(dict[str:str]): Results from LangSmith model invocation against example dataset runs.\n      user_id_dict(dict[str:dict[str:str]]): Dictionary mapping of LangSmith example run type to Labelbox chat names and alignment.\n\n    Returns:\n        dict[str:str]: Labelbox conversation text format\n    \"\"\"\n    lb_conversations = []\n    for key, conversational in chain_results[\"results\"].items():\n        lb_conversation = {\n            \"row_data\": {\n                \"type\": \"application/vnd.labelbox.conversational\",\n                \"version\": 1,\n                \"messages\": [],\n                \"modelOutputs\": [],\n            },\n            \"global_key\": key,\n            \"media_type\": \"CONVERSATIONAL\",\n        }\n        if \"input\" in conversational[\"output\"]:\n            for i, input in enumerate(conversational[\"output\"][\"input\"]):\n                lb_conversation[\"row_data\"][\"messages\"].append({\n                    \"content\": input[\"data\"][\"content\"],\n                    \"timestampUsec\": i + 1,\n                    \"user\": {\n                        \"userId\": user_id_dict[input[\"type\"]][\"id\"],\n                        \"name\": input[\"type\"],\n                    },\n                    \"canLabel\": True,\n                    \"align\": user_id_dict[input[\"type\"]][\"align\"],\n                    \"messageId\": str(uuid4()),\n                })\n\n        # Custom model output\n        if \"reference\" in conversational:\n            reference = conversational[\"reference\"][\"output\"]\n            lb_conversation[\"row_data\"][\"modelOutputs\"].append({\n                \"title\": \"Custom Model Response\",\n                \"content\": reference[\"data\"][\"content\"],\n                \"modelConfigName\": \"Custom Model - Example Config\",\n            })\n\n        # Popular model output\n        if \"output\" in conversational[\"output\"]:\n            output = conversational[\"output\"][\"output\"]\n            lb_conversation[\"row_data\"][\"modelOutputs\"].append({\n                \"title\": \"Popular LLM Response\",\n                \"content\": output,\n                \"modelConfigName\": \"GPT-3.5 - Example Config\",\n            })\n\n        lb_conversations.append(lb_conversation)\n    return lb_conversations",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Create Labelbox Dataset"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "dataset = lb_client.create_dataset(name=\"demo_langchain\")",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Attach Conversation Text to Dataset"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "task = dataset.create_data_rows(\n    import_conversational(\n        chain_results,\n        {\n            \"human\": {\n                \"id\": \"human\",\n                \"align\": \"right\"\n            },\n            \"ai\": {\n                \"id\": \"ai\",\n                \"align\": \"left\"\n            },\n        },\n    ))\ntask.wait_till_done()\n\nprint(task.errors)",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Cleanup"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "# dataset.delete()",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}