{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "!pip install git+https://github.com/huggingface/transformers -q\n",
        "!pip install accelerate -q\n",
        "!pip install -q 'labelbox[data]' -q\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from labelbox.schema.ontology import OntologyBuilder\n",
        "from labelbox import Client, MALPredictionImport\n",
        "from labelbox.data.annotation_types import (\n",
        "    Label, ImageData, ClassificationAnnotation, Text\n",
        ")\n",
        "import uuid\n",
        "import torch\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "import json\n",
        "import labelbox"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Enter your API KEY below. Link to [how to create](https://docs.labelbox.com/reference/create-api-key) API KEY"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "MYAPI = \"API KEY\"\n",
        "client = Client(MYAPI)  "
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Build the ontology used to create project"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "ontology_builder = labelbox.OntologyBuilder(\n",
        "    classifications=[labelbox.Classification(\n",
        "        class_type=labelbox.Classification.Type.TEXT,\n",
        "        name=\"BLIP model prediction\"  # name of object\n",
        "    ), labelbox.Classification(\n",
        "        class_type=labelbox.Classification.Type.TEXT,\n",
        "        name=\"Human caption\"  # name of object\n",
        "    )]\n",
        ")\n",
        "\n",
        "ontology = client.create_ontology(\"BLIP\", ontology_builder.asdict(), media_type=labelbox.MediaType.Image)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Create project and attach the ontology"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "project = client.create_project(name = \"BLIP Pre label\", media_type=labelbox.MediaType.Image)\n",
        "project.setup_editor(ontology)\n",
        "ontology_from_project = labelbox.OntologyBuilder.from_project(project)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Export the Datarow IDs from datset so that they can be attached to the project "
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Set the export params to include/exclude certain fields. Make sure each of these fields are correctly grabbed \n",
        "export_params= {\n",
        "    \"attachments\": True,\n",
        "    \"metadata_fields\": True,\n",
        "    \"data_row_details\": True,\n",
        "    \"project_details\": True,\n",
        "    \"performance_details\": True\n",
        "  \n",
        "}\n",
        "\n",
        "# You can set the range for last_activity_at\n",
        "# For context, last_activity_at captures the creation and modification of labels, metadata, status, comments and reviews.\n",
        "# Note: This is an AND logic between the filters, so usually using one filter is sufficient.\n",
        "\n",
        "filters= {\n",
        "            \"last_activity_at\": [\"2000-01-01 00:00:00\", \"2050-01-01 00:00:00\"]\n",
        "}\n",
        "\n",
        "dataset = client.get_dataset(\"Dataset ID\")\n",
        "export_task = dataset.export_v2(params=export_params, filters=filters)\n",
        "export_task.wait_till_done()\n",
        "if export_task.errors:\n",
        "  print(export_task.errors)\n",
        "export_result = export_task.result\n",
        "print(\"results: \", json.dumps(export_result[:3], indent = 4))\n",
        "data_row_ids = [dr[\"data_row\"][\"id\"] for dr in export_result[:100]]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Attach batch to the project"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "batch = project.create_batch(\n",
        "  \"Adding assets\", # name of the batch\n",
        "  data_row_ids, # list of Data Rows\n",
        "  1 # priority between 1-5\n",
        ")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "#Initialize and load a pre-trained BLIP-2 model\n",
        "\n",
        "If a GPU is available, the model will be moved to the GPU to take advantage of its parallel processing capabilities, which can significantly speed up computations. If a GPU is not available, the model will run on the CPU (Central Processing Unit) instead."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16\n",
        ")\n",
        "model.to(device)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Example of the image and the output of the model"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "url = 'https://storage.googleapis.com/sfr-vision-language-research/LAVIS/assets/merlion.png' \n",
        "image = Image.open(requests.get(url, stream=True).raw).convert('RGB')   \n",
        "display(image.resize((596, 437)))\n",
        "inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "print(generated_text)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Collect inferences to be used as prelabels"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "queued_data_rows = project.export_queued_data_rows()\n",
        "ground_truth_list = list()\n",
        "\n",
        "for data_row in queued_data_rows:\n",
        "  url = data_row[\"rowData\"]\n",
        "  image = Image.open(requests.get(url, stream=True).raw)\n",
        "  inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "  generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
        "  generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "  \n",
        "  text_annotation = labelbox.data.annotation_types.ClassificationAnnotation(\n",
        "      name=\"BLIP model prediction\", \n",
        "      value=labelbox.data.annotation_types.Text(answer = generated_text)\n",
        "    )\n",
        "  \n",
        "  ground_truth_list.append(Label(\n",
        "        data= ImageData(uid = data_row[\"id\"]), annotations = [text_annotation]\n",
        "    ))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "#Upload prelabels to project "
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "  upload_task = labelbox.MALPredictionImport.create_from_objects(client, project.uid, str(uuid.uuid4()), ground_truth_list)\n",
        "  upload_task.wait_until_done()\n",
        "  print(upload_task.errors)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}