{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "<td>",
        "   <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/></a>",
        "</td>\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "<td>\n",
        "<a href=\"https://colab.research.google.com/github/Labelbox/labelbox-python/blob/develop/examples/exports/exporting_to_CSV.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "<a href=\"https://github.com/Labelbox/labelbox-python/tree/develop/examples/exports/exporting_to_CSV.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://img.shields.io/badge/GitHub-100000?logo=github&logoColor=white\" alt=\"GitHub\"></a>\n",
        "</td>"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Export to CSV or Pandas format\n",
        "\n",
        "This notebook serves as a simplified How-To guide and provides examples of converting Labelbox export JSON to a CSV and [Pandas](https://github.com/Labelbox/labelpandas) friendly format.  "
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Advance approach\n",
        "\n",
        "For a more abstracted approach please visit our [LabelPandas](https://github.com/Labelbox/labelpandas) library. You can use this library to abstract the steps that are about to be shown. In addition, this library support importing CSV data. \n",
        "\n",
        "We strongly encourage collaboration - please free to fork this repo and tweak the code base to work for you own data, and make pull requests if you have suggestions on how to enhance the overall experience, add new features, or improve general performance."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Set up"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "%pip install -q --upgrade \"Labelbox[data]\"\n%pip install -q pandas",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": "import labelbox as lb\nimport labelbox.types as lb_types\nimport uuid\nfrom pprint import pprint\nimport csv\nimport pandas as pd",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## API key and client\n",
        "Provide a valid API key below to connect to the Labelbox client properly. For more information, please review the [Create API Key](https://docs.labelbox.com/reference/create-api-key) guide."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VySWQiOiJjbHAxZ2w2OGIwMDBkMDc3eGRrcnI5azhmIiwib3JnYW5pemF0aW9uSWQiOiJjbDVibjhxdnExYXY5MDd4dGIzYnA4cTYwIiwiYXBpS2V5SWQiOiJjbHgyMDk4bmYwM3Q1MDd6MjBueXI5N2x2Iiwic2VjcmV0IjoiYzhhYzlkZDJmZTgyZTcwNmM4MzQ4MzZhNDIwMWVjZjEiLCJpYXQiOjE3MTc2MDI2MjYsImV4cCI6MjM0ODc1NDYyNn0.JxwtcWEOjS_sush7FhH2m7KZylPWUOYPcxO3z8Hg058\"\nclient = lb.Client(api_key=API_KEY)",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Create or select example project\n",
        "\n",
        "The below steps will setup a project that can be used for this demo. Please feel free to comment out the below code block and uncomment the code block to receive your own project directly. For more information on this set up visit our quick start guide."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "### Create Project"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "# Create dataset with image data row\nglobal_key = str(uuid.uuid4())\n\ntest_img_url = {\n    \"row_data\":\n        \"https://storage.googleapis.com/labelbox-datasets/image_sample_data/2560px-Kitano_Street_Kobe01s5s4110.jpeg\",\n    \"global_key\":\n        global_key,\n}\n\ndataset = client.create_dataset(name=\"image-demo-dataset\")\ntask = dataset.create_data_rows([test_img_url])\ntask.wait_till_done()\nprint(\"Errors:\", task.errors)\nprint(\"Failed data rows:\", task.failed_data_rows)\n\n# Create ontology\nontology_builder = lb.OntologyBuilder(\n    classifications=[  # List of Classification objects\n        lb.Classification(\n            class_type=lb.Classification.Type.RADIO,\n            name=\"radio_question\",\n            options=[\n                lb.Option(value=\"first_radio_answer\"),\n                lb.Option(value=\"second_radio_answer\"),\n            ],\n        ),\n        lb.Classification(\n            class_type=lb.Classification.Type.CHECKLIST,\n            name=\"checklist_question\",\n            options=[\n                lb.Option(value=\"first_checklist_answer\"),\n                lb.Option(value=\"second_checklist_answer\"),\n            ],\n        ),\n        lb.Classification(class_type=lb.Classification.Type.TEXT,\n                          name=\"free_text\"),\n        lb.Classification(\n            class_type=lb.Classification.Type.RADIO,\n            name=\"nested_radio_question\",\n            options=[\n                lb.Option(\n                    \"first_radio_answer\",\n                    options=[\n                        lb.Classification(\n                            class_type=lb.Classification.Type.RADIO,\n                            name=\"sub_radio_question\",\n                            options=[lb.Option(\"first_sub_radio_answer\")],\n                        )\n                    ],\n                )\n            ],\n        ),\n    ],\n    tools=[  # List of Tool objects\n        lb.Tool(tool=lb.Tool.Type.BBOX, name=\"bounding_box\"),\n        lb.Tool(\n            tool=lb.Tool.Type.BBOX,\n            name=\"bbox_with_radio_subclass\",\n            classifications=[\n                lb.Classification(\n                    class_type=lb.Classification.Type.RADIO,\n                    name=\"sub_radio_question\",\n                    options=[lb.Option(value=\"tool_first_sub_radio_answer\")],\n                ),\n            ],\n        ),\n    ],\n)\n\nontology = client.create_ontology(\n    \"Image CSV Demo Ontology\",\n    ontology_builder.asdict(),\n    media_type=lb.MediaType.Image,\n)\n\n# Set up project and connect ontology\nproject = client.create_project(name=\"Image Annotation Import Demo\",\n                                media_type=lb.MediaType.Image)\nproject.setup_editor(ontology)\n\n# Send data row towards our project\nbatch = project.create_batch(\n    \"image-demo-batch\",\n    global_keys=[\n        global_key\n    ],  # paginated collection of data row objects, list of data row ids or global keys\n    priority=1,\n)\n\nprint(f\"Batch: {batch}\")\n\n# Create a label and imported it towards our project\nradio_annotation = lb_types.ClassificationAnnotation(\n    name=\"radio_question\",\n    value=lb_types.Radio(answer=lb_types.ClassificationAnswer(\n        name=\"second_radio_answer\")),\n)\nchecklist_annotation = lb_types.ClassificationAnnotation(\n    name=\"checklist_question\",\n    value=lb_types.Checklist(answer=[\n        lb_types.ClassificationAnswer(name=\"first_checklist_answer\"),\n        lb_types.ClassificationAnswer(name=\"second_checklist_answer\"),\n    ]),\n)\ntext_annotation = lb_types.ClassificationAnnotation(\n    name=\"free_text\",\n    value=lb_types.Text(answer=\"sample text\"),\n)\nnested_radio_annotation = lb_types.ClassificationAnnotation(\n    name=\"nested_radio_question\",\n    value=lb_types.Radio(answer=lb_types.ClassificationAnswer(\n        name=\"first_radio_answer\",\n        classifications=[\n            lb_types.ClassificationAnnotation(\n                name=\"sub_radio_question\",\n                value=lb_types.Radio(answer=lb_types.ClassificationAnswer(\n                    name=\"first_sub_radio_answer\")),\n            )\n        ],\n    )),\n)\nbbox_annotation = lb_types.ObjectAnnotation(\n    name=\"bounding_box\",\n    value=lb_types.Rectangle(\n        start=lb_types.Point(x=1690, y=977),\n        end=lb_types.Point(x=1915, y=1307),\n    ),\n)\nbbox_with_radio_subclass_annotation = lb_types.ObjectAnnotation(\n    name=\"bbox_with_radio_subclass\",\n    value=lb_types.Rectangle(\n        start=lb_types.Point(x=541, y=933),  # x = left, y = top\n        end=lb_types.Point(x=871, y=1124),  # x= left + width , y = top + height\n    ),\n    classifications=[\n        lb_types.ClassificationAnnotation(\n            name=\"sub_radio_question\",\n            value=lb_types.Radio(answer=lb_types.ClassificationAnswer(\n                name=\"tool_first_sub_radio_answer\")),\n        )\n    ],\n)\n\nlabel = []\nannotations = [\n    radio_annotation,\n    nested_radio_annotation,\n    checklist_annotation,\n    text_annotation,\n    bbox_annotation,\n    bbox_with_radio_subclass_annotation,\n]\n\nlabel.append(\n    lb_types.Label(data={\"global_key\": global_key}, annotations=annotations))\n\nupload_job = lb.LabelImport.create_from_objects(\n    client=client,\n    project_id=project.uid,\n    name=\"label_import_job\" + str(uuid.uuid4()),\n    labels=label,\n)\n\nupload_job.wait_until_done()\nprint(\"Errors:\", upload_job.errors)\nprint(\"Status of uploads: \", upload_job.statuses)",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Select project"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "# PROJECT_ID = None\n# project = client.get_project(PROJECT_ID)",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## CSV format overview\n",
        "\n",
        "In order to convert our Labelbox JSON data to a format more CSV friendly, we must first define the needed structure of our JSON. A common format that is versatile for both the built-in Python CSV writer and Pandas is as follows: \n",
        "\n",
        "```python\n",
        "[\n",
        "    {\"<column_1>\":\"<answer_1\", \"<column_2>\":\"<answer_2\" ..},\n",
        "    \n",
        "    {\"<column_1>\":\"<answer_1\", \"<column_2>\":\"<answer_2\" ..},\n",
        "    ..\n",
        "]\n",
        "```\n",
        "\n",
        "Essentially, we need to get our JSON data towards a list of Python dictionaries with each Python dictionary representing one row, each key representing a column and each value being an individual cell of our CSV table. Once we have our data to this format it is trivial to create Pandas DataFrames or write our CSV file. The tricky part is getting Labelbox export JSON towards this format."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Labelbox JSON format\n",
        "\n",
        "Labelbox JSON format is centralized at the individual data row of your export. This format allows expandability when things evolve and provides a centralized view of fields such as metadata or data row details. The main labels are located inside the projects key and can be nested which can make it difficult to parse. For complete samples of our project export format visit our export quick reference page. \n",
        "\n",
        "To get Labelbox export JSON format to our CSV format we established, we must do the following:\n",
        "\n",
        "1. Establish our base data row columns (project_id, data_row_id, global_key etc)\n",
        "2. Create our columns for label fields (label detail and annotations we care about)\n",
        "3. Define our functions and strategy used to parse through our data\n",
        "4. Setting up our main data row handler function\n",
        "5. Export our data\n",
        "6. Convert to our desired format"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Step 1: Establish our base columns\n",
        "\n",
        "We first establish our base columns that represent individual data row details. Typically, this columns information can be received from within one or two levels of a Labelbox export per data row. \n",
        "\n",
        "Please feel free to modify the below columns if you want to include more. You will need to update the code later in this guide to pick up any additional columns."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "data_row_base_columns = [\n    \"Data Row ID\",\n    \"Global Key\",\n    \"External ID\",\n    \"Project ID\",\n]",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Step 2: Create our columns for label fields\n",
        "\n",
        "In this step, we define our label details base columns we want to include in our CSV. In this case, we will use the following:"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "label_base_columns = [\"Label ID\", \"Created By\", \"Skipped\"]",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "We then need to establish the annotations we want to include in our columns. The order of our list matters since that is the order of what our columns will be presented. You can approach getting the annotations in a list in a number of ways including hard defining the columns. We will be doing a mapping between `feature_schema_Id` and the our column name. The reason introduce this mapping is the annotation name can be the same in certain situations but `feature_schema_ids` are completely unique. In the code below, I will be recursively going through the ontology we created to get our `feature_schema_ids` and column names based on the name of the features. In the next step of this guide we will provide more information on recursion in context of parsing through JSON or Python dictionaries."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "def get_classification_features(classifications: list, class_list=[]) -> None:\n    \"\"\"Finds classification features inside an ontology recursively and returns them in a list\"\"\"\n    for classification in classifications:\n        if \"name\" in classification:\n            class_list.append({\n                \"feature_schema_id\": classification[\"featureSchemaId\"],\n                \"column_name\": classification[\"instructions\"],\n            })\n        if \"options\" in classification:\n            get_classification_features(classification[\"options\"], class_list)\n    return class_list\n\n\ndef get_tool_features(tools: list) -> None:\n    \"\"\"Creates list of tool names from ontology\"\"\"\n    tool_list = []\n    for tool in tools:\n        tool_list.append({\n            \"feature_schema_id\": tool[\"featureSchemaId\"],\n            \"column_name\": tool[\"name\"],\n        })\n        if \"classifications\" in tool:\n            tool_list = get_classification_features(tool[\"classifications\"],\n                                                    tool_list)\n    return tool_list",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": "# Get ontology from project and normalized towards python dictionary\nontology = project.ontology().normalized\n\nclass_annotation_columns = get_classification_features(\n    ontology[\"classifications\"])\ntool_annotation_columns = get_tool_features(ontology[\"tools\"])\n\npprint(class_annotation_columns)",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Step 3: Define our functions and strategy used to parse through our data\n",
        "\n",
        "Now that we have our columns defined we need to come up with a strategy of navigating our export data. Review this [sample export](https://docs.labelbox.com/reference/export-image-annotations#sample-project-export) to follow along. While creating our columns it is always best to first check if a key exists in your data row before populating a column this is especially import for optional fields. In this demo, we will populate the value `None` for anything not present which will result in a blank cell our our CSV.\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "### Data row detail base columns\n",
        "The data row details can be access within a depth of one or two keys. Below is a function we will use the access the columns we defined. The parameters are the data row itself, dictionary row that will be used to make our list and our base columns list."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "def get_base_data_row_columns(data_row: dict[str:str], csv_row: dict[str:str],\n                              base_columns: list[str]) -> dict[str:str]:\n    for base_column in base_columns:\n        if base_column == \"Data Row ID\":\n            csv_row[base_column] = data_row[\"data_row\"][\"id\"]\n\n        elif base_column == \"Global Key\":\n            if (\"global_key\"\n                    in data_row[\"data_row\"]):  # Check if global key exists\n                csv_row[base_column] = data_row[\"data_row\"][\"global_key\"]\n            else:\n                csv_row[base_column] = (\n                    None  # If global key does not exist on data row set cell to None. This will create a blank cell on your csv\n                )\n\n        elif base_column == \"External ID\":\n            if (\"external_id\"\n                    in data_row[\"data_row\"]):  # Check if external_id exists\n                csv_row[base_column] = data_row[\"data_row\"][\"external_id\"]\n            else:\n                csv_row[base_column] = (\n                    None  # If external id does not exist on data row set cell to None. This will create a blank cell on your csv\n                )\n\n        elif base_column == \"Project ID\":\n            csv_row[base_column] = project.uid\n    return csv_row",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Label detail base columns\n",
        "The label details are similar to data row details but they exist at a label level of our export. Later in the guide we will demonstrate how to get our exported data row at this level. The function below shows the process of obtaining the details we defined above. The parameters are the label, the dictionary row that we will be modifying and the label detail column list we created."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "def get_base_label_columns(label: dict[str:str], csv_row: dict[str:str],\n                           label_base_columns: list[str]) -> dict[str:str]:\n    for label_base_column in label_base_columns:\n        if label_base_column == \"Label ID\":\n            csv_row[label_base_column] = label[\"id\"]\n\n        elif label_base_columns == \"Created By\":\n            if (\n                    \"label_details\" in label\n            ):  # Check if label details is present. This field can be omitted in export\n                csv_row[label_base_column] = label_base_columns[\n                    \"label_details\"][\"created_by\"]\n            else:\n                csv_row[label_base_column] = None\n\n        elif label_base_column == \"Skipped\":\n            if (\n                    \"performance_details\" in label\n            ):  # Check if performance details are present. This field can be omitted in export.\n                csv_row[label_base_column] = label[\"performance_details\"][\n                    \"skipped\"]\n            else:\n                csv_row[label_base_column] = None\n\n    return csv_row",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Label annotation columns\n",
        "The label annotations are the final columns we will need to obtain. The approach of obtaining these fields are more challenging then the approach we made for our detail columns. If we attempt to obtain the fields with conditional statements and hard defined paths we will run into issues as each label can have annotations in different orders, annotations can be at different depths or not present at all. This will quickly create a mess especially when we want our methods to work for more then one ontology. The best and cleanest way of obtaining these annotations inside our export data is through recursive function.\n",
        "\n",
        "#### Recursion\n",
        "A recursive function can be defined as a routine that calls itself directly or indirectly. They solve a problems by solving smaller instances of the same problem. This technique is commonly used in programming to solve problems that can be broken down into simpler, similar subproblem. Our sub problem in this case is obtaining the each individual annotations. A recursive function is divided into two components:\n",
        "\n",
        "- **Base case:** This is a termination condition that prevents the function from calling itself indefinitely.\n",
        "\n",
        "- **Recursive case:** In the recursive case, the function calls itself with the modified arguments. The recursive case should move closer to the base case with each iteration.\n",
        "\n",
        "For our example, our base case will be either the annotation exists on the label (return the value/answer) or it does not (return `None`). Our recursive case would be finding more classifications to parse.\n",
        "\n",
        "In the below code block, I will highlight a few important details inside our function. Essentially, we will be navigating through our JSON file by moving one classification key at a time until we find our annotation or if everything has been searched returning `None` which will populate a blank cell on our CSV table. \n",
        "\n",
        "#### Tools\n",
        "Tools are not nested but they can have nested classifications we will use or `get_feature_answers` function below to find the nested classification. Since tool are at the base level of a label and each tool has a different value key name we will only be searching for bounding boxes for this tutorial. If you wanted to include other tools reference our export guide for your data type and find the appropriate key to add on."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "from pprint import pprint\n\n\ndef get_feature_answers(feature: str,\n                        annotations: list[dict[str:str]]) -> None | str:\n    \"\"\"Returns answer of feature provided by navigating through a label's annotation list. Will return None if answer is not found.\n\n    Args:\n        feature (str): feature we are searching\n        classifications (list[dict[str:str]]): annotation list we are looking for the feature\n\n    Returns:\n        None | str: The answer/value of the feature returns None if nothing is found\n    \"\"\"\n    for annotation in annotations:\n        print(annotation)\n        if (annotation[\"feature_schema_id\"] == feature[\"feature_schema_id\"]\n           ):  # Base conditions (found feature)\n            if \"text_answer\" in annotation:\n                return annotation[\"text_answer\"][\"content\"]\n            if \"radio_answer\" in annotation:\n                return annotation[\"radio_answer\"][\"value\"]\n            if \"checklist_answers\" in annotation:\n                # Since classifications can have more then one answer. This is set up to combine all classifications separated by a comma. Feel free to modify.\n                return \", \".join([\n                    check_list_ans[\"value\"]\n                    for check_list_ans in annotation[\"checklist_answers\"]\n                ])\n            if \"bounding_box\" in annotation:\n                return annotation[\"bounding_box\"]\n            # Add more tools here with similar pattern as above\n\n        # Recursion cases (found more classifications to search through)\n        if \"radio_answer\" in annotation:\n            if len(annotation[\"radio_answer\"][\"classifications\"]) > 0:\n                value = get_feature_answers(\n                    feature, annotation[\"radio_answer\"][\"classifications\"]\n                )  # Call function again return value if answer found\n                if value:\n                    return value\n        if \"checklist_answers\" in annotation:\n            for checklist_ans in annotation[\"checklist_answers\"]:\n                if len(checklist_ans[\"classifications\"]) > 0:\n                    value = get_feature_answers(\n                        feature, checklist_ans[\"classifications\"])\n                    if value:\n                        return value\n        if (\"classifications\"\n                in annotation):  # case for if tool has classifications\n            if len(annotation[\"classifications\"]) > 0:\n                value = get_feature_answers(feature,\n                                            annotation[\"classifications\"])\n                if value:\n                    return value\n\n    return None  # Base case if searched through classifications and nothing was found (end of JSON). This can be omitted but included to visualize",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Step 4: Setting up our main data row handler function\n",
        "Before we can start exporting we need to set up our main data row handler. This function will be feed straight into our export. This function will put everything together and connect all the pieces. We will also be defining our global dictionary list that will be used to create our CSVs. The output parameter represents each data row."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "GLOBAL_CSV_LIST = []\n\n\ndef main(output: lb.BufferedJsonConverterOutput):\n\n    # Navigate to our label list\n    labels = output.json[\"projects\"][project.uid][\"labels\"]\n    for label in labels:\n        # Define our CSV \"row\"\n        csv_row = dict()\n\n        # Start with data row base columns\n        csv_row = get_base_data_row_columns(output.json, csv_row,\n                                            data_row_base_columns)\n\n        # Add our label details\n        csv_row = get_base_label_columns(label, csv_row, label_base_columns)\n\n        pprint(label)\n        # Add classification features\n        for classification in class_annotation_columns:\n            csv_row[classification[\"column_name\"]] = get_feature_answers(\n                classification, label[\"annotations\"][\"classifications\"])\n\n        pprint(tool_annotation_columns)\n        # Add tools features\n        for tool in tool_annotation_columns:\n            csv_row[tool[\"column_name\"]] = get_feature_answers(\n                tool, label[\"annotations\"][\"objects\"])\n\n        # Append to global csv list\n        GLOBAL_CSV_LIST.append(csv_row)",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Step 5: Export our data\n",
        "Now that we have defined functions and strategies, we are ready to export. Below we are exporting directly from our project and feeding in our main function we created above."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "# Params required to obtain all fields we need\nparams = {\"performance_details\": True, \"label_details\": True}\n\nexport_task = project.export(params=params)\nexport_task.wait_till_done()\n\n# Conditional for if export task has errors\nif export_task.has_errors():\n    export_task.get_buffered_stream(stream_type=lb.StreamType.ERRORS).start(\n        stream_handler=lambda error: print(error))\n\nif export_task.has_result():\n    export_json = export_task.get_buffered_stream(\n        stream_type=lb.StreamType.RESULT).start(\n            stream_handler=main\n        )  # Feeding our data row handler directly into export",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "If everything went through correctly you should see your `GLOBAL_CSV_LIST` printed out below with all your \"rows\" filled out."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "pprint(GLOBAL_CSV_LIST)",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Step 6: Convert to desired format\n",
        "\n",
        "The hard part is now completed!\ud83d\ude80 Now that your have your export in a flatten format you can now easily convert to a CSV or a Pandas DataFrame!"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "### Option A: CSV writer"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "with open(\"file.csv\", \"w\", newline=\"\") as csvfile:\n    # Columns\n    fieldnames = (data_row_base_columns + label_base_columns +\n                  [name[\"column_name\"] for name in class_annotation_columns] +\n                  [name[\"column_name\"] for name in tool_annotation_columns])\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n    writer.writeheader()\n\n    for row in GLOBAL_CSV_LIST:\n        writer.writerow(row)",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Option B: Pandas DataFrame"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": "columns = (data_row_base_columns + label_base_columns +\n           [name[\"column_name\"] for name in class_annotation_columns] +\n           [name[\"column_name\"] for name in tool_annotation_columns])\npd.DataFrame(GLOBAL_CSV_LIST, columns=columns)",
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}