{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "under-fishing",
   "metadata": {
    "id": "EyNkbpW7ouEf"
   },
   "source": [
    "  \n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/></a>\n",
    "  </td>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-nickel",
   "metadata": {},
   "source": [
    "----\n",
    "# Model Diagnostics Guide\n",
    "\n",
    "\n",
    "Throughout the process of training your machine learning (ML) model, you may want to investigate your model's failures in order to understand which areas need improvement. Looking at an error analysis after each training iteration can help you understand whether you need to revise your annotations, make your ontology more clear, or create more training data that targets a specific area.\n",
    "Labelbox now offers a Model Diagnostics tool that analyzes the performance of your model's predictions in a single interface.\n",
    "With Model Diagnostics, you can:\n",
    "*   Inspect model behavior across experiments\n",
    "*   Adjust model hyperparameters and visualize model failures\n",
    "*   Use the Python SDK to create the analysis pipeline\n",
    "\n",
    "## How it works\n",
    "\n",
    "Configuring Model Diagnostics is all done via the SDK. We have created a Google colab notebook to demonstrate this process. The notebook also includes a section that leverages MAL in order to quickly create ground truth annotations.\n",
    "An Experiment is a specific instance of a model generating output in the form of predictions.\n",
    "In Labelbox, the `Model` object represents your ML model and it is what you'll be performing experiments on. It references a set of annotations specified by an ontology. \n",
    "The `Model Run` object represents the experiment itself. It is a specific instance of a `Model` with preconfigured hyperparameters (training data). You can upload inferences across each `Model Run`, filter by IoU score, and compare your model's predictions against the annotations from your training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cPUzODPjf0r",
   "metadata": {
    "id": "1cPUzODPjf0r"
   },
   "source": [
    "## Not ready to try with your model\n",
    "\n",
    "For an end-to-end example with an existing dataset check out this [notebook](https://colab.research.google.com/drive/1ZHCd0rWqsX4_sNaOq_ZQkdrHKEWAsrnU)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-court",
   "metadata": {
    "id": "stupid-court"
   },
   "source": [
    "# Steps\n",
    "1. Select a project\n",
    "2. Exports labels\n",
    "3. Upload labels and predictions for Diagnostics\n",
    "------ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-magic",
   "metadata": {
    "id": "subsequent-magic"
   },
   "source": [
    "## Environment Setup\n",
    "* Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-minister",
   "metadata": {
    "id": "voluntary-minister"
   },
   "outputs": [],
   "source": [
    "!pip install \"labelbox[data]==3.0.0rc0\" \\\n",
    "             scikit-image \\\n",
    "             tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-worship",
   "metadata": {
    "id": "wooden-worship"
   },
   "outputs": [],
   "source": [
    "# Run these if running in a colab notebook\n",
    "COLAB = \"google.colab\" in str(get_ipython())\n",
    "if COLAB:\n",
    "    !git clone https://github.com/Labelbox/labelbox-python.git\n",
    "    !cd labelbox-python\n",
    "    !mv labelbox-python/examples/model_assisted_labeling/*.py ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-leone",
   "metadata": {
    "id": "latter-leone"
   },
   "source": [
    "* Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "committed-richards",
   "metadata": {
    "id": "committed-richards"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-03 20:33:19.683087: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-08-03 20:33:19.683279: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import numpy as np\n",
    "from tqdm import notebook\n",
    "import csv\n",
    "import ndjson\n",
    "import os\n",
    "\n",
    "from labelbox.schema.ontology import OntologyBuilder, Tool\n",
    "from labelbox import Client, LabelingFrontend, MALPredictionImport\n",
    "from labelbox.data.metrics.iou import data_row_miou\n",
    "from labelbox.data.metrics.iou import data_row_miou\n",
    "from labelbox.data.serialization import NDJsonConverter\n",
    "from labelbox.data.annotation_types import (\n",
    "    ScalarMetric, \n",
    "    LabelList, \n",
    "    Label, \n",
    "    RasterData, \n",
    "    Mask, \n",
    "    Point, \n",
    "    Rectangle, \n",
    "    ObjectAnnotation\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    from image_model import predict, load_model, class_mappings\n",
    "except ModuleNotFoundError: \n",
    "    # !git clone https://github.com/Labelbox/labelbox-python.git\n",
    "    # !cd labelbox-python && git checkout mea-dev\n",
    "    # !mv labelbox-python/examples/model_assisted_labeling/*.py .\n",
    "    raise Exception(\"You will need to run from the labelbox-python git repo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-promotion",
   "metadata": {
    "id": "alternate-promotion"
   },
   "source": [
    "* Configure client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "economic-chase",
   "metadata": {
    "id": "economic-chase"
   },
   "outputs": [],
   "source": [
    "API_KEY = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affecting-myanmar",
   "metadata": {
    "id": "affecting-myanmar"
   },
   "outputs": [],
   "source": [
    "API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VySWQiOiJja2s0cTF2Z3djMHZwMDcwNHhoeDdtNHZrIiwib3JnYW5pemF0aW9uSWQiOiJja2s0cTF2Z2Fwc2F1MDczMjRhd25zanEyIiwiYXBpS2V5SWQiOiJja3J3bWNmZXQwa2N6MHkyYzh4Z3E1NHhoIiwic2VjcmV0IjoiZGM0ZTEwM2E1ZTQ2YzRiOGFkZWU2ZmMxMGM2ZTAwMTkiLCJpYXQiOjE2MjgwMjg4NTQsImV4cCI6MjI1OTE4MDg1NH0.-zl_aqbd0IoCRsKFHps0HzNhGOUFaVt6bb24AUVj28k\"\n",
    "client = Client(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-venture",
   "metadata": {
    "id": "blessed-venture"
   },
   "source": [
    "## Select a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "SzMRhPV4J5Bb",
   "metadata": {
    "id": "SzMRhPV4J5Bb"
   },
   "outputs": [],
   "source": [
    "project_id = \"ckrwr3agx1g2w0y0x9vpg7xnp\" #None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "qF_pOaoDhINR",
   "metadata": {
    "id": "qF_pOaoDhINR"
   },
   "outputs": [],
   "source": [
    "project = client.get_project(project_id)\n",
    "ontology = project.ontology()\n",
    "schema_lookup = {tool.name: tool for tool in ontology.tools()}\n",
    "schema_id_lookup = {tool.feature_schema_id: tool for tool in ontology.tools()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hwtnJRmMJlPO",
   "metadata": {
    "id": "hwtnJRmMJlPO"
   },
   "source": [
    "## Export Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1kbmVPSxJaHw",
   "metadata": {
    "id": "1kbmVPSxJaHw"
   },
   "outputs": [],
   "source": [
    "MAX_LABELS = 2000\n",
    "# we have a limit of 2000 labels\n",
    "labels = [l for idx, l in enumerate(project.label_generator()) if idx < MAX_LABELS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-burden",
   "metadata": {
    "id": "dated-burden"
   },
   "source": [
    "## Create Predictions\n",
    "\n",
    "* The prediction format is the same \n",
    "* Loop over data_rows, make predictions, and create ndjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-savings",
   "metadata": {
    "id": "asian-savings"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* Note that if you are using your own model you will want to change the logic for constructing the different \n",
    "types..\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "predictions = LabelList([])\n",
    "for label in notebook.tqdm(labels):\n",
    "    image = label.data\n",
    "    height, width = image.data.shape[:2]\n",
    "\n",
    "    ### --- replace (start) --- ###\n",
    "    # Build a list of annotation objects from your model inferences\n",
    "    \n",
    "    prediction = predict(np.array([image.im_bytes]), min_score=0.5, height=height, width = width)\n",
    "    boxes, classes, seg_masks = prediction[\"boxes\"], prediction[\"class_indices\"], prediction[\"seg_masks\"]\n",
    "    annotations = []\n",
    "    for box, class_idx, seg in zip(boxes, classes, seg_masks):\n",
    "        if class_idx in class_mappings:\n",
    "            class_info = class_mappings.get(class_idx)\n",
    "            if class_info['kind'] == Tool.Type.POLYGON:\n",
    "                value = Polygon(points = [Point(x = x, y = y) for x,y in np.roll(pts, 1, axis=-1)])\n",
    "            elif class_info['kind'] == Tool.Type.BBOX:\n",
    "                value = Rectangle(start = Point(x = box[1], y = box[0]), end = Point(x=box[3], y=box[2]))\n",
    "            elif class_info['kind'] == Tool.Type.POINT:\n",
    "                value = Point(x=(box[1] + box[3]) / 2., y = (box[0] + box[2]) / 2.)\n",
    "            elif class_info['kind'] == Tool.Type.SEGMENTATION:\n",
    "                value = Mask(mask = Raster(arr = seg), color = ())\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported kind found. {class_info['kind']}\")\n",
    "            annotations.append(ObjectAnnotation(name = class_info['name'], value = value))\n",
    "            \n",
    "    ### --- replace (end) --- ###    \n",
    "    predictions.append(Label(data = image, annotations = annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-catering",
   "metadata": {
    "id": "smoking-catering"
   },
   "source": [
    "## MEA\n",
    "\n",
    "Throughout the process of training your machine learning (ML) model, you may want to investigate your modelâ€™s failures in order to understand which areas need improvement. Looking at an error analysis after each training iteration can help you understand whether you need to revise your annotations, make your ontology more clear, or create more training data that targets a specific area.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Create a model\n",
    "    * Think of this as a model that you want to perform experiments on\n",
    "2. Create a model run\n",
    "    * Think of this as a single experiment for a particular model.\n",
    "    * E.g. this model run is for an instance of a model with particular hyperparameters\n",
    "3. Select the ground truth annotations for analysis\n",
    "4. Compute metrics\n",
    "4. Upload model predictions and metrics to labelbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-minnesota",
   "metadata": {
    "id": "mental-minnesota"
   },
   "outputs": [],
   "source": [
    "lb_model = client.create_model(name = f\"{project.name}-model\", ontology_id = project.ontology().uid)\n",
    "lb_model_run = lb_model.create_model_run(\"0.0.0\")\n",
    "lb_model_run.upsert_labels([label.uid for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-fairy",
   "metadata": {
    "id": "committed-fairy"
   },
   "outputs": [],
   "source": [
    "label_lookup = {label.data.uid : label for label in labels}\n",
    "\n",
    "for pred in predictions:\n",
    "    label = label_lookup.get(pred.data.uid)\n",
    "    if label is None:\n",
    "        # No label for the prediction..\n",
    "        continue\n",
    "\n",
    "    score = data_row_miou(label, pred)\n",
    "    if score is None:\n",
    "        continue\n",
    "        \n",
    "    pred.annotations.append(\n",
    "        ScalarMetric(value = score)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-addition",
   "metadata": {
    "id": "anonymous-addition"
   },
   "outputs": [],
   "source": [
    "upload_task = lb_model_run.add_predictions(f'mea-import-{uuid.uuid4()}', predictions + metric_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imAIXxCV_fG_",
   "metadata": {
    "id": "imAIXxCV_fG_"
   },
   "outputs": [],
   "source": [
    "upload_task.wait_until_done()\n",
    "upload_task.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-printing",
   "metadata": {},
   "source": [
    "### Open Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, annotation_group in enumerate(lb_model_run.annotation_groups()):\n",
    "    if idx == 5:\n",
    "        break\n",
    "    print(annotation_group.url)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model Diagnostics Guide",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
