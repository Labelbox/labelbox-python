{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "<td>\n",
        "   <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/></a>\n",
        "</td>"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "<td>\n",
        "<a href=\"https://colab.research.google.com/github/Labelbox/labelbox-python/blob/master/examples/model_experiments/model_diagnostics/model_diagnostics_guide.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "<a href=\"https://github.com/Labelbox/labelbox-python/tree/master/examples/model_experiments/model_diagnostics/model_diagnostics_guide.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://img.shields.io/badge/GitHub-100000?logo=github&logoColor=white\" alt=\"GitHub\"></a>\n",
        "</td>"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "\n",
        "# Model Diagnostics Guide\n",
        "\n",
        "\n",
        "Throughout the process of training your machine learning (ML) model, you may want to investigate your model's failures in order to understand which areas need improvement. Looking at an error analysis after each training iteration can help you understand whether you need to revise your annotations, make your ontology more clear, or create more training data that targets a specific area.\n",
        "Labelbox now offers a Model Diagnostics tool that analyzes the performance of your model's predictions in a single interface.\n",
        "With Model Diagnostics, you can:\n",
        "*   Inspect model behavior across experiments\n",
        "*   Adjust model hyperparameters and visualize model failures\n",
        "*   Use the Python SDK to create the analysis pipeline\n",
        "\n",
        "## How it works\n",
        "\n",
        "Configuring Model Diagnostics is all done via the SDK. We have created a Google colab notebook to demonstrate this process. The notebook also includes a section that leverages MAL in order to quickly create ground truth annotations.\n",
        "An Experiment is a specific instance of a model generating output in the form of predictions.\n",
        "In Labelbox, the `Model` object represents your ML model and it is what you'll be performing experiments on. It references a set of annotations specified by an ontology. \n",
        "The `Model Run` object represents the experiment itself. It is a specific instance of a `Model` with preconfigured hyperparameters (training data). You can upload inferences across each `Model Run`, filter by IoU score, and compare your model's predictions against the annotations from your training data."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Not ready to try with your model\n",
        "\n",
        "For an end-to-end example with an existing dataset check out this [notebook](model_diagnostics_demo.ipynb)\n",
        "\n",
        "\n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Steps\n",
        "1. Make sure you are signed up for the beta. If not navigate here https://labelbox.com/product/model-diagnostics\n",
        "2. Select a project\n",
        "3. Exports labels\n",
        "4. Upload labels and predictions for Diagnostics\n",
        "------ "
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "* Install dependencies"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "!pip install \"labelbox[data]\""
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Run these if running in a colab notebook\n",
        "COLAB = \"google.colab\" in str(get_ipython())"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "* Import libraries"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "import uuid\n",
        "\n",
        "from tqdm import notebook\n",
        "\n",
        "import labelbox as lb\n",
        "import labelbox.types as lb_types\n",
        "from labelbox.data.metrics import feature_miou_metric, feature_confusion_matrix_metric\n",
        "from labelbox.data.metrics.group import get_label_pairs"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "* Configure client"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "API Key\n",
        "\n",
        "Provide a valid api key below in order to properly connect to the Labelbox Client."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Add your api key\n",
        "API_KEY = None\n",
        "client = lb.Client(api_key=API_KEY)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Select a project"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "project_id = None"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "project = client.get_project(project_id)\n",
        "ontology = project.ontology()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Export Labels"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "MAX_LABELS = 2000\n",
        "# we have a limit of 2000 labels\n",
        "labels = [\n",
        "    l for idx, l in enumerate(project.label_generator()) if idx < MAX_LABELS\n",
        "]"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Create Predictions\n",
        "* Loop over data_rows, make predictions, and annotation types"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "predictions = list()\n",
        "for label in notebook.tqdm(labels):\n",
        "    annotations = []\n",
        "    image = label.data\n",
        "\n",
        "    ### --- replace (start) --- ###\n",
        "\n",
        "    # Build a list of annotation objects from your model inferences\n",
        "    # image.value is just the numpy array representation of the image\n",
        "    prediction = model.predict(image.value)\n",
        "    # Iterate of segmentation channels or instances depending on your model architecture ( or both )\n",
        "    for instance, (xmin, ymin, xmax, ymax), seg, class_idx in prediction:\n",
        "        class_name = class_names.get(class_idx)\n",
        "\n",
        "        # Construct the right annotation value (pick one of the following) and append to list of annotations\n",
        "        # See annotation types notebooks for more on how to construct these objects\n",
        "        # https://github.com/Labelbox/labelbox-python/tree/master/examples/annotation_types\n",
        "\n",
        "        value = lb_types.Polygon(points=[lb_types.Point(x=x, y=y) for x, y in instance])\n",
        "        value = lb_types.Rectangle(start=lb_types.Point(x=xmin, y=ymin),\n",
        "                          end=lb_types.Point(x=xmax, y=ymax))\n",
        "        value = lb_types.Point(x=x, y=y)\n",
        "        value = lb_types.Line(points=[lb_types.Point(x=x, y=y) for x, y in instance])\n",
        "        value = lb_types.Mask(mask=lb_types.MaskData.from_2D_arr(seg * grayscale_color),\n",
        "                     color=(grayscale_color,) * 3)\n",
        "\n",
        "        annotations.append(lb_types.ObjectAnnotation(name=class_name, value=value))\n",
        "\n",
        "    ### --- replace (end) --- ###\n",
        "    predictions.append(lb_types.Label(data=image, annotations=annotations))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Prepare for upload\n",
        "* Add any mising urls or references to labelbox (data row ids)"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "signer = lambda _bytes: client.upload_data(content=_bytes, sign=True)\n",
        "predictions.add_url_to_masks(signer)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## MEA\n",
        "\n",
        "Throughout the process of training your machine learning (ML) model, you may want to investigate your model\u2019s failures in order to understand which areas need improvement. Looking at an error analysis after each training iteration can help you understand whether you need to revise your annotations, make your ontology more clear, or create more training data that targets a specific area.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. Create a model\n",
        "    * Think of this as a model that you want to perform experiments on\n",
        "2. Create a model run\n",
        "    * Think of this as a single experiment for a particular model.\n",
        "    * E.g. this model run is for an instance of a model with particular hyperparameters\n",
        "3. Select the ground truth annotations for analysis\n",
        "4. Compute metrics\n",
        "4. Upload model predictions and metrics to labelbox"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "lb_model = client.create_model(name=f\"{project.name}-model\",\n",
        "                               ontology_id=project.ontology().uid)\n",
        "lb_model_run_hyperparameters = {\"batch_size\": 1000}\n",
        "lb_model_run = lb_model.create_model_run(\"0.0.0\", lb_model_run_hyperparameters)\n",
        "lb_model_run.upsert_labels([label.uid for label in labels])"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "pairs = get_label_pairs(labels, predictions, filter_mismatch=True)\n",
        "for (ground_truth, prediction) in pairs.values():\n",
        "    metrics = []\n",
        "    metrics.extend(\n",
        "        feature_miou_metric(ground_truth.annotations, prediction.annotations))\n",
        "    metrics.extend(\n",
        "        feature_confusion_matrix_metric(ground_truth.annotations,\n",
        "                                        prediction.annotations))\n",
        "    prediction.annotations.extend(metrics)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "upload_task = lb_model_run.add_predictions(\n",
        "    f'mea-import-{uuid.uuid4()}', predictions)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "upload_task.wait_until_done()\n",
        "upload_task.state"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "### Open Model Run"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "for idx, model_run_data_row in enumerate(lb_model_run.model_run_data_rows()):\n",
        "    if idx == 5:\n",
        "        break\n",
        "    print(model_run_data_row.url)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}