{
  "nbformat": 4,
  "nbformat_minor": 1,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "# Getting started \n",
        "## Importing a labeled dataset\n",
        "---"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "!pip3 install labelbox[data]\n",
        "import labelbox\n",
        "from labelbox.schema.ontology import OntologyBuilder, Tool, Classification,Option\n",
        "from labelbox.schema.annotation_import import MALPredictionImport\n",
        "from labelbox.schema.annotation_import import LabelImport\n",
        "from labelbox.schema.queue_mode import QueueMode\n",
        "from labelbox.schema.media_type import MediaType\n",
        "from labelbox import LabelingFrontend\n",
        "from labelbox.data.annotation_types import (\n",
        "    Label,\n",
        "    Point,\n",
        "    LabelList,\n",
        "    ImageData,\n",
        "    Rectangle,\n",
        "    ObjectAnnotation,\n",
        ")\n",
        "from labelbox.schema.data_row_metadata import (\n",
        "    DataRowMetadata,\n",
        "    DataRowMetadataField,\n",
        "    DeleteDataRowMetadata,\n",
        "    DataRowMetadataKind\n",
        ")\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import datetime\n",
        "import random"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "## Generic data download function\n",
        "def download_files(filemap):\n",
        "    path, uri = filemap    \n",
        "    ## Download data\n",
        "    if not os.path.exists(path):\n",
        "        r = requests.get(uri, stream=True)\n",
        "        if r.status_code == 200:\n",
        "            with open(path, 'wb') as f:\n",
        "                for chunk in r:\n",
        "                    f.write(chunk)\n",
        "    return path"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Setup Labelbox client"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "## Generate API key: https://app.labelbox.com/account/api-keys\n",
        "LB_API_KEY = None\n",
        "client = labelbox.Client(LB_API_KEY)\n",
        "\n",
        "DATA_ROWS = \"https://storage.googleapis.com/labelbox-datasets/VHR_geospatial/geospatial_datarows.json\"\n",
        "ANNOTATIONS = \"https://storage.googleapis.com/labelbox-datasets/VHR_geospatial/geospatial_annotations.json\""
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Download a public dataset"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "download_files((\"data_rows.json\", DATA_ROWS))\n",
        "download_files((\"annotations.json\", ANNOTATIONS))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "with open('data_rows.json', 'r') as fp:\n",
        "    data_rows = json.load(fp)\n",
        "\n",
        "with open('annotations.json', 'r') as fp:\n",
        "    annotations = json.load(fp)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Create a dataset"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "dataset = client.create_dataset(name=\"Geospatial vessel detection\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Import Data Rows with Metadata"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "# Here is an example of adding two metadata fields to your Data Rows: a \"captureDateTime\" field with datetime value, and a \"tag\" field with string value\n",
        "metadata_ontology = client.get_data_row_metadata_ontology()\n",
        "datetime_schema_id = metadata_ontology.reserved_by_name[\"captureDateTime\"].uid\n",
        "tag_schema_id = metadata_ontology.reserved_by_name[\"tag\"].uid\n",
        "tag_items = [\"WorldView-1\", \"WorldView-2\", \"WorldView-3\", \"WorldView-4\"]\n",
        "\n",
        "for datarow in tqdm(data_rows):\n",
        "    dt = datetime.datetime.utcnow() + datetime.timedelta(days=random.random()*30) # this is random datetime value\n",
        "    tag_item = random.choice(tag_items) # this is a random tag value\n",
        "\n",
        "    # Option 1: Specify metadata with a list of DataRowMetadataField. This is the recommended option since it comes with validation for metadata fields.\n",
        "    metadata_fields = [\n",
        "                       DataRowMetadataField(schema_id=datetime_schema_id, value=dt), \n",
        "                       DataRowMetadataField(schema_id=tag_schema_id, value=tag_item)\n",
        "                       ]\n",
        "\n",
        "    # Option 2: Uncomment to try. Alternatively, you can specify the metadata fields with dictionary format without declaring the DataRowMetadataField objects. It is equivalent to Option 1.\n",
        "    # metadata_fields = [\n",
        "    #                    {\"schema_id\": datetime_schema_id, \"value\": dt}, \n",
        "    #                    {\"schema_id\": tag_schema_id, \"value\": tag_item}\n",
        "    #                    ]\n",
        "\n",
        "    datarow[\"metadata_fields\"] = metadata_fields"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "task = dataset.create_data_rows(data_rows)\n",
        "task.wait_till_done()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "Examine a Data Row"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "datarow = next(dataset.data_rows())\n",
        "print(datarow)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Setup a labeling project"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "ontology = OntologyBuilder()\n",
        "\n",
        "for tool in annotations['categories']:\n",
        "  print(tool['name'])\n",
        "  ontology.add_tool(Tool(tool = Tool.Type.BBOX, name = tool['name']))\n",
        "\n",
        "ontology = client.create_ontology(\"Vessel detection ontology\", ontology.asdict())\n",
        "project = client.create_project(name=\"Vessel detection\", media_type=MediaType.Image)\n",
        "project.setup_editor(ontology)\n",
        "ontology_from_project = OntologyBuilder.from_project(project)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "Prepare and queue batch of Data Rows to the project"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "data_rows = [dr.uid for dr in list(dataset.export_data_rows())]\n",
        "\n",
        "# Randomly select 200 Data Rows\n",
        "sampled_data_rows = random.sample(data_rows, 200)\n",
        "\n",
        "batch = project.create_batch(\n",
        "  \"Initial batch\", # name of the batch\n",
        "  sampled_data_rows, # list of Data Rows\n",
        "  1 # priority between 1-5\n",
        ")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Process ground truth annotations for import"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "queued_data_rows = project.export_queued_data_rows()\n",
        "ground_truth_list = LabelList()\n",
        "\n",
        "for datarow in queued_data_rows:\n",
        "  annotations_list = []\n",
        "  folder = datarow['externalId'].split(\"/\")[0]\n",
        "  id = datarow['externalId'].split(\"/\")[1]\n",
        "  if folder == \"positive_image_set\":\n",
        "    for image in annotations['images']:\n",
        "      if (image['file_name']==id):\n",
        "        for annotation in annotations['annotations']:\n",
        "          if annotation['image_id'] == image['id']:\n",
        "            bbox = annotation['bbox']\n",
        "            id = annotation['category_id'] - 1\n",
        "            class_name = ontology_from_project.tools[id].name\n",
        "            annotations_list.append(ObjectAnnotation(\n",
        "                name = class_name,\n",
        "                value = Rectangle(start = Point(x = bbox[0], y = bbox[1]), end = Point(x = bbox[2]+bbox[0], y = bbox[3]+bbox[1])),\n",
        "            ))\n",
        "  image = ImageData(uid = datarow['id'])\n",
        "  ground_truth_list.append(Label(data = image, annotations = annotations_list))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Import ground truth annotation"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {},
      "source": [
        "start_time = time.time()\n",
        "## Upload annotations\n",
        "upload_task = LabelImport.create_from_objects(client, project.uid, \"geospatial-import-job-1\", ground_truth_list)\n",
        "print(upload_task)\n",
        "\n",
        "#Wait for upload to finish (Will take up to five minutes)\n",
        "upload_task.wait_until_done()\n",
        "print(upload_task.errors)\n",
        "print(\"--- Finished in %s mins ---\" % ((time.time() - start_time)/60))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# queued_data_rows = [dr['id'] for dr in list(project.export_queued_data_rows())]\n",
        "# data_rows = [dr.uid for dr in list(dataset.export_data_rows())]\n",
        "# data_rows_not_queued = list(set(data_rows)- set(queued_data_rows))\n",
        "\n",
        "# # Randomly select 200 Data Rows\n",
        "# sampled_data_rows = random.sample(data_rows_not_queued, 200)\n",
        "\n",
        "# batch = project.create_batch(\n",
        "#   \"Second batch\", # name of the batch\n",
        "#   sampled_data_rows, # list of Data Rows\n",
        "#   5 # priority between 1-5\n",
        "# )\n"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}