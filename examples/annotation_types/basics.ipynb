{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incoming-linux",
   "metadata": {},
   "source": [
    "## Annotation Types\n",
    "This is a common format for representing human and machine generated annotations. A standard interface allows us to build tools that only need to work with a single interface. For example, if model predictions and labels are all represented by a common format we can write all of our etl, visualization code, training code to work with a single interface. Annotation types can also provide a seamless transition between local modeling and using labelbox. Some of the helper functions include:\n",
    "* Build annotations locally with local file paths, numpy arrays, or urls and create data rows with a single line of code\n",
    "* Easily upload model predictions by converting predictions to \n",
    "* Configure project ontology from model inferences\n",
    "* Easily access video data without having to worry about downloading each frame.\n",
    "* Helper functions for drawing annotations, converting them into shapely obejects, and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-accreditation",
   "metadata": {},
   "source": [
    "------- \n",
    "## Installation\n",
    "* Installing annotation types requires a slightly different pattern\n",
    "    - `pip install \"labelbox[data]\"`\n",
    "* `pip install labelbox` is still valid but it won't add the required dependencies. If you only want the client functionality of the SDK then don't add the data extras. However, you will likely get import errors if attempting to use the annotation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"labelbox[data]==3.0.0rc1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-marina",
   "metadata": {},
   "source": [
    "## Imports\n",
    "* All annotation types can be imported from `labelbox.data.annotation_types`\n",
    "* Helper functions that are compatible with annotation types can be found under `labelbox.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "from labelbox.data.annotation_types import (\n",
    "    Label,\n",
    "    RasterData,\n",
    "    LabelList,\n",
    "    TextData,\n",
    "    VideoData,\n",
    "    ObjectAnnotation,\n",
    "    ClassificationAnnotation,    \n",
    "    Polygon,\n",
    "    Rectangle,\n",
    "    Line,\n",
    "    Mask,\n",
    "    Point,\n",
    "    Checklist,\n",
    "    Radio,\n",
    "    Text,\n",
    "    TextEntity,\n",
    "    ClassificationAnswer\n",
    ")\n",
    "from labelbox import (\n",
    "    LabelingFrontend,\n",
    "    Client, \n",
    "    OntologyBuilder, \n",
    "    Tool, \n",
    "    Classification, \n",
    "    Option\n",
    ")\n",
    "from shapely.ops import transform\n",
    "from shapely.affinity import affine_transform\n",
    "import requests\n",
    "import IPython\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't want to give google access to drive you can skip this cell\n",
    "# and manually set `API_KEY` below.\n",
    "COLAB = \"google.colab\" in str(get_ipython())\n",
    "if COLAB:\n",
    "    !pip install colab-env -qU\n",
    "    from colab_env import envvar_handler\n",
    "    envvar_handler.envload()\n",
    "\n",
    "API_KEY = os.environ.get(\"LABELBOX_API_KEY\")\n",
    "if not os.environ.get(\"LABELBOX_API_KEY\"):\n",
    "    API_KEY = getpass(\"Please enter your labelbox api key\")\n",
    "    if COLAB:\n",
    "        envvar_handler.add_env(\"LABELBOX_API_KEY\", API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only update this if you have an on-prem deployment\n",
    "ENDPOINT = \"https://api.labelbox.com/graphql\"\n",
    "client = Client(api_key=API_KEY, endpoint=ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-jerusalem",
   "metadata": {},
   "source": [
    "## Basic Introduction\n",
    "* Construct annotations, labels, and label collections to create a common representation for all data\n",
    "    * A `LabelCollection` is essentially a list or generator for working with a collection of Labels\n",
    "    * A `Label` is a type of data and its associated annotations. E.g. an image and image annotations\n",
    "    * An annotation is either an `ObjectAnnotation` or a `ClassificationAnnotation`. These annotations contain a name and a `Geometry`, `Classification`, or some `Text` data. They also support a classifications field for subclasses.\n",
    "* In addition to create common interfaces for reducing the amount of etl work, each of these types have a set of helper functions that make ML workflows much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-atmosphere",
   "metadata": {},
   "source": [
    "### Constructing Annotation Objects\n",
    "* Annotation objects can be created in one of three ways:\n",
    "1. Manually construct them - usually used for model inferences\n",
    "    - Covered in this notebook\n",
    "2. Export labels to an annotation generator\n",
    "    - `project.label_generator()`\n",
    "    - `project.video_label_generator()`\n",
    "3. Use a converter to load from another format\n",
    "    - Covered in converters.ipynb notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-implementation",
   "metadata": {},
   "source": [
    "### Basic LabelCollection\n",
    "* A Label collection is either a labelList or LabelGenerator containing Labels\n",
    "    * More on this in label_containers.ipynb\n",
    "* Each label contains:\n",
    "    1. Data\n",
    "    2. Annotations associated with that data\n",
    "* The individual data and annotations will be explained below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [Label(\n",
    "    data = RasterData(url = \"http://my-img.jpg\"),\n",
    "    annotations = [\n",
    "        ObjectAnnotation(\n",
    "            value = Point(x = 10, y = 10),\n",
    "            name = \"target\"\n",
    "        )\n",
    "    ]\n",
    ")]\n",
    "labels = LabelList(labels) # or LabelList(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-router",
   "metadata": {},
   "source": [
    "* All models are pydantic so we can easily convert all of our objects to dictionaries and view the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0].schema()\n",
    "labels[0].dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-involvement",
   "metadata": {},
   "source": [
    "# Data \n",
    "* There are three classes to represent data\n",
    "    * TextData\n",
    "    * VideoData\n",
    "    * RasterData\n",
    "* The data objects can be constructed from various representations\n",
    "    - remote (url, uri)\n",
    "    - disk ( file path)\n",
    "    - compressed (image bytes)\n",
    "    - memory ( numpy array )\n",
    "* Then the access pattern is consistent regardless of the format used to construct the object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-cutting",
   "metadata": {},
   "source": [
    "### RasterData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data can be instantiated with any of the following\n",
    "image = RasterData(im_bytes = b'bytes')\n",
    "image = RasterData(arr = np.zeros((10,10,3)).astype(np.uint8))\n",
    "image = RasterData(file_path = '/tmp/img.jpg')\n",
    "image_url = \"https://picsum.photos/id/1003/200/300\"\n",
    "image = RasterData(url = image_url)\n",
    "# All of these can access the numpy representation the same way:\n",
    "np_data = image.data\n",
    "\n",
    "\n",
    "im = Image.fromarray(np_data)\n",
    "im.resize((im.size[0]//2, im.size[1]//2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-harvey",
   "metadata": {},
   "source": [
    "### TextData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text has the same access pattern as imagery.\n",
    "text = TextData(file_path = \"/tmp/some local file.txt\")\n",
    "text = TextData(text = \" some text content...\")\n",
    "text = TextData(url = \"https://filesamples.com/samples/document/txt/sample3.txt\")\n",
    "\n",
    "print(text.data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-seattle",
   "metadata": {},
   "source": [
    "### VideoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerMeltdowns.mp4\"\n",
    "\n",
    "video = VideoData(file_path = \"some video path.mp4\")\n",
    "video = VideoData(frames = { 1: np.zeros((32, 32, 3), dtype = np.uint8)})\n",
    "video = VideoData(url=video_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx, frame in video.data:\n",
    "    # Show every 50th frame\n",
    "    if idx % 50 == 0:\n",
    "        video_im = Image.fromarray(frame)\n",
    "        w,h = video_im.size\n",
    "        IPython.display.display( video_im.resize((w//16, h//16) ))        \n",
    "    if idx > 250:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-blackberry",
   "metadata": {},
   "source": [
    "### Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-blackberry",
   "metadata": {},
   "source": [
    "#### Adding data row information\n",
    "* This is not required ( and can be added at a later time but you can add data row information to your data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = TextData(text = \"some text\", uid = \"ckrey6o07000008l50uk2gcr3\", external_id = \"my_text_data_row\")\n",
    "rd = RasterData(url = image_url, uid = \"ckrey7te2000108l5hl8564dr\", external_id = \"my_raster_data_row\")\n",
    "vd = VideoData(url = video_url, uid = \"ckrey7xef000208l57hwfgi3v\", external_id = \"my_video_data_row\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-venice",
   "metadata": {},
   "source": [
    "#### Non-public urls\n",
    "* If the urls in your data is not publicly accessible you can override the fetching logic\n",
    "* For TextData and ImageData overwrite the following function and make sure it has the same signature. `data.fetch_remote(self) -> bytes`.\n",
    "* For VideoData, the signature is `VideoData.fetch_remote(self, local_path)`. This function needs to download the video file locally to that local_path to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.set_fetch_fn(lambda self: requests.get(self.url, headers = {...}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-jacket",
   "metadata": {},
   "source": [
    "# Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-minute",
   "metadata": {},
   "source": [
    "* There are 4 types of annotations\n",
    "    1. ObjectAnnotation\n",
    "        - Objects with location information\n",
    "        - Annotations that are found in the object field of the labelbox export\n",
    "        - Classes: Point, Polygon, Mask, Line, Rectangle, Named Entity\n",
    "    2. ClassificationAnnotation\n",
    "        - Classifications that can apply to data or another annotation\n",
    "        - Classes: Checklist, Radio, Text, Dropdown\n",
    "    3. VideoObjectAnnotation\n",
    "        - Same as object annotation but there are extra fields for video information\n",
    "    4. VideoClassificationAnnotation\n",
    "        - Same as classification annotation but there are extra fields for video information    \n",
    "-------- \n",
    "* Create an annotation by providing the following:\n",
    "1. Value\n",
    "    - Must be either a Geometry, TextEntity, or Classification\n",
    "    - This is the same as a top level tool in labelbox\n",
    "2. name or schema_id\n",
    "    - This is the id that corresponds to a particular class or just simply the class name\n",
    "    - If uploading to labelbox this must match a field in an ontology.\n",
    "3. (Optional) Classifications \n",
    "    - List of ClassificationAnnotations. This self referencing field enables infinite nesting of classifications.\n",
    "    - Be careful with how you use the tool. Labelbox does not support nesting classifications\n",
    "    - E.g. you can have tool.classifications but not tool.classifications[0].classifications\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-inspection",
   "metadata": {},
   "source": [
    "### ObjectAnnotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-fruit",
   "metadata": {},
   "source": [
    "##### Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_annotation = ObjectAnnotation(\n",
    "    value = Point(x = 5, y = 3),\n",
    "    name = \"point class name\"\n",
    ")\n",
    "\n",
    "point_annotation = ObjectAnnotation(\n",
    "    value = Point(x = 5, y = 3),\n",
    "    schema_id = \"ckrgcgl89000108jtggc9e687\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-karen",
   "metadata": {},
   "source": [
    "##### Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given some polygon:\n",
    "xy_poly = [\n",
    "    [60, 161], [67, 177], [76, 180], [77, 222], [82, 246], [78, 291], [72, 300], [87, 300], \n",
    "    [94, 244], [103, 243], [100, 269], [90, 290], [95, 296], [104, 292], [108, 272], \n",
    "    [111, 300], [121, 300], [117, 244], [128, 236], [133, 298], [142, 297], [137, 250], \n",
    "    [146, 208], [138, 185], [120, 180], [105, 189], [112, 162], [93, 156], [72, 160], \n",
    "]\n",
    "\n",
    "polygon_annotation = ObjectAnnotation(\n",
    "        value = Polygon(points = [Point(x = x, y = y) for x,y in xy_poly]),\n",
    "            name = \"polygon class name\"\n",
    ")\n",
    "\n",
    "polygon_annotation = ObjectAnnotation(\n",
    "        value = Polygon(points = [Point(x = x, y = y) for x,y in xy_poly]),\n",
    "            schema_id = \"ckrgcel71000008jtd9mn0szu\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-sharing",
   "metadata": {},
   "source": [
    "##### Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_line = [[0,0], [10,5], [15,5]]\n",
    "\n",
    "line_annotation = ObjectAnnotation(\n",
    "        value = Line(points = [Point(x = x, y = y) for x,y in xy_line]),\n",
    "            name = \"line class name\"\n",
    ")\n",
    "\n",
    "line_annotation = ObjectAnnotation(\n",
    "        value = Line(points = [Point(x = x, y = y) for x,y in xy_line]),\n",
    "            schema_id = \"ckrgcel71000008jtd9mn0szu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-april",
   "metadata": {},
   "source": [
    "##### Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = Point(x = 0, y = 0)\n",
    "end = Point(x = 5, y = 10)\n",
    "\n",
    "rectangle_annotation = ObjectAnnotation(\n",
    "        value = Rectangle(start = start, end = end),\n",
    "            name = \"rectangle class name\"\n",
    ")\n",
    "\n",
    "rectangle_annotation = ObjectAnnotation(\n",
    "        value = Rectangle(start = start, end = end),\n",
    "            schema_id = \"ckrgcel71000008jtd9mn0szu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-earthquake",
   "metadata": {},
   "source": [
    "##### Mask\n",
    "* The mask can be any RasterData object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_annotation = ObjectAnnotation(\n",
    "        value = Mask(mask =  RasterData(arr = np.zeros((32,32,3), dtype = np.uint8)), color = (255,255,255)),\n",
    "            name = \"mask class name\"\n",
    ")\n",
    "\n",
    "mask_annotation = ObjectAnnotation(\n",
    "        value = Mask(mask =  RasterData(arr = np.zeros((32,32,3), dtype = np.uint8)), color = (255,255,255)),\n",
    "            schema_id = \"ckrgcel71000008jtd9mn0szu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-short",
   "metadata": {},
   "source": [
    "* Multiple mask objects can reference the same mask to save memory and reduce the number of files that need to be uploaded (i.e. one seg mask for multiple classes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_data = RasterData(arr = np.zeros((32,32,3), dtype = np.uint8))\n",
    "mask_annotation = ObjectAnnotation(\n",
    "        value = Mask(mask  = raster_data, color = [255,255,255]),\n",
    "            name = \"eyes\"\n",
    ")\n",
    "\n",
    "mask_annotation = ObjectAnnotation(\n",
    "        value = Mask(mask = raster_data, color = [0,255,255]),\n",
    "            name = \"nose\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-seeker",
   "metadata": {},
   "source": [
    "* The Mask class support either RGB or Grayscale masks\n",
    "* Grayscale masks must have a single color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_data = RasterData(arr = np.zeros((32,32, 3), dtype = np.uint8))\n",
    "mask_annotation = ObjectAnnotation(\n",
    "        value = Mask(mask  = raster_data, color = (128,255,255)),\n",
    "            name = \"eye\"\n",
    ")\n",
    "\n",
    "mask_annotation = ObjectAnnotation(\n",
    "        value = Mask(mask = raster_data, color =(255,255,255)),\n",
    "            name = \"nose\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-coast",
   "metadata": {},
   "source": [
    "##### Text Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entity_annotation = ObjectAnnotation(\n",
    "        value = TextEntity(start  = 10, end = 12),\n",
    "            name = \"person\"\n",
    ")\n",
    "\n",
    "entity_annotation = ObjectAnnotation(\n",
    "        value = TextEntity(start  = 10, end = 12),\n",
    "            schema_id = \"ckrgddyli000108mk0c0t9qya\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-collaboration",
   "metadata": {},
   "source": [
    "##### Geometry Utilities\n",
    "* All of the previous objects except TextEntity inherit from the Geometry base class\n",
    "* They have the following properties and functions\n",
    "    1. raster(height width, kwargs)\n",
    "    2. shape - property\n",
    "    3. geometry - property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_annotation.value.shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-yeast",
   "metadata": {},
   "source": [
    "* Flip because shapely origin is different from image coords\n",
    "```\n",
    "      Shapely Coordinates           Image Coordinates\n",
    "           ------                        -------    \n",
    "                               0 → → Greater X → →\n",
    "    ↑                          ↓\n",
    "Greater Y                    ↓\n",
    "    ↑                       Greater Y\n",
    "    ↑                          ↓\n",
    "    0 → → Greater X → →\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_y_axis(poly, height):\n",
    "    return affine_transform(poly,[1, 0, 0, -1, 0, height])\n",
    "\n",
    "flip_y_axis(polygon_annotation.value.shapely, im.size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-novel",
   "metadata": {},
   "source": [
    "##### Shape to Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = (255,255,255)\n",
    "np_mask = polygon_annotation.value.raster(height = im.size[1], width = im.size[0], color = color)\n",
    "Image.fromarray(np.hstack([np_mask, np_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-narrow",
   "metadata": {},
   "source": [
    "##### Mask to Shape\n",
    "* Adds extra vertices to be as accurate as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_annotation = Mask(\n",
    "    mask = RasterData(arr = np_mask),\n",
    "    color = color \n",
    ")\n",
    "\n",
    "flip_y_axis(mask_annotation.shapely, im.size[1])\n",
    "# Simplify with mask_annotation.shapely.simplify(<float> simplification error tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-shareware",
   "metadata": {},
   "source": [
    "#### Masks with multiple annotations\n",
    "* Since masks can contain multiple classes you can split them up by calling raster on an individual annotation.\n",
    "* E.g. eyes and nose for our deer image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a combined mask from polygons\n",
    "eye_color, nose_color = (255,255,255), (255,0,0)\n",
    "h,w = np_data.shape[:2]\n",
    "eyes = [\n",
    "    Polygon(points = [Point(x=x,y=y) for x,y in [[82, 180], [83, 184], [88, 184], [86, 180]]]),\n",
    "    Polygon(points = [Point(x=x,y=y) for x,y in [[97, 182], [99, 184], [102, 183], [101, 180], [98, 180]]]),    \n",
    "]\n",
    "eye_masks = np.max([eye.raster(height = h, width = w) for eye in eyes], axis = 0)\n",
    "nose = Polygon(points =[ Point(x=x,y=y) for x,y in [[95, 192], [93, 197], [96, 198], [100, 197], [100, 194], [100, 192], [96, 192]]])\n",
    "nose_mask = nose.raster(height = h, width = w, color = nose_color)\n",
    "# Picks the brighter color if there is overlap. \n",
    "# If you don't want overlap then just simply create separate masks\n",
    "np_seg_mask = np.max([nose_mask, eye_masks], axis = 0)\n",
    "Image.fromarray(np.hstack([np_seg_mask, np_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-state",
   "metadata": {},
   "source": [
    "* Create two mask annotations from this one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_data = RasterData(arr = np_seg_mask)\n",
    "eye_mask = Mask(mask = mask_data, color = eye_color)\n",
    "nose_mask = Mask(mask = mask_data, color = nose_color)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-storm",
   "metadata": {},
   "source": [
    "* Calling `mask.raster()` will return a mask with pixels equal to the specified color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_raster = eye_mask.raster()\n",
    "nose_raster = nose_mask.raster()\n",
    "Image.fromarray(np.hstack([eye_raster,nose_raster, np_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-montana",
   "metadata": {},
   "source": [
    "### Classification Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-stomach",
   "metadata": {},
   "source": [
    "#### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_annotation = ClassificationAnnotation(\n",
    "    name = \"my text class\", \n",
    "    value = Text(answer = \"some text answer\")                               \n",
    ")\n",
    "\n",
    "text_annotation = ClassificationAnnotation(\n",
    "    schema_id = \"my text class\", \n",
    "    value = Text(answer = \"some text answer\")                               \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-japan",
   "metadata": {},
   "source": [
    "#### Radio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_annotation = ClassificationAnnotation(\n",
    "    name = \"is dog\",\n",
    "    value = Radio(answer = ClassificationAnswer(name = \"dog\")) \n",
    ")\n",
    "\n",
    "\n",
    "radio_annotation = ClassificationAnnotation(\n",
    "    schema_id = \"ckresqdg7000001jnb70v4zcc\",\n",
    "    value = Radio(answer = ClassificationAnswer(schema_id = \"ckrdy06ia000007ky94h04qlj\")) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-manual",
   "metadata": {},
   "source": [
    "##### Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checklist_annotation = ClassificationAnnotation(\n",
    "    schema_id = \"ckrestd5g000101jnhudjf29a\",\n",
    "    value = Checklist(answer = [ClassificationAnswer(schema_id = \"ckrdy06ia000007ky94h04qlj\")])\n",
    ")\n",
    "                                \n",
    "checklist_annotation = ClassificationAnnotation(\n",
    "    name = \"weather\",\n",
    "    value = Checklist(answer = [ClassificationAnswer(name = \"cloudy\")]) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-sauce",
   "metadata": {},
   "source": [
    "### Subclassifications\n",
    "* Objects can have nested classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "subclass_annotation = ObjectAnnotation(\n",
    "    value = Polygon(points = [Point(x = x, y = y) for x,y in xy_poly]),\n",
    "    name = \"deer\",\n",
    "    classifications = [\n",
    "        checklist_annotation, radio_annotation\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-brass",
   "metadata": {},
   "source": [
    "# Labels\n",
    "* Labels connect a list of annotations to data such as images, text, and video.\n",
    "* Labels have a convenient set of functions for dealing with that collection of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = Label(\n",
    "    data = RasterData(url = image_url),\n",
    "    annotations = [\n",
    "        ObjectAnnotation(\n",
    "            value = Polygon(points = [Point(x = x, y = y) for x,y in xy_poly]),\n",
    "            name = \"deer\"\n",
    "        ),\n",
    "        ObjectAnnotation(\n",
    "            name = \"deer_eyes\",\n",
    "            value = Mask(mask = RasterData(arr = np_mask), color = color)\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-preservation",
   "metadata": {},
   "source": [
    "### Interacting with labelbox:\n",
    "* For this label to be compatible with labelbox we need the following:\n",
    "    - all named features must have schema_ids\n",
    "    - all data must have urls\n",
    "        - masks\n",
    "        - images\n",
    "        - videos\n",
    "        - text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-behavior",
   "metadata": {},
   "source": [
    "### Uploading Urls\n",
    "* It doesn't matter how urls are set\n",
    "* Manually setting urls or use helper functions are both valid for working with labelbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set just by adding a value:\n",
    "image = RasterData(arr = np_data)\n",
    "image.url = \"http://myurl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-indiana",
   "metadata": {},
   "source": [
    "* Use functions to upload urls:\n",
    "    - Upload image urls : `Label.add_url_to_data(signer)`\n",
    "    - Upload segmentation masks : `label.add_url_to_masks(signer)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-blond",
   "metadata": {},
   "source": [
    "----- \n",
    "* Define the signer\n",
    "    - Creating urls requires a function that maps bytes to a str (url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signing_function(obj_bytes: bytes) -> str:\n",
    "    # WARNING: Do not use this signer. You will not be able to resign these images at a later date\n",
    "    url = client.upload_data(content=obj_bytes, sign=True)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-milan",
   "metadata": {},
   "source": [
    "* Create a complex label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label = Label(\n",
    "    data = RasterData(arr = np_data),\n",
    "    annotations = [\n",
    "         ObjectAnnotation(\n",
    "            name = \"deer_nose\",\n",
    "            value = nose_mask,\n",
    "            classifications = [\n",
    "                ClassificationAnnotation(\n",
    "                    name = \"description\",\n",
    "                    value = Radio(\n",
    "                        answer = ClassificationAnswer(name = \"wet\")\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        ObjectAnnotation(\n",
    "            name = \"deer_eyes\",\n",
    "            value = eye_mask\n",
    "        ),\n",
    "        ObjectAnnotation(\n",
    "            value = Polygon(points = [Point(x = x, y = y) for x,y in xy_poly]),\n",
    "            name = \"deer\"\n",
    "        ),\n",
    "        ClassificationAnnotation(name = \"image_description\", \n",
    "            value = Checklist(answer = [\n",
    "                ClassificationAnswer(name = \"bright\"),\n",
    "                ClassificationAnswer(name = \"not_blurry\"),            \n",
    "         ])),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-murray",
   "metadata": {},
   "source": [
    "* Check that the image does not have a url\n",
    "* Add the url\n",
    "* Check that the url was added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before\",label.data.url)\n",
    "label.add_url_to_data(signing_function)\n",
    "print(\"After\", label.data.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-outdoors",
   "metadata": {},
   "source": [
    "* Masks also need urls for annotation imports\n",
    "* Note that the url is the same when uploaded.\n",
    "    - The url is only uploaded once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also have a mask that needs a url:\n",
    "mask_annotations = [annot for annot in label.annotations if isinstance(annot.value, Mask)]\n",
    "for annot in mask_annotations:\n",
    "    print(f\"{annot.name} before\" , annot.value.mask.url)\n",
    "    \n",
    "    \n",
    "label.add_url_to_masks(signing_function)\n",
    "\n",
    "\n",
    "for annot in mask_annotations:\n",
    "    print(f\"{annot.name} after\" , annot.value.mask.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-disabled",
   "metadata": {},
   "source": [
    "### Creating Data Rows\n",
    "* Our Labels objects are great for working with locally but we might want to upload to labelbox\n",
    "* This is required for MAL, MEA, and to add additional labels to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = client.create_dataset(name = \"label_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the signing function is only used if a url is not already provided\n",
    "print(label.data.uid)\n",
    "label.create_data_row(dataset, signing_function)\n",
    "print(label.data.uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-steam",
   "metadata": {},
   "source": [
    "### Assigning Schema Ids:\n",
    "* All tools, classifications, and options either have names or schema_ids.\n",
    "* Locally it is convenient to provide a name so that we don't need a labelbox project to use these interfaces.\n",
    "* To use MAL and MEA schema ids are required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When uploading for MAL or MEA we need an ontology.\n",
    "# So let's create one\n",
    "\n",
    "\n",
    "# These names have to match our object names exactly!!\n",
    "ontology_builder = OntologyBuilder(tools=[\n",
    "    Tool(tool=Tool.Type.POLYGON, name=\"deer\"),\n",
    "    Tool(tool=Tool.Type.SEGMENTATION, name=\"deer_nose\", \n",
    "         classifications = [\n",
    "             Classification(\n",
    "                 class_type = Classification.Type.RADIO, \n",
    "                 instructions = \"description\", \n",
    "                 options = [Option(value = \"wet\")]\n",
    "             )]),\n",
    "    Tool(tool=Tool.Type.SEGMENTATION, name=\"deer_eyes\")    \n",
    "], classifications = [\n",
    "    Classification(\n",
    "        Classification.Type.CHECKLIST, \n",
    "        instructions = \"image_description\", \n",
    "        options = [Option(value = \"bright\"), Option(value = \"not_blurry\"), Option(value = \"dark\")])])\n",
    "\n",
    "\n",
    "editor = next(\n",
    "    client.get_labeling_frontends(where=LabelingFrontend.name == \"Editor\"))\n",
    "project = client.create_project(name=\"test_annotation_types\")\n",
    "project.setup(editor, ontology_builder.asdict())\n",
    "project.datasets.connect(dataset)\n",
    "\n",
    "ontology = OntologyBuilder.from_project(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_schema_ids(label):\n",
    "    for annotation in label.annotations:\n",
    "        print(f\"Object : {annotation.name} - {annotation.schema_id}\")\n",
    "        for classification in getattr(annotation, 'classifications', []):\n",
    "            print(f\"--- Subclass : {classification.name} - {classification.schema_id}\")\n",
    "            option = classification.value\n",
    "            print(f\"--- --- Options: {option.answer.name} - {option.answer.schema_id}\")\n",
    "\n",
    "        if isinstance(annotation, ClassificationAnnotation):\n",
    "            for option in annotation.value.answer:\n",
    "                print(f\"--- Options: {option.name} - {option.schema_id}\")\n",
    "show_schema_ids(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "label.assign_schema_ids(ontology)\n",
    "show_schema_ids(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-prerequisite",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "* Annotation types should be thought of as low level interfaces\n",
    "* We are working on a set of tools to make this less verbose. Please provide any feedback!\n",
    "* Checkout other notebooks to see how to use higher level tools that are compatible with these interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "dataset.delete()\n",
    "project.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-programming",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-theorem",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
