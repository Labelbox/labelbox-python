{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "db768cda",
            "metadata": {},
            "source": [
                "<td>\n",
                "   <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/></a>\n",
                "</td>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb5611d0",
            "metadata": {},
            "source": [
                "<td>\n",
                "<a href=\"https://colab.research.google.com/github/Labelbox/labelbox-python/blob/develop/examples/annotation_types/basics.ipynb\" target=\"_blank\"><img\n",
                "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
                "</td>\n",
                "\n",
                "<td>\n",
                "<a href=\"https://github.com/Labelbox/labelbox-python/tree/develop/examples/annotation_types/basics.ipynb\" target=\"_blank\"><img\n",
                "src=\"https://img.shields.io/badge/GitHub-100000?logo=github&logoColor=white\" alt=\"GitHub\"></a>\n",
                "</td>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "incoming-linux",
            "metadata": {},
            "source": [
                "## Annotation Types\n",
                "This is a common format for representing human and machine generated annotations. A standard interface allows us to build one set of tools that is compatible with all of our data. For example, if model predictions and labels are all represented by a common format we can write all of our etl, visualization code, training code to work with a single interface. Annotation types can also provide a seamless transition between local modeling and using labelbox. Some of the helper functions include:\n",
                "* Build annotations locally with local file paths, numpy arrays, or urls and create data rows with a single line of code\n",
                "* Easily upload model predictions for MAL or MEA by converting annotation objects to the import format\n",
                "* Configure project ontologies from a set of model inferences\n",
                "* Easily access video data without having to worry about downloading each frame's annotations.\n",
                "* Helper functions for drawing annotations, converting them into shapely objects, and much more."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "guilty-accreditation",
            "metadata": {},
            "source": [
                "------- \n",
                "## Installation\n",
                "* Installing annotation types requires a slightly different pattern\n",
                "    - `pip install \"labelbox[data]\"`\n",
                "* `pip install labelbox` is still valid but it won't add the required dependencies. If you only want the client functionality of the SDK then don't add the [data] extras. However, you will likely get import errors if attempting to use the annotation types"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "literary-qualification",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install \"labelbox[data]\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "latter-marina",
            "metadata": {},
            "source": [
                "## Imports\n",
                "* All annotation types can be imported from `labelbox.data.annotation_types`\n",
                "* Helper functions that are compatible with annotation types can be found under `labelbox.data`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "productive-power",
            "metadata": {},
            "outputs": [],
            "source": [
                "from labelbox.data.annotation_types import (\n",
                "    Label, ImageData, MaskData, LabelList, TextData, VideoData,\n",
                "    ObjectAnnotation, ClassificationAnnotation, Polygon, Rectangle, Line, Mask,\n",
                "    Point, Checklist, Radio, Text, TextEntity, ClassificationAnswer)\n",
                "from labelbox.schema.media_type import MediaType\n",
                "from labelbox import (LabelingFrontend, Client, OntologyBuilder, Tool,\n",
                "                      Classification, Option)\n",
                "from shapely.ops import transform\n",
                "from shapely.affinity import affine_transform\n",
                "import requests\n",
                "import IPython\n",
                "from PIL import Image\n",
                "from io import BytesIO\n",
                "import numpy as np\n",
                "import os"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3150348b",
            "metadata": {},
            "source": [
                "# API Key and Client\n",
                "Provide a valid api key below in order to properly connect to the Labelbox Client."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "selective-harrison",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add your api key\n",
                "API_KEY = None\n",
                "client = Client(api_key=API_KEY)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "democratic-jerusalem",
            "metadata": {},
            "source": [
                "## Basic Introduction\n",
                "* Construct annotations, labels, and label collections to create a common representation for all data\n",
                "    * A `LabelCollection` is essentially a list or generator for working with a collection of Labels\n",
                "    * A `Label` is a type of data and its associated annotations. E.g. an image and image annotations\n",
                "    * An annotation is either an `ObjectAnnotation` or a `ClassificationAnnotation`. These annotations contain a name and a `Geometry`, `Classification`, or some `Text` data. They also support a classifications field for subclasses.\n",
                "* In addition to create common interfaces for reducing the amount of etl work, each of these types have a set of helper functions that make ML workflows much easier."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "settled-atmosphere",
            "metadata": {},
            "source": [
                "### Constructing Annotation Objects\n",
                "* Annotation objects can be created in one of three ways:\n",
                "1. Manually construct them - usually used for model inferences\n",
                "    - Covered in this notebook\n",
                "2. Export labels to an annotation generator\n",
                "    - `project.label_generator()`\n",
                "3. Use a converter to load from another format\n",
                "    - Covered in the converters.ipynb notebook."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "terminal-implementation",
            "metadata": {},
            "source": [
                "### Basic LabelCollection\n",
                "* A Label collection is either a `labelList` or `LabelGenerator` containing `Labels`\n",
                "    * More on this in label_containers.ipynb\n",
                "* Each label contains:\n",
                "    1. Data\n",
                "    2. Annotations associated with that data\n",
                "* The individual data and annotations will be explained below"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "valuable-objective",
            "metadata": {},
            "outputs": [],
            "source": [
                "labels = [\n",
                "    Label(\n",
                "        data=ImageData(url=\"http://my-img.jpg\"),\n",
                "        annotations=[ObjectAnnotation(value=Point(x=10, y=10), name=\"target\")])\n",
                "]\n",
                "labels = LabelList(labels)  # or LabelList(labels)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "circular-router",
            "metadata": {},
            "source": [
                "* All models are pydantic models so we can easily convert all of our objects to dictionaries and view the schema."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "judicial-galaxy",
            "metadata": {},
            "outputs": [],
            "source": [
                "labels[0].schema()\n",
                "labels[0].dict()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "neither-involvement",
            "metadata": {},
            "source": [
                "# Data \n",
                "* There are three classes to represent data\n",
                "    * TextData\n",
                "    * VideoData\n",
                "    * ImageData\n",
                "* The data objects can be constructed from various representations\n",
                "    - remote (url, uri)\n",
                "    - disk ( file path)\n",
                "    - compressed (image bytes)\n",
                "    - memory ( numpy array )\n",
                "* Then the access pattern is consistent regardless of the format used to construct the object\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "associate-cutting",
            "metadata": {},
            "source": [
                "### ImageData"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "excellent-oakland",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data can be instantiated with any of the following\n",
                "image = ImageData(im_bytes=b'bytes')\n",
                "image = ImageData(arr=np.zeros((10, 10, 3)).astype(np.uint8))\n",
                "image = ImageData(file_path='/tmp/img.jpg')\n",
                "image_url = \"https://picsum.photos/id/1003/200/300\"\n",
                "image = ImageData(url=image_url)\n",
                "# All of these can access the numpy representation the same way:\n",
                "np_data = image.value\n",
                "\n",
                "im = Image.fromarray(np_data)\n",
                "im.resize((im.size[0] // 2, im.size[1] // 2))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "appropriate-harvey",
            "metadata": {},
            "source": [
                "### TextData"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "smart-pearl",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Text has the same access pattern as imagery.\n",
                "text = TextData(file_path=\"/tmp/some local file.txt\")\n",
                "text = TextData(text=\" some text content...\")\n",
                "text = TextData(url=\"https://filesamples.com/samples/document/txt/sample3.txt\")\n",
                "\n",
                "print(text.value[:100])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "growing-seattle",
            "metadata": {},
            "source": [
                "### VideoData"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "looking-pharmacy",
            "metadata": {},
            "outputs": [],
            "source": [
                "video_url = \"http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerMeltdowns.mp4\"\n",
                "\n",
                "video = VideoData(file_path=\"some video path.mp4\")\n",
                "video = VideoData(frames={1: np.zeros((32, 32, 3), dtype=np.uint8)})\n",
                "video = VideoData(url=video_url)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "closed-newcastle",
            "metadata": {},
            "outputs": [],
            "source": [
                "for idx, frame in video.value:\n",
                "    # Show every 50th frame\n",
                "    if idx % 50 == 0:\n",
                "        video_im = Image.fromarray(frame)\n",
                "        w, h = video_im.size\n",
                "        IPython.display.display(video_im.resize((w // 16, h // 16)))\n",
                "    if idx > 250:\n",
                "        break"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "limiting-blackberry",
            "metadata": {},
            "source": [
                "### Advanced"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "opposed-blackberry",
            "metadata": {},
            "source": [
                "#### Adding data row information\n",
                "* This is not required ( and can be added at a later time but you can add data row information to your data)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "rising-grammar",
            "metadata": {},
            "outputs": [],
            "source": [
                "td = TextData(text=\"some text\",\n",
                "              uid=\"ckrey6o07000008l50uk2gcr3\",\n",
                "              external_id=\"my_text_data_row\")\n",
                "rd = ImageData(url=image_url,\n",
                "               uid=\"ckrey7te2000108l5hl8564dr\",\n",
                "               external_id=\"my_raster_data_row\")\n",
                "vd = VideoData(url=video_url,\n",
                "               uid=\"ckrey7xef000208l57hwfgi3v\",\n",
                "               external_id=\"my_video_data_row\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dense-venice",
            "metadata": {},
            "source": [
                "#### Non-public urls\n",
                "* If the urls in your data is not publicly accessible you can override the fetching logic\n",
                "* For `TextData` and `ImageData` overwrite the following function and make sure it has the same signature. `data.fetch_remote(self) -> bytes`.\n",
                "* For `VideoData`, the signature is `VideoData.fetch_remote(self, local_path)`. This function needs to download the video file locally to that local_path to work."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "exceptional-diary",
            "metadata": {},
            "outputs": [],
            "source": [
                "image.set_fetch_fn(lambda self: requests.get(self.url, headers={...}).content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "contemporary-jacket",
            "metadata": {},
            "source": [
                "# Annotations"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "protective-minute",
            "metadata": {},
            "source": [
                "* There are 4 types of annotations\n",
                "    1. `ObjectAnnotation`\n",
                "        - Objects with location information\n",
                "        - Annotations that are found in the object field of the labelbox export\n",
                "        - Classes: `Point`, `Polygon`, `Mask`, `Line`, `Rectangle`, `TextEntity`\n",
                "    2. `ClassificationAnnotation`\n",
                "        - Classifications that can apply to data or another annotation\n",
                "        - Classes: `Checklist`, `Radio`, `Text`, `Dropdown`\n",
                "    3. `VideoObjectAnnotation`\n",
                "        - Same as object annotation but there are extra fields for video information\n",
                "    4. `VideoClassificationAnnotation`\n",
                "        - Same as classification annotation but there are extra fields for video information    \n",
                "-------- \n",
                "* Create an annotation by providing the following:\n",
                "1. Value\n",
                "    - Must be either a `Geometry`, `TextEntity`, or `Classification`\n",
                "    - This is the same as a top level tool in labelbox\n",
                "2. Name or feature_schema_id\n",
                "    - This is the id that corresponds to a particular class or just simply the class name\n",
                "    - If uploading to labelbox this must match a field in an ontology.\n",
                "3. (Optional) Classifications \n",
                "    - List of `ClassificationAnnotations`. This self referencing field enables infinite nesting of classifications.\n",
                "    - Be careful with how you use the tool. Labelbox does not support nesting classifications\n",
                "    - E.g. you can have tool.classifications but not tool.classifications[0].classifications\n",
                "        "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "humanitarian-inspection",
            "metadata": {},
            "source": [
                "### ObjectAnnotation"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dynamic-fruit",
            "metadata": {},
            "source": [
                "##### Point"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "hollow-bulletin",
            "metadata": {},
            "outputs": [],
            "source": [
                "point_annotation = ObjectAnnotation(value=Point(x=5, y=3),\n",
                "                                    name=\"point class name\")\n",
                "\n",
                "point_annotation = ObjectAnnotation(\n",
                "    value=Point(x=5, y=3), feature_schema_id=\"ckrgcgl89000108jtggc9e687\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "endless-karen",
            "metadata": {},
            "source": [
                "##### Polygon"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "loose-buffer",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Given some polygon:\n",
                "xy_poly = [\n",
                "    [60, 161],\n",
                "    [67, 177],\n",
                "    [76, 180],\n",
                "    [77, 222],\n",
                "    [82, 246],\n",
                "    [78, 291],\n",
                "    [72, 300],\n",
                "    [87, 300],\n",
                "    [94, 244],\n",
                "    [103, 243],\n",
                "    [100, 269],\n",
                "    [90, 290],\n",
                "    [95, 296],\n",
                "    [104, 292],\n",
                "    [108, 272],\n",
                "    [111, 300],\n",
                "    [121, 300],\n",
                "    [117, 244],\n",
                "    [128, 236],\n",
                "    [133, 298],\n",
                "    [142, 297],\n",
                "    [137, 250],\n",
                "    [146, 208],\n",
                "    [138, 185],\n",
                "    [120, 180],\n",
                "    [105, 189],\n",
                "    [112, 162],\n",
                "    [93, 156],\n",
                "    [72, 160],\n",
                "]\n",
                "\n",
                "polygon_annotation = ObjectAnnotation(\n",
                "    value=Polygon(points=[Point(x=x, y=y) for x, y in xy_poly]),\n",
                "    name=\"polygon class name\")\n",
                "\n",
                "polygon_annotation = ObjectAnnotation(\n",
                "    value=Polygon(points=[Point(x=x, y=y) for x, y in xy_poly]),\n",
                "    feature_schema_id=\"ckrgcel71000008jtd9mn0szu\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "improved-sharing",
            "metadata": {},
            "source": [
                "##### Line"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "statistical-steering",
            "metadata": {},
            "outputs": [],
            "source": [
                "xy_line = [[0, 0], [10, 5], [15, 5]]\n",
                "\n",
                "line_annotation = ObjectAnnotation(\n",
                "    value=Line(points=[Point(x=x, y=y) for x, y in xy_line]),\n",
                "    name=\"line class name\")\n",
                "\n",
                "line_annotation = ObjectAnnotation(\n",
                "    value=Line(points=[Point(x=x, y=y) for x, y in xy_line]),\n",
                "    feature_schema_id=\"ckrgcel71000008jtd9mn0szu\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "consistent-april",
            "metadata": {},
            "source": [
                "##### Rectangle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "closing-determination",
            "metadata": {},
            "outputs": [],
            "source": [
                "start = Point(x=0, y=0)\n",
                "end = Point(x=5, y=10)\n",
                "\n",
                "rectangle_annotation = ObjectAnnotation(value=Rectangle(start=start, end=end),\n",
                "                                        name=\"rectangle class name\")\n",
                "\n",
                "rectangle_annotation = ObjectAnnotation(\n",
                "    value=Rectangle(start=start, end=end),\n",
                "    feature_schema_id=\"ckrgcel71000008jtd9mn0szu\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "prostate-earthquake",
            "metadata": {},
            "source": [
                "##### Mask\n",
                "* The mask can be any ImageData object.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "asian-gates",
            "metadata": {},
            "outputs": [],
            "source": [
                "mask_annotation = ObjectAnnotation(value=Mask(\n",
                "    mask=MaskData(arr=np.zeros((32, 32, 3), dtype=np.uint8)),\n",
                "    color=(255, 255, 255)),\n",
                "                                   name=\"mask class name\")\n",
                "\n",
                "mask_annotation = ObjectAnnotation(\n",
                "    value=Mask(mask=MaskData(arr=np.zeros((32, 32, 3), dtype=np.uint8)),\n",
                "               color=(255, 255, 255)),\n",
                "    feature_schema_id=\"ckrgcel71000008jtd9mn0szu\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "agreed-short",
            "metadata": {},
            "source": [
                "* Multiple mask objects can reference the same mask to save memory and reduce the number of files that need to be uploaded (i.e. one seg mask for multiple classes).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "approved-klein",
            "metadata": {},
            "outputs": [],
            "source": [
                "raster_data = MaskData(arr=np.zeros((32, 32, 3), dtype=np.uint8))\n",
                "mask_annotation = ObjectAnnotation(value=Mask(mask=raster_data,\n",
                "                                              color=[255, 255, 255]),\n",
                "                                   name=\"eyes\")\n",
                "\n",
                "mask_annotation = ObjectAnnotation(value=Mask(mask=raster_data,\n",
                "                                              color=[0, 255, 255]),\n",
                "                                   name=\"nose\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "rocky-seeker",
            "metadata": {},
            "source": [
                "* The Mask class support either RGB masks only"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "japanese-extraction",
            "metadata": {},
            "outputs": [],
            "source": [
                "raster_data = MaskData(arr=np.zeros((32, 32, 3), dtype=np.uint8))\n",
                "mask_annotation = ObjectAnnotation(value=Mask(mask=raster_data,\n",
                "                                              color=(128, 255, 255)),\n",
                "                                   name=\"eye\")\n",
                "\n",
                "mask_annotation = ObjectAnnotation(value=Mask(mask=raster_data,\n",
                "                                              color=(255, 255, 255)),\n",
                "                                   name=\"nose\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "metric-coast",
            "metadata": {},
            "source": [
                "##### Text Entity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "representative-malpractice",
            "metadata": {},
            "outputs": [],
            "source": [
                "entity_annotation = ObjectAnnotation(value=TextEntity(start=10, end=12),\n",
                "                                     name=\"person\")\n",
                "\n",
                "entity_annotation = ObjectAnnotation(\n",
                "    value=TextEntity(start=10, end=12),\n",
                "    feature_schema_id=\"ckrgddyli000108mk0c0t9qya\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "close-collaboration",
            "metadata": {},
            "source": [
                "##### Geometry Utilities\n",
                "* All of the previous objects except TextEntity inherit from the Geometry base class\n",
                "* They have the following properties and functions\n",
                "    1. draw(height width, kwargs)\n",
                "    2. shape - property\n",
                "    3. geometry - property"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "liquid-marsh",
            "metadata": {},
            "outputs": [],
            "source": [
                "polygon_annotation.value.shapely"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "unnecessary-yeast",
            "metadata": {},
            "source": [
                "* Flip because shapely origin is different from image coords\n",
                "```\n",
                "      Shapely Coordinates           Image Coordinates\n",
                "           ------                        -------    \n",
                "                               0 → → Greater X → →\n",
                "    ↑                          ↓\n",
                "Greater Y                    ↓\n",
                "    ↑                       Greater Y\n",
                "    ↑                          ↓\n",
                "    0 → → Greater X → →\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "fixed-priority",
            "metadata": {},
            "outputs": [],
            "source": [
                "def flip_y_axis(poly, height):\n",
                "    return affine_transform(poly, [1, 0, 0, -1, 0, height])\n",
                "\n",
                "\n",
                "flip_y_axis(polygon_annotation.value.shapely, im.size[1])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "important-novel",
            "metadata": {},
            "source": [
                "##### Shape to Mask"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "split-planner",
            "metadata": {},
            "outputs": [],
            "source": [
                "color = (255, 255, 255)\n",
                "np_mask = polygon_annotation.value.draw(height=im.size[1],\n",
                "                                        width=im.size[0],\n",
                "                                        color=color)\n",
                "Image.fromarray(np.hstack([np_mask, np_data]))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "static-narrow",
            "metadata": {},
            "source": [
                "##### Mask to Shape\n",
                "* Adds extra vertices to be as accurate as possible"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "young-herald",
            "metadata": {},
            "outputs": [],
            "source": [
                "mask_annotation = Mask(mask=MaskData(arr=np_mask), color=color)\n",
                "\n",
                "flip_y_axis(mask_annotation.shapely, im.size[1])\n",
                "# Simplify with mask_annotation.shapely.simplify(<float> simplification error tolerance)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "awful-shareware",
            "metadata": {},
            "source": [
                "#### Masks with multiple annotations\n",
                "* Since masks can contain multiple classes you can split them up by calling raster on an individual annotation.\n",
                "* E.g. eyes and nose for our deer image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "colored-vermont",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build a combined mask from polygons\n",
                "eye_color, nose_color = (255, 255, 255), (255, 0, 0)\n",
                "h, w = np_data.shape[:2]\n",
                "eyes = [\n",
                "    Polygon(points=[\n",
                "        Point(x=x, y=y) for x, y in [[82, 180], [83, 184], [88, 184], [86, 180]]\n",
                "    ]),\n",
                "    Polygon(points=[\n",
                "        Point(x=x, y=y)\n",
                "        for x, y in [[97, 182], [99, 184], [102, 183], [101, 180], [98, 180]]\n",
                "    ]),\n",
                "]\n",
                "eye_masks = np.max([eye.draw(height=h, width=w) for eye in eyes], axis=0)\n",
                "nose = Polygon(points=[\n",
                "    Point(x=x, y=y) for x, y in [[95, 192], [93, 197], [96, 198], [100, 197],\n",
                "                                 [100, 194], [100, 192], [96, 192]]\n",
                "])\n",
                "nose_mask = nose.draw(height=h, width=w, color=nose_color)\n",
                "# Picks the brighter color if there is overlap.\n",
                "# If you don't want overlap then just simply create separate masks\n",
                "np_seg_mask = np.max([nose_mask, eye_masks], axis=0)\n",
                "Image.fromarray(np.hstack([np_seg_mask, np_data]))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "accredited-state",
            "metadata": {},
            "source": [
                "* Create two mask annotations from this one image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "broadband-doubt",
            "metadata": {},
            "outputs": [],
            "source": [
                "mask_data = MaskData(arr=np_seg_mask)\n",
                "eye_mask = Mask(mask=mask_data, color=eye_color)\n",
                "nose_mask = Mask(mask=mask_data, color=nose_color)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "swiss-storm",
            "metadata": {},
            "source": [
                "* Calling `mask.draw()` will return a mask with pixels equal to the specified color"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "id": "progressive-feeding",
            "metadata": {},
            "outputs": [],
            "source": [
                "eye_raster = eye_mask.draw()\n",
                "nose_raster = nose_mask.draw()\n",
                "Image.fromarray(np.hstack([eye_raster, nose_raster, np_data]))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "institutional-montana",
            "metadata": {},
            "source": [
                "### Classification Annotation"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "split-stomach",
            "metadata": {},
            "source": [
                "#### Text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "id": "celtic-approval",
            "metadata": {},
            "outputs": [],
            "source": [
                "text_annotation = ClassificationAnnotation(\n",
                "    name=\"my text class\", value=Text(answer=\"some text answer\"))\n",
                "\n",
                "text_annotation = ClassificationAnnotation(\n",
                "    feature_schema_id=\"my text class\", value=Text(answer=\"some text answer\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "silent-japan",
            "metadata": {},
            "source": [
                "#### Radio"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "orange-liberty",
            "metadata": {},
            "outputs": [],
            "source": [
                "radio_annotation = ClassificationAnnotation(\n",
                "    name=\"is dog\", value=Radio(answer=ClassificationAnswer(name=\"dog\")))\n",
                "\n",
                "radio_annotation = ClassificationAnnotation(\n",
                "    feature_schema_id=\"ckresqdg7000001jnb70v4zcc\",\n",
                "    value=Radio(answer=ClassificationAnswer(\n",
                "        feature_schema_id=\"ckrdy06ia000007ky94h04qlj\")))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "editorial-manual",
            "metadata": {},
            "source": [
                "##### Checklist"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "id": "closing-federal",
            "metadata": {},
            "outputs": [],
            "source": [
                "checklist_annotation = ClassificationAnnotation(\n",
                "    feature_schema_id=\"ckrestd5g000101jnhudjf29a\",\n",
                "    value=Checklist(answer=[\n",
                "        ClassificationAnswer(feature_schema_id=\"ckrdy06ia000007ky94h04qlj\")\n",
                "    ]))\n",
                "\n",
                "checklist_annotation = ClassificationAnnotation(\n",
                "    name=\"weather\",\n",
                "    value=Checklist(answer=[ClassificationAnswer(name=\"cloudy\")]))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "younger-sauce",
            "metadata": {},
            "source": [
                "### Subclassifications\n",
                "* Objects can have nested classifications"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "id": "partial-mission",
            "metadata": {},
            "outputs": [],
            "source": [
                "subclass_annotation = ObjectAnnotation(\n",
                "    value=Polygon(points=[Point(x=x, y=y) for x, y in xy_poly]),\n",
                "    name=\"deer\",\n",
                "    classifications=[checklist_annotation, radio_annotation])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "mathematical-brass",
            "metadata": {},
            "source": [
                "# Labels\n",
                "* Labels connect a list of annotations to data such as images, text, and video.\n",
                "* Labels have a convenient set of functions for dealing with that collection of data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "id": "legislative-blink",
            "metadata": {},
            "outputs": [],
            "source": [
                "label = Label(data=ImageData(url=image_url),\n",
                "              annotations=[\n",
                "                  ObjectAnnotation(value=Polygon(\n",
                "                      points=[Point(x=x, y=y) for x, y in xy_poly]),\n",
                "                                   name=\"deer\"),\n",
                "                  ObjectAnnotation(name=\"deer_eyes\",\n",
                "                                   value=Mask(mask=MaskData(arr=np_mask),\n",
                "                                              color=color))\n",
                "              ])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "headed-preservation",
            "metadata": {},
            "source": [
                "### Interacting with labelbox:\n",
                "* For this label to be compatible with labelbox we need the following:\n",
                "    - all named features must have feature_schema_ids\n",
                "    - all data must have urls\n",
                "        - masks\n",
                "        - images\n",
                "        - videos\n",
                "        - text\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "honey-behavior",
            "metadata": {},
            "source": [
                "### Uploading Urls\n",
                "* It doesn't matter how urls are set\n",
                "* Manually setting urls or use helper functions are both valid for working with labelbox"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "id": "sized-trash",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Manually set just by adding a value:\n",
                "image = ImageData(arr=np_data)\n",
                "image.url = \"http://myurl\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "express-indiana",
            "metadata": {},
            "source": [
                "* Use functions to upload urls:\n",
                "    - Upload image urls : `Label.add_url_to_data(signer)`\n",
                "    - Upload segmentation masks : `label.add_url_to_masks(signer)`"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "gross-blond",
            "metadata": {},
            "source": [
                "----- \n",
                "* Define the signer\n",
                "    - Creating urls requires a function that maps bytes to a str (url)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "id": "incomplete-annotation",
            "metadata": {},
            "outputs": [],
            "source": [
                "def signing_function(obj_bytes: bytes) -> str:\n",
                "    # Do not use this signer. You will not be able to resign these images at a later date\n",
                "    url = client.upload_data(content=obj_bytes, sign=True)\n",
                "    return url"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "genuine-milan",
            "metadata": {},
            "source": [
                "* Create a complex label"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "id": "actual-noise",
            "metadata": {},
            "outputs": [],
            "source": [
                "label = Label(\n",
                "    data=ImageData(arr=np_data),\n",
                "    annotations=[\n",
                "        ObjectAnnotation(\n",
                "            name=\"deer_nose\",\n",
                "            value=nose_mask,\n",
                "            classifications=[\n",
                "                ClassificationAnnotation(\n",
                "                    name=\"description\",\n",
                "                    value=Radio(answer=ClassificationAnswer(name=\"wet\")))\n",
                "            ]),\n",
                "        ObjectAnnotation(name=\"deer_eyes\", value=eye_mask),\n",
                "        ObjectAnnotation(\n",
                "            value=Polygon(points=[Point(x=x, y=y) for x, y in xy_poly]),\n",
                "            name=\"deer\"),\n",
                "        ClassificationAnnotation(name=\"image_description\",\n",
                "                                 value=Checklist(answer=[\n",
                "                                     ClassificationAnswer(name=\"bright\"),\n",
                "                                     ClassificationAnswer(name=\"not_blurry\"),\n",
                "                                 ])),\n",
                "    ])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "precise-murray",
            "metadata": {},
            "source": [
                "* Check that the image does not have a url\n",
                "* Add the url\n",
                "* Check that the url was added"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "id": "retired-canadian",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Before\", label.data.url)\n",
                "label.add_url_to_data(signing_function)\n",
                "print(\"After\", label.data.url)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "right-outdoors",
            "metadata": {},
            "source": [
                "* Masks also need urls for annotation imports\n",
                "* Note that the url is the same when uploaded.\n",
                "    - The url is only uploaded once"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "id": "tested-premises",
            "metadata": {},
            "outputs": [],
            "source": [
                "# We also have a mask that needs a url:\n",
                "mask_annotations = [\n",
                "    annot for annot in label.annotations if isinstance(annot.value, Mask)\n",
                "]\n",
                "for annot in mask_annotations:\n",
                "    print(f\"{annot.name} before\", annot.value.mask.url)\n",
                "\n",
                "label.add_url_to_masks(signing_function)\n",
                "\n",
                "for annot in mask_annotations:\n",
                "    print(f\"{annot.name} after\", annot.value.mask.url)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "neutral-disabled",
            "metadata": {},
            "source": [
                "### Creating Data Rows\n",
                "* `Labels` objects are great for working with locally but we might want to upload to labelbox\n",
                "* This is required for MAL, MEA, and to add additional labels to the data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "id": "explicit-interval",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = client.create_dataset(name=\"label_dataset\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "id": "equivalent-plain",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Note that the signing function is only used if a url is not already provided\n",
                "print(label.data.uid)\n",
                "label.create_data_row(dataset, signing_function)\n",
                "print(label.data.uid)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "literary-steam",
            "metadata": {},
            "source": [
                "### Assigning Schema Ids:\n",
                "* All tools, classifications, and options either have names or feature_schema_ids.\n",
                "* Locally it is convenient to provide a name so that we don't need a labelbox project to use these interfaces."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "id": "bearing-logan",
            "metadata": {},
            "outputs": [],
            "source": [
                "# When uploading for MAL or MEA we need an ontology.\n",
                "# So let's create one\n",
                "\n",
                "# These names have to match our object names exactly!!\n",
                "ontology_builder = OntologyBuilder(tools=[\n",
                "    Tool(tool=Tool.Type.POLYGON, name=\"deer\"),\n",
                "    Tool(tool=Tool.Type.SEGMENTATION,\n",
                "         name=\"deer_nose\",\n",
                "         classifications=[\n",
                "             Classification(class_type=Classification.Type.RADIO,\n",
                "                            name=\"description\",\n",
                "                            options=[Option(value=\"wet\")])\n",
                "         ]),\n",
                "    Tool(tool=Tool.Type.SEGMENTATION, name=\"deer_eyes\")\n",
                "],\n",
                "                                   classifications=[\n",
                "                                       Classification(\n",
                "                                           Classification.Type.CHECKLIST,\n",
                "                                           name=\"image_description\",\n",
                "                                           options=[\n",
                "                                               Option(value=\"bright\"),\n",
                "                                               Option(value=\"not_blurry\"),\n",
                "                                               Option(value=\"dark\")\n",
                "                                           ])\n",
                "                                   ])\n",
                "\n",
                "editor = next(\n",
                "    client.get_labeling_frontends(where=LabelingFrontend.name == \"Editor\"))\n",
                "project = client.create_project(name=\"test_annotation_types\", media_type=MediaType.Image)\n",
                "project.setup(editor, ontology_builder.asdict())\n",
                "project.datasets.connect(dataset)\n",
                "\n",
                "ontology = OntologyBuilder.from_project(project)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "special-prerequisite",
            "metadata": {},
            "source": [
                "### Next Steps\n",
                "* Annotation types should be thought of as low level interfaces\n",
                "* We are working on a set of tools to make working with annotation types less verbose. Please provide any feedback!\n",
                "* Checkout other notebooks to see how to use higher level tools that are compatible with these interfaces"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "id": "expired-bryan",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cleanup\n",
                "dataset.delete()\n",
                "project.delete()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
