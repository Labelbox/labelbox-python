{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increasing-operator",
   "metadata": {},
   "source": [
    "# Label Containers\n",
    "* There are two high level containers for labels\n",
    "    1. [`LabelList`](#LabelList)\n",
    "    2. [`LabelGenerator`](#LabelGenerator)\n",
    "* Tools that are built to convert between formats, help with etl, and model training all will operate on these containers\n",
    "* Make sure to read basics. Explanations are not repeated here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"labelbox[data]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "from labelbox import Client\n",
    "from labelbox.data.annotation_types import LabelList, LabelGenerator\n",
    "from labelbox.schema.ontology import OntologyBuilder, Tool, Classification, Option\n",
    "from labelbox import LabelingFrontend\n",
    "from labelbox.data.annotation_types import (\n",
    "    Label, \n",
    "    ImageData,\n",
    "    MaskData,\n",
    "    Mask, \n",
    "    Point, \n",
    "    Polygon, \n",
    "    ClassificationAnswer, \n",
    "    Radio, \n",
    "    Checklist, \n",
    "    ObjectAnnotation, \n",
    "    ClassificationAnnotation\n",
    ")\n",
    "import requests\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't want to give google access to drive you can skip this cell\n",
    "# and manually set `API_KEY` below.\n",
    "COLAB = \"google.colab\" in str(get_ipython())\n",
    "if COLAB:\n",
    "    !pip install colab-env -qU\n",
    "    from colab_env import envvar_handler\n",
    "    envvar_handler.envload()\n",
    "\n",
    "API_KEY = os.environ.get(\"LABELBOX_API_KEY\")\n",
    "if not os.environ.get(\"LABELBOX_API_KEY\"):\n",
    "    API_KEY = getpass(\"Please enter your labelbox api key\")\n",
    "    if COLAB:\n",
    "        envvar_handler.add_env(\"LABELBOX_API_KEY\", API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only update this if you have an on-prem deployment\n",
    "ENDPOINT = \"https://api.labelbox.com/graphql\"\n",
    "client = Client(api_key=API_KEY, endpoint=ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-burlington",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "* The following functions are explained in the [basics notebooks](https://colab.research.google.com/github/Labelbox/labelbox-python/blob/develop/examples/annotation_types/basics.ipynb)\n",
    "* Please skip to the [LabelList](#LabelList) section to continue with this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signing_function(obj_bytes: bytes) -> str:\n",
    "    # Do not use this signer. You will not be able to resign these images at a later date.\n",
    "    url = client.upload_data(content=obj_bytes, sign=True)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polygon():\n",
    "    # Given some polygon:\n",
    "    xy_poly = [\n",
    "        [60, 161], [67, 177], [76, 180], [77, 222], [82, 246], [78, 291], [72, 300], [87, 300], \n",
    "        [94, 244], [103, 243], [100, 269], [90, 290], [95, 296], [104, 292], [108, 272], \n",
    "        [111, 300], [121, 300], [117, 244], [128, 236], [133, 298], [142, 297], [137, 250], \n",
    "        [146, 208], [138, 185], [120, 180], [105, 189], [112, 162], [93, 156], [72, 160], \n",
    "    ]\n",
    "    return Polygon(points = [Point(x = x, y = y) for x,y in xy_poly])\n",
    "\n",
    "\n",
    "def get_labels():\n",
    "    im_h, im_w = 300, 200\n",
    "    image_url = \"https://picsum.photos/id/1003/200/300\"\n",
    "    nose_color, eye_color = (0,255,0), (255,0,0)\n",
    "    nose_mask = Point(x = 96, y = 194).draw(im_h, im_w, thickness = 3)\n",
    "    eye_masks = [\n",
    "         Point(x = 84, y = 182).draw(im_h, im_w, thickness = 3),\n",
    "        Point(x = 99, y = 181).draw(im_h, im_w, thickness = 3),\n",
    "    ]\n",
    "    mask_arr = np.max([*eye_masks,nose_mask] , axis = 0)\n",
    "    mask = MaskData(arr = mask_arr)\n",
    "    return [Label(\n",
    "        data = ImageData(im_bytes = requests.get(image_url).content),\n",
    "        annotations = [\n",
    "            ObjectAnnotation(value = get_polygon(),name = \"deer\"),\n",
    "            ObjectAnnotation(name = \"deer_eyes\", value = Mask(mask = mask, color = eye_color)),  \n",
    "            ObjectAnnotation(name = \"deer_nose\", value = Mask(mask = mask, color = nose_color),\n",
    "                classifications = [\n",
    "                    ClassificationAnnotation(\n",
    "                        name = \"nose_description\",\n",
    "                        value = Radio(\n",
    "                            answer = ClassificationAnswer(name = \"wet\")\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            ClassificationAnnotation(name = \"image_description\", value = Checklist(answer = [\n",
    "                ClassificationAnswer(name = \"bright\")\n",
    "            ]))\n",
    "        ]\n",
    "    )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_schema_ids(label):\n",
    "    for annotation in label.annotations:\n",
    "        print(f\"Object : {annotation.name} - {annotation.feature_schema_id}\")\n",
    "        for classification in getattr(annotation, 'classifications', []):\n",
    "            print(f\"--- Subclass : {classification.name} - {classification.feature_schema_id}\")\n",
    "            option = classification.value\n",
    "            print(f\"--- --- Options: {option.answer.name} - {option.answer.feature_schema_id}\")\n",
    "\n",
    "        if isinstance(annotation, ClassificationAnnotation):\n",
    "            for option in annotation.value.answer:\n",
    "                print(f\"--- Options: {option.name} - {option.feature_schema_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_project():\n",
    "    # These names have to match our object names exactly!!\n",
    "    ontology_builder = OntologyBuilder(tools=[\n",
    "        Tool(tool=Tool.Type.POLYGON, name=\"deer\"),\n",
    "        Tool(tool=Tool.Type.SEGMENTATION, name=\"deer_nose\", \n",
    "             classifications = [\n",
    "                 Classification(\n",
    "                     class_type = Classification.Type.RADIO, \n",
    "                     instructions = \"nose_description\", \n",
    "                     options = [Option(value = \"wet\")]\n",
    "                 )]),\n",
    "        Tool(tool=Tool.Type.SEGMENTATION, name=\"deer_eyes\")    \n",
    "    ], classifications = [\n",
    "        Classification(\n",
    "            Classification.Type.CHECKLIST, \n",
    "            instructions = \"image_description\", \n",
    "            options = [Option(value = \"bright\"), Option(value = \"not_blurry\"), Option(value = \"dark\")])])\n",
    "\n",
    "    editor = next(\n",
    "        client.get_labeling_frontends(where=LabelingFrontend.name == \"Editor\"))\n",
    "    project = client.create_project(name=\"test_annotation_types\")\n",
    "    project.setup(editor, ontology_builder.asdict())\n",
    "    dataset = client.create_dataset(name = 'my_ds')\n",
    "    project.datasets.connect(dataset)\n",
    "\n",
    "    ontology = OntologyBuilder.from_project(project)\n",
    "    return ontology, dataset, project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mask_urls(label):\n",
    "    for annotation in label.annotations:\n",
    "        if isinstance(annotation.value, Mask):\n",
    "            print(annotation.value.mask.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_references(label):\n",
    "    print('\\n---  schema ids ---\\n')\n",
    "    show_feature_schema_ids(label)\n",
    "    print(\"\\n--- mask urls ---\\n\")\n",
    "    print_mask_urls(label)\n",
    "    print('\\n--- image url ---\\n')\n",
    "    print(label.data.url)    \n",
    "    print('\\n--- data row reference ---\\n')\n",
    "    print(original_label.data.uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-ocean",
   "metadata": {},
   "source": [
    "# LabelList\n",
    "* This object is essentially a list of Labels with a set of helpful utilties\n",
    "* It is simple and fast at the expense of memory\n",
    "    * Larger datasets shouldn't use label list ( or at least will require more memory )\n",
    "* Why use label list over just a list of labels?\n",
    "    * Multithreaded utilities (faster)\n",
    "    * Compatible with converter functions (functions useful for translating between formats, etl, and training )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels()\n",
    "label_list = LabelList(labels)\n",
    "\n",
    "# Also build LabelLists iteratively\n",
    "label_list = LabelList()\n",
    "for label in labels:\n",
    "    label_list.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-command",
   "metadata": {},
   "source": [
    "## Iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterable, behaves like a list\n",
    "for label in label_list:\n",
    "    print(type(label))\n",
    "# Get length\n",
    "print(len(label_list))\n",
    "# By index\n",
    "print(type(label_list[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-league",
   "metadata": {},
   "source": [
    "### Upload segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add urls to all segmentation masks:\n",
    "# (in parallel)\n",
    "for label in label_list:\n",
    "    print_mask_urls(label)\n",
    "    \n",
    "label_list.add_url_to_masks(signing_function)\n",
    "\n",
    "for label in label_list:\n",
    "    print_mask_urls(label)\n",
    "# Again note that these all share the same segmentation mask\n",
    "# ( This is determined by the fact that they share the same reference )\n",
    "# This mask is only uploaded once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-tactics",
   "metadata": {},
   "source": [
    "### Create signed urls for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add urls to all segmentation masks:\n",
    "# (in parallel)\n",
    "print(label_list[0].data.url)\n",
    "label_list.add_url_to_data(signing_function)\n",
    "print(label_list[0].data.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-miracle",
   "metadata": {},
   "source": [
    "### Add to labelbox dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the next two sections we need an ontology and dataset\n",
    "ontology, dataset, project = setup_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_list[0].data.uid)\n",
    "# Note that this function will assign a uuid as the external id if it isn't provided.\n",
    "label_list.add_to_dataset(dataset, signing_function)\n",
    "print(label_list[0].data.uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-translation",
   "metadata": {},
   "source": [
    "### Add schema ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in label_list:\n",
    "    show_feature_schema_ids(label)\n",
    "# Note that this function will assign a uuid as the external id if it isn't provided.\n",
    "label_list.assign_feature_schema_ids(ontology)\n",
    "print('-'* 50)\n",
    "for label in label_list:\n",
    "    show_feature_schema_ids(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup:\n",
    "dataset.delete()\n",
    "project.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-auditor",
   "metadata": {},
   "source": [
    "# LabelGenerator\n",
    "* This object generates labels and provides a set of helpful utilties\n",
    "* This object is complex and slower than the `LabelList` in order to be highly memory efficient\n",
    "    * Larger datasets should use label generators\n",
    "* Why use label generator over just a generator that yields labels?\n",
    "    * Parallel io operations are run in the background to prepare results\n",
    "    * Compatible with converter functions (functions useful for translating between formats, etl, and training )\n",
    "* The first qsize elements run serially from when the chained functions are added.\n",
    "    * After that iterating will get much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels()\n",
    "label_generator = LabelGenerator(labels)\n",
    "ontology, dataset, project = setup_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can't show the before and afters because the generator is not repeatable\n",
    "\n",
    "try:\n",
    "    label = next(label_generator)\n",
    "    print(\"Ran once\")\n",
    "    label = next(label_generator)\n",
    "    print(\"Ran twice\")\n",
    "except StopIteration:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not support indexing ( it is a generator.. )\n",
    "try:\n",
    "    label_generator[0]\n",
    "    print(\"Can index\")\n",
    "except TypeError:\n",
    "    print(\"Unable to index\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-opportunity",
   "metadata": {},
   "source": [
    "### Functions to modify results\n",
    "* We can set functions to run on the result of the generator\n",
    "* Since these are run in background threads it is a lot faster than applying them on each label individually\n",
    "* The functions are lazily evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate because we already went through all of the items when we showed that it isn't repeatable\n",
    "original_label = labels[0]\n",
    "\n",
    "show_references(original_label)\n",
    "label_generator = LabelGenerator(labels) \\\n",
    "        .add_url_to_masks(signing_function) \\\n",
    "        .add_to_dataset(dataset, signing_function) \\\n",
    "        .assign_feature_schema_ids(ontology)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_references(original_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = next(label_generator)\n",
    "show_references(original_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-blood",
   "metadata": {},
   "source": [
    "* Note that the first qsize elements run serially from when the chained functions are added.\n",
    "* After that iterating will get much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelGenerators can be converted to a LabelList\n",
    "LabelGenerator(labels).as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.delete()\n",
    "project.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-merchandise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-berkeley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
